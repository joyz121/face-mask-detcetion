{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda3\\envs\\deeplearning\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import bs4\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision import transforms, datasets, models\n",
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_box(obj):\n",
    "    \n",
    "    xmin = int(obj.find('xmin').text)\n",
    "    ymin = int(obj.find('ymin').text)\n",
    "    xmax = int(obj.find('xmax').text)\n",
    "    ymax = int(obj.find('ymax').text)\n",
    "    \n",
    "    return [xmin, ymin, xmax, ymax]\n",
    "\n",
    "def generate_label(obj):\n",
    "    if obj.find('name').text == \"with_mask\":\n",
    "        return 1\n",
    "    elif obj.find('name').text == \"mask_weared_incorrect\":\n",
    "        return 2\n",
    "    return 0\n",
    "\n",
    "def generate_target(image_id, file): \n",
    "    with open(file) as f:\n",
    "        data = f.read()\n",
    "        soup = bs4.BeautifulSoup(data, 'xml')\n",
    "        objects = soup.find_all('object')\n",
    "\n",
    "        num_objs = len(objects)\n",
    "\n",
    "        # Bounding boxes for objects\n",
    "        # In coco format, bbox = [xmin, ymin, width, height]\n",
    "        # In pytorch, the input should be [xmin, ymin, xmax, ymax]\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        for i in objects:\n",
    "            boxes.append(generate_box(i))\n",
    "            labels.append(generate_label(i))\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        # Labels (In my case, I only one class: target class or background)\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "        # Tensorise img_id\n",
    "        img_id = torch.tensor([image_id])\n",
    "        # Annotation is in dictionary format\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"image_id\"] = img_id\n",
    "        \n",
    "        return target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get images list\n",
    "imgs= list(sorted(os.listdir(\"C:/Users/90761/Desktop/kaggle/face-mask-detection/dataset/archive/images/\")))\n",
    "# get labels list\n",
    "labels=list(sorted(os.listdir(\"C:/Users/90761/Desktop/kaggle/face-mask-detection/dataset/archive/annotations/\")))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskDataset(object):\n",
    "    def __init__(self,transforms):\n",
    "        self.transforms=transforms\n",
    "        self.imgs=list(sorted(os.listdir(\"C:/Users/90761/Desktop/kaggle/face-mask-detection/dataset/archive/images/\")))\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        file_img='maksssksksss'+str(idx)+'.png'\n",
    "        file_label='maksssksksss'+str(idx)+'.xml'\n",
    "        img_path=os.path.join(\"C:/Users/90761/Desktop/kaggle/face-mask-detection/dataset/archive/images/\", file_img)\n",
    "        label_path=os.path.join(\"C:/Users/90761/Desktop/kaggle/face-mask-detection/dataset/archive/annotations/\",file_label)\n",
    "        img=Image.open(img_path).convert(\"RGB\")\n",
    "        target=generate_target(idx, label_path)\n",
    "        if self.transforms is not None:\n",
    "            img = self.transforms(img)\n",
    "        return img,target\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transform= transforms.Compose([\n",
    "        transforms.ToTensor(), \n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "dataset=MaskDataset(data_transform)\n",
    "dataloader=torch.utils.data.DataLoader(dataset,batch_size=4,collate_fn=collate_fn)\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_instance_segmentation(num_classes):\n",
    "    # load an instance segmentation model pre-trained pre-trained on COCO\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=torchvision.models.detection.FasterRCNN_ResNet50_FPN_Weights.COCO_V1)\n",
    "    # get number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    return model\n",
    "\n",
    "model = get_model_instance_segmentation(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'boxes': tensor([[ 79., 105., 109., 142.],\n",
      "        [185., 100., 226., 144.],\n",
      "        [325.,  90., 360., 141.]], device='cuda:0'), 'labels': tensor([0, 1, 0], device='cuda:0'), 'image_id': tensor([0], device='cuda:0')}, {'boxes': tensor([[321.,  34., 354.,  69.],\n",
      "        [224.,  38., 261.,  73.],\n",
      "        [299.,  58., 315.,  81.],\n",
      "        [143.,  74., 174., 115.],\n",
      "        [ 74.,  69.,  95.,  99.],\n",
      "        [191.,  67., 221.,  93.],\n",
      "        [ 21.,  73.,  44.,  93.],\n",
      "        [369.,  70., 398.,  99.],\n",
      "        [ 83.,  56., 111.,  89.]], device='cuda:0'), 'labels': tensor([1, 1, 1, 1, 1, 1, 1, 1, 0], device='cuda:0'), 'image_id': tensor([1], device='cuda:0')}, {'boxes': tensor([[ 68.,  42., 105.,  69.],\n",
      "        [154.,  47., 178.,  74.],\n",
      "        [238.,  34., 262.,  69.],\n",
      "        [333.,  31., 366.,  65.]], device='cuda:0'), 'labels': tensor([1, 1, 1, 2], device='cuda:0'), 'image_id': tensor([2], device='cuda:0')}, {'boxes': tensor([[ 52.,  53.,  73.,  76.],\n",
      "        [ 72.,  53.,  92.,  75.],\n",
      "        [112.,  51., 120.,  68.],\n",
      "        [155.,  60., 177.,  83.],\n",
      "        [189.,  59., 210.,  80.],\n",
      "        [235.,  57., 257.,  78.],\n",
      "        [289.,  60., 309.,  83.],\n",
      "        [313.,  68., 333.,  90.],\n",
      "        [351.,  35., 364.,  59.]], device='cuda:0'), 'labels': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'), 'image_id': tensor([3], device='cuda:0')}]\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "for imgs, annotations in dataloader:\n",
    "    imgs = list(img.to(device) for img in imgs)\n",
    "    annotations = [{k: v.to(device) for k, v in t.items()} for t in annotations]\n",
    "    print(annotations)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1/214, Loss: 1.1745675802230835\n",
      "Iteration: 2/214, Loss: 0.6301990747451782\n",
      "Iteration: 3/214, Loss: 0.3290580213069916\n",
      "Iteration: 4/214, Loss: 0.9844484329223633\n",
      "Iteration: 5/214, Loss: 0.30488312244415283\n",
      "Iteration: 6/214, Loss: 0.37946823239326477\n",
      "Iteration: 7/214, Loss: 0.20081521570682526\n",
      "Iteration: 8/214, Loss: 0.44382888078689575\n",
      "Iteration: 9/214, Loss: 0.6705703735351562\n",
      "Iteration: 10/214, Loss: 1.7708250284194946\n",
      "Iteration: 11/214, Loss: 0.6534496545791626\n",
      "Iteration: 12/214, Loss: 0.8332648277282715\n",
      "Iteration: 13/214, Loss: 0.11863160878419876\n",
      "Iteration: 14/214, Loss: 2.0190484523773193\n",
      "Iteration: 15/214, Loss: 0.43926000595092773\n",
      "Iteration: 16/214, Loss: 0.3698033094406128\n",
      "Iteration: 17/214, Loss: 2.194249153137207\n",
      "Iteration: 18/214, Loss: 0.09814711660146713\n",
      "Iteration: 19/214, Loss: 0.08205889910459518\n",
      "Iteration: 20/214, Loss: 1.0562340021133423\n",
      "Iteration: 21/214, Loss: 0.18489094078540802\n",
      "Iteration: 22/214, Loss: 0.4247574806213379\n",
      "Iteration: 23/214, Loss: 0.2694385349750519\n",
      "Iteration: 24/214, Loss: 1.252687692642212\n",
      "Iteration: 25/214, Loss: 0.17829909920692444\n",
      "Iteration: 26/214, Loss: 0.3364510238170624\n",
      "Iteration: 27/214, Loss: 0.33148109912872314\n",
      "Iteration: 28/214, Loss: 0.39429253339767456\n",
      "Iteration: 29/214, Loss: 0.3235222399234772\n",
      "Iteration: 30/214, Loss: 0.5959188938140869\n",
      "Iteration: 31/214, Loss: 0.17582209408283234\n",
      "Iteration: 32/214, Loss: 0.5745037198066711\n",
      "Iteration: 33/214, Loss: 0.04310072958469391\n",
      "Iteration: 34/214, Loss: 0.4048191010951996\n",
      "Iteration: 35/214, Loss: 0.115197092294693\n",
      "Iteration: 36/214, Loss: 0.5711968541145325\n",
      "Iteration: 37/214, Loss: 0.5302698612213135\n",
      "Iteration: 38/214, Loss: 0.4228779375553131\n",
      "Iteration: 39/214, Loss: 0.8097515106201172\n",
      "Iteration: 40/214, Loss: 1.2277798652648926\n",
      "Iteration: 41/214, Loss: 0.31958693265914917\n",
      "Iteration: 42/214, Loss: 0.8728070259094238\n",
      "Iteration: 43/214, Loss: 0.5783373713493347\n",
      "Iteration: 44/214, Loss: 0.055563028901815414\n",
      "Iteration: 45/214, Loss: 1.060616135597229\n",
      "Iteration: 46/214, Loss: 0.11463860422372818\n",
      "Iteration: 47/214, Loss: 0.9033597707748413\n",
      "Iteration: 48/214, Loss: 0.10444588214159012\n",
      "Iteration: 49/214, Loss: 0.13341175019741058\n",
      "Iteration: 50/214, Loss: 0.1020413413643837\n",
      "Iteration: 51/214, Loss: 0.2709842622280121\n",
      "Iteration: 52/214, Loss: 0.674480140209198\n",
      "Iteration: 53/214, Loss: 0.19846779108047485\n",
      "Iteration: 54/214, Loss: 0.11262266337871552\n",
      "Iteration: 55/214, Loss: 0.6875087022781372\n",
      "Iteration: 56/214, Loss: 0.09595029056072235\n",
      "Iteration: 57/214, Loss: 0.12272060662508011\n",
      "Iteration: 58/214, Loss: 0.1806809902191162\n",
      "Iteration: 59/214, Loss: 0.7997075915336609\n",
      "Iteration: 60/214, Loss: 0.013897620141506195\n",
      "Iteration: 61/214, Loss: 0.6691004037857056\n",
      "Iteration: 62/214, Loss: 0.13370929658412933\n",
      "Iteration: 63/214, Loss: 0.01834842562675476\n",
      "Iteration: 64/214, Loss: 0.14596998691558838\n",
      "Iteration: 65/214, Loss: 0.8029176592826843\n",
      "Iteration: 66/214, Loss: 0.8912410736083984\n",
      "Iteration: 67/214, Loss: 0.6479792594909668\n",
      "Iteration: 68/214, Loss: 0.24479937553405762\n",
      "Iteration: 69/214, Loss: 0.3459526300430298\n",
      "Iteration: 70/214, Loss: 0.2780929207801819\n",
      "Iteration: 71/214, Loss: 1.0020455121994019\n",
      "Iteration: 72/214, Loss: 0.8407121300697327\n",
      "Iteration: 73/214, Loss: 0.238274946808815\n",
      "Iteration: 74/214, Loss: 0.45845454931259155\n",
      "Iteration: 75/214, Loss: 0.7722731828689575\n",
      "Iteration: 76/214, Loss: 0.178657665848732\n",
      "Iteration: 77/214, Loss: 0.23066294193267822\n",
      "Iteration: 78/214, Loss: 0.30477824807167053\n",
      "Iteration: 79/214, Loss: 0.2130669802427292\n",
      "Iteration: 80/214, Loss: 0.3153659701347351\n",
      "Iteration: 81/214, Loss: 0.3343879282474518\n",
      "Iteration: 82/214, Loss: 0.08969052881002426\n",
      "Iteration: 83/214, Loss: 0.07718829810619354\n",
      "Iteration: 84/214, Loss: 0.08683031052350998\n",
      "Iteration: 85/214, Loss: 0.534302294254303\n",
      "Iteration: 86/214, Loss: 0.42863351106643677\n",
      "Iteration: 87/214, Loss: 0.6290398240089417\n",
      "Iteration: 88/214, Loss: 0.04516375809907913\n",
      "Iteration: 89/214, Loss: 0.1522008627653122\n",
      "Iteration: 90/214, Loss: 0.09759344160556793\n",
      "Iteration: 91/214, Loss: 0.09651675820350647\n",
      "Iteration: 92/214, Loss: 0.3278503119945526\n",
      "Iteration: 93/214, Loss: 0.5714831352233887\n",
      "Iteration: 94/214, Loss: 0.17997166514396667\n",
      "Iteration: 95/214, Loss: 0.7741907835006714\n",
      "Iteration: 96/214, Loss: 0.12244800478219986\n",
      "Iteration: 97/214, Loss: 0.2811543345451355\n",
      "Iteration: 98/214, Loss: 1.170756459236145\n",
      "Iteration: 99/214, Loss: 0.25441133975982666\n",
      "Iteration: 100/214, Loss: 0.4277556836605072\n",
      "Iteration: 101/214, Loss: 0.3380492031574249\n",
      "Iteration: 102/214, Loss: 0.02791569009423256\n",
      "Iteration: 103/214, Loss: 0.6694349050521851\n",
      "Iteration: 104/214, Loss: 0.5232079029083252\n",
      "Iteration: 105/214, Loss: 0.5324264764785767\n",
      "Iteration: 106/214, Loss: 0.03886711597442627\n",
      "Iteration: 107/214, Loss: 0.8819332122802734\n",
      "Iteration: 108/214, Loss: 0.9468486309051514\n",
      "Iteration: 109/214, Loss: 0.07636363804340363\n",
      "Iteration: 110/214, Loss: 0.32132288813591003\n",
      "Iteration: 111/214, Loss: 1.1141213178634644\n",
      "Iteration: 112/214, Loss: 0.621031641960144\n",
      "Iteration: 113/214, Loss: 0.6382206082344055\n",
      "Iteration: 114/214, Loss: 0.5509834885597229\n",
      "Iteration: 115/214, Loss: 0.13053451478481293\n",
      "Iteration: 116/214, Loss: 0.7928522229194641\n",
      "Iteration: 117/214, Loss: 0.45348426699638367\n",
      "Iteration: 118/214, Loss: 0.11412353068590164\n",
      "Iteration: 119/214, Loss: 0.1344931423664093\n",
      "Iteration: 120/214, Loss: 0.4468955099582672\n",
      "Iteration: 121/214, Loss: 0.12679114937782288\n",
      "Iteration: 122/214, Loss: 0.2914893925189972\n",
      "Iteration: 123/214, Loss: 0.2415262758731842\n",
      "Iteration: 124/214, Loss: 0.06950173527002335\n",
      "Iteration: 125/214, Loss: 0.3404031991958618\n",
      "Iteration: 126/214, Loss: 0.08481346815824509\n",
      "Iteration: 127/214, Loss: 0.07716185599565506\n",
      "Iteration: 128/214, Loss: 0.3401305675506592\n",
      "Iteration: 129/214, Loss: 0.47286054491996765\n",
      "Iteration: 130/214, Loss: 0.4802246391773224\n",
      "Iteration: 131/214, Loss: 0.08567497879266739\n",
      "Iteration: 132/214, Loss: 0.6325668096542358\n",
      "Iteration: 133/214, Loss: 0.03705141320824623\n",
      "Iteration: 134/214, Loss: 0.08296480774879456\n",
      "Iteration: 135/214, Loss: 0.052934471517801285\n",
      "Iteration: 136/214, Loss: 0.01795397326350212\n",
      "Iteration: 137/214, Loss: 0.017649361863732338\n",
      "Iteration: 138/214, Loss: 0.10812120139598846\n",
      "Iteration: 139/214, Loss: 0.11505131423473358\n",
      "Iteration: 140/214, Loss: 0.945866048336029\n",
      "Iteration: 141/214, Loss: 0.12294697016477585\n",
      "Iteration: 142/214, Loss: 1.041593074798584\n",
      "Iteration: 143/214, Loss: 0.0815671756863594\n",
      "Iteration: 144/214, Loss: 0.4326079487800598\n",
      "Iteration: 145/214, Loss: 0.158092200756073\n",
      "Iteration: 146/214, Loss: 0.10024777799844742\n",
      "Iteration: 147/214, Loss: 0.6384158134460449\n",
      "Iteration: 148/214, Loss: 0.42707470059394836\n",
      "Iteration: 149/214, Loss: 0.12266398221254349\n",
      "Iteration: 150/214, Loss: 0.16131839156150818\n",
      "Iteration: 151/214, Loss: 0.11982148885726929\n",
      "Iteration: 152/214, Loss: 0.41232234239578247\n",
      "Iteration: 153/214, Loss: 0.1251855194568634\n",
      "Iteration: 154/214, Loss: 0.1369468718767166\n",
      "Iteration: 155/214, Loss: 0.7829354405403137\n",
      "Iteration: 156/214, Loss: 0.47815680503845215\n",
      "Iteration: 157/214, Loss: 0.04175947234034538\n",
      "Iteration: 158/214, Loss: 0.22889627516269684\n",
      "Iteration: 159/214, Loss: 0.07368020713329315\n",
      "Iteration: 160/214, Loss: 0.266207218170166\n",
      "Iteration: 161/214, Loss: 0.7305299639701843\n",
      "Iteration: 162/214, Loss: 0.1124941036105156\n",
      "Iteration: 163/214, Loss: 0.2583978474140167\n",
      "Iteration: 164/214, Loss: 0.06964799761772156\n",
      "Iteration: 165/214, Loss: 0.806643009185791\n",
      "Iteration: 166/214, Loss: 0.01981666497886181\n",
      "Iteration: 167/214, Loss: 0.1419130265712738\n",
      "Iteration: 168/214, Loss: 0.09182216227054596\n",
      "Iteration: 169/214, Loss: 0.9875972867012024\n",
      "Iteration: 170/214, Loss: 0.7196378111839294\n",
      "Iteration: 171/214, Loss: 0.9354557394981384\n",
      "Iteration: 172/214, Loss: 0.32573843002319336\n",
      "Iteration: 173/214, Loss: 0.12478623539209366\n",
      "Iteration: 174/214, Loss: 0.1055506095290184\n",
      "Iteration: 175/214, Loss: 0.4506016671657562\n",
      "Iteration: 176/214, Loss: 0.15836696326732635\n",
      "Iteration: 177/214, Loss: 0.4543023705482483\n",
      "Iteration: 178/214, Loss: 0.3040894865989685\n",
      "Iteration: 179/214, Loss: 0.10674791783094406\n",
      "Iteration: 180/214, Loss: 0.009850510396063328\n",
      "Iteration: 181/214, Loss: 0.48484301567077637\n",
      "Iteration: 182/214, Loss: 0.11592084914445877\n",
      "Iteration: 183/214, Loss: 0.7222957611083984\n",
      "Iteration: 184/214, Loss: 0.462026447057724\n",
      "Iteration: 185/214, Loss: 0.22833795845508575\n",
      "Iteration: 186/214, Loss: 0.12372461706399918\n",
      "Iteration: 187/214, Loss: 0.26996561884880066\n",
      "Iteration: 188/214, Loss: 0.46426090598106384\n",
      "Iteration: 189/214, Loss: 1.3177895545959473\n",
      "Iteration: 190/214, Loss: 0.474721223115921\n",
      "Iteration: 191/214, Loss: 1.0845831632614136\n",
      "Iteration: 192/214, Loss: 0.08443506807088852\n",
      "Iteration: 193/214, Loss: 0.5138655304908752\n",
      "Iteration: 194/214, Loss: 0.11188021302223206\n",
      "Iteration: 195/214, Loss: 0.18264061212539673\n",
      "Iteration: 196/214, Loss: 0.6999273896217346\n",
      "Iteration: 197/214, Loss: 0.5579449534416199\n",
      "Iteration: 198/214, Loss: 0.23631535470485687\n",
      "Iteration: 199/214, Loss: 0.15986990928649902\n",
      "Iteration: 200/214, Loss: 0.6232173442840576\n",
      "Iteration: 201/214, Loss: 0.2650160491466522\n",
      "Iteration: 202/214, Loss: 0.6072185635566711\n",
      "Iteration: 203/214, Loss: 0.07885698229074478\n",
      "Iteration: 204/214, Loss: 0.2092689573764801\n",
      "Iteration: 205/214, Loss: 0.4396677017211914\n",
      "Iteration: 206/214, Loss: 0.04368769749999046\n",
      "Iteration: 207/214, Loss: 0.839614987373352\n",
      "Iteration: 208/214, Loss: 0.09573321044445038\n",
      "Iteration: 209/214, Loss: 0.3344631791114807\n",
      "Iteration: 210/214, Loss: 0.3424598276615143\n",
      "Iteration: 211/214, Loss: 0.2064639776945114\n",
      "Iteration: 212/214, Loss: 0.22322914004325867\n",
      "Iteration: 213/214, Loss: 0.698173999786377\n",
      "Iteration: 214/214, Loss: 0.27712738513946533\n",
      "tensor(87.1555, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Iteration: 1/214, Loss: 0.1459071934223175\n",
      "Iteration: 2/214, Loss: 0.05247687175869942\n",
      "Iteration: 3/214, Loss: 0.11728516966104507\n",
      "Iteration: 4/214, Loss: 0.8997412919998169\n",
      "Iteration: 5/214, Loss: 0.09917722642421722\n",
      "Iteration: 6/214, Loss: 0.11316069960594177\n",
      "Iteration: 7/214, Loss: 0.2779742479324341\n",
      "Iteration: 8/214, Loss: 0.13626325130462646\n",
      "Iteration: 9/214, Loss: 0.1551845520734787\n",
      "Iteration: 10/214, Loss: 0.7731480598449707\n",
      "Iteration: 11/214, Loss: 0.40450260043144226\n",
      "Iteration: 12/214, Loss: 0.34370312094688416\n",
      "Iteration: 13/214, Loss: 0.006653570104390383\n",
      "Iteration: 14/214, Loss: 1.0080299377441406\n",
      "Iteration: 15/214, Loss: 0.08088615536689758\n",
      "Iteration: 16/214, Loss: 0.06464220583438873\n",
      "Iteration: 17/214, Loss: 1.0869896411895752\n",
      "Iteration: 18/214, Loss: 0.017210831865668297\n",
      "Iteration: 19/214, Loss: 0.013642330653965473\n",
      "Iteration: 20/214, Loss: 0.4872693717479706\n",
      "Iteration: 21/214, Loss: 0.0772285908460617\n",
      "Iteration: 22/214, Loss: 0.2771824896335602\n",
      "Iteration: 23/214, Loss: 0.13602617383003235\n",
      "Iteration: 24/214, Loss: 0.7313818335533142\n",
      "Iteration: 25/214, Loss: 0.18457850813865662\n",
      "Iteration: 26/214, Loss: 0.27294179797172546\n",
      "Iteration: 27/214, Loss: 0.2848122715950012\n",
      "Iteration: 28/214, Loss: 0.3554087281227112\n",
      "Iteration: 29/214, Loss: 0.29487159848213196\n",
      "Iteration: 30/214, Loss: 0.3433367908000946\n",
      "Iteration: 31/214, Loss: 0.07783634215593338\n",
      "Iteration: 32/214, Loss: 0.2790265679359436\n",
      "Iteration: 33/214, Loss: 0.027372173964977264\n",
      "Iteration: 34/214, Loss: 0.3305383324623108\n",
      "Iteration: 35/214, Loss: 0.11741938441991806\n",
      "Iteration: 36/214, Loss: 0.2753458619117737\n",
      "Iteration: 37/214, Loss: 0.284162700176239\n",
      "Iteration: 38/214, Loss: 0.43429818749427795\n",
      "Iteration: 39/214, Loss: 0.5344685912132263\n",
      "Iteration: 40/214, Loss: 0.9522022008895874\n",
      "Iteration: 41/214, Loss: 0.13930392265319824\n",
      "Iteration: 42/214, Loss: 0.4655260741710663\n",
      "Iteration: 43/214, Loss: 0.44771701097488403\n",
      "Iteration: 44/214, Loss: 0.018955016508698463\n",
      "Iteration: 45/214, Loss: 0.7617673873901367\n",
      "Iteration: 46/214, Loss: 0.06542044878005981\n",
      "Iteration: 47/214, Loss: 0.5801668763160706\n",
      "Iteration: 48/214, Loss: 0.09334833920001984\n",
      "Iteration: 49/214, Loss: 0.0640704408288002\n",
      "Iteration: 50/214, Loss: 0.13733533024787903\n",
      "Iteration: 51/214, Loss: 0.18738654255867004\n",
      "Iteration: 52/214, Loss: 0.6399253010749817\n",
      "Iteration: 53/214, Loss: 0.16619186103343964\n",
      "Iteration: 54/214, Loss: 0.062380965799093246\n",
      "Iteration: 55/214, Loss: 0.3777562081813812\n",
      "Iteration: 56/214, Loss: 0.06480760872364044\n",
      "Iteration: 57/214, Loss: 0.08578633517026901\n",
      "Iteration: 58/214, Loss: 0.07344524562358856\n",
      "Iteration: 59/214, Loss: 0.648684561252594\n",
      "Iteration: 60/214, Loss: 0.010381076484918594\n",
      "Iteration: 61/214, Loss: 0.5588674545288086\n",
      "Iteration: 62/214, Loss: 0.07748252898454666\n",
      "Iteration: 63/214, Loss: 0.006469930987805128\n",
      "Iteration: 64/214, Loss: 0.1691426932811737\n",
      "Iteration: 65/214, Loss: 0.5387981534004211\n",
      "Iteration: 66/214, Loss: 0.5932477712631226\n",
      "Iteration: 67/214, Loss: 0.4622878134250641\n",
      "Iteration: 68/214, Loss: 0.16903860867023468\n",
      "Iteration: 69/214, Loss: 0.2713535726070404\n",
      "Iteration: 70/214, Loss: 0.2464151829481125\n",
      "Iteration: 71/214, Loss: 0.6326574087142944\n",
      "Iteration: 72/214, Loss: 0.38172709941864014\n",
      "Iteration: 73/214, Loss: 0.24937057495117188\n",
      "Iteration: 74/214, Loss: 0.33175623416900635\n",
      "Iteration: 75/214, Loss: 0.592653751373291\n",
      "Iteration: 76/214, Loss: 0.08732840418815613\n",
      "Iteration: 77/214, Loss: 0.16314244270324707\n",
      "Iteration: 78/214, Loss: 0.2810182571411133\n",
      "Iteration: 79/214, Loss: 0.19726194441318512\n",
      "Iteration: 80/214, Loss: 0.19824714958667755\n",
      "Iteration: 81/214, Loss: 0.21539311110973358\n",
      "Iteration: 82/214, Loss: 0.08671826124191284\n",
      "Iteration: 83/214, Loss: 0.038545336574316025\n",
      "Iteration: 84/214, Loss: 0.05814417824149132\n",
      "Iteration: 85/214, Loss: 0.42226067185401917\n",
      "Iteration: 86/214, Loss: 0.25610998272895813\n",
      "Iteration: 87/214, Loss: 0.410647451877594\n",
      "Iteration: 88/214, Loss: 0.008416595868766308\n",
      "Iteration: 89/214, Loss: 0.06274169683456421\n",
      "Iteration: 90/214, Loss: 0.0843776986002922\n",
      "Iteration: 91/214, Loss: 0.05261751264333725\n",
      "Iteration: 92/214, Loss: 0.1743895709514618\n",
      "Iteration: 93/214, Loss: 0.4267965257167816\n",
      "Iteration: 94/214, Loss: 0.12645354866981506\n",
      "Iteration: 95/214, Loss: 0.5855868458747864\n",
      "Iteration: 96/214, Loss: 0.07958734035491943\n",
      "Iteration: 97/214, Loss: 0.2312992960214615\n",
      "Iteration: 98/214, Loss: 0.7527984976768494\n",
      "Iteration: 99/214, Loss: 0.11533419042825699\n",
      "Iteration: 100/214, Loss: 0.3417649567127228\n",
      "Iteration: 101/214, Loss: 0.20650239288806915\n",
      "Iteration: 102/214, Loss: 0.009407663717865944\n",
      "Iteration: 103/214, Loss: 0.5724650025367737\n",
      "Iteration: 104/214, Loss: 0.18810030817985535\n",
      "Iteration: 105/214, Loss: 0.28999876976013184\n",
      "Iteration: 106/214, Loss: 0.013253307901322842\n",
      "Iteration: 107/214, Loss: 0.7121229767799377\n",
      "Iteration: 108/214, Loss: 0.7472037076950073\n",
      "Iteration: 109/214, Loss: 0.16784806549549103\n",
      "Iteration: 110/214, Loss: 0.15405231714248657\n",
      "Iteration: 111/214, Loss: 0.6737809777259827\n",
      "Iteration: 112/214, Loss: 0.29932641983032227\n",
      "Iteration: 113/214, Loss: 0.35735422372817993\n",
      "Iteration: 114/214, Loss: 0.23686791956424713\n",
      "Iteration: 115/214, Loss: 0.04460713639855385\n",
      "Iteration: 116/214, Loss: 0.769523024559021\n",
      "Iteration: 117/214, Loss: 0.20224663615226746\n",
      "Iteration: 118/214, Loss: 0.05891260877251625\n",
      "Iteration: 119/214, Loss: 0.07678060978651047\n",
      "Iteration: 120/214, Loss: 0.339842289686203\n",
      "Iteration: 121/214, Loss: 0.09396927803754807\n",
      "Iteration: 122/214, Loss: 0.2944461405277252\n",
      "Iteration: 123/214, Loss: 0.10479862242937088\n",
      "Iteration: 124/214, Loss: 0.04725704342126846\n",
      "Iteration: 125/214, Loss: 0.1895996481180191\n",
      "Iteration: 126/214, Loss: 0.04290734976530075\n",
      "Iteration: 127/214, Loss: 0.09455063194036484\n",
      "Iteration: 128/214, Loss: 0.2908761203289032\n",
      "Iteration: 129/214, Loss: 0.3811928629875183\n",
      "Iteration: 130/214, Loss: 0.3572687804698944\n",
      "Iteration: 131/214, Loss: 0.04631485044956207\n",
      "Iteration: 132/214, Loss: 0.40195584297180176\n",
      "Iteration: 133/214, Loss: 0.01727350428700447\n",
      "Iteration: 134/214, Loss: 0.08938676863908768\n",
      "Iteration: 135/214, Loss: 0.09800630807876587\n",
      "Iteration: 136/214, Loss: 0.021567339077591896\n",
      "Iteration: 137/214, Loss: 0.0121992202475667\n",
      "Iteration: 138/214, Loss: 0.12636683881282806\n",
      "Iteration: 139/214, Loss: 0.09809145331382751\n",
      "Iteration: 140/214, Loss: 0.6380316019058228\n",
      "Iteration: 141/214, Loss: 0.07814673334360123\n",
      "Iteration: 142/214, Loss: 0.7625369429588318\n",
      "Iteration: 143/214, Loss: 0.042086366564035416\n",
      "Iteration: 144/214, Loss: 0.39074841141700745\n",
      "Iteration: 145/214, Loss: 0.10497770458459854\n",
      "Iteration: 146/214, Loss: 0.06652481108903885\n",
      "Iteration: 147/214, Loss: 0.4637569487094879\n",
      "Iteration: 148/214, Loss: 0.3880189061164856\n",
      "Iteration: 149/214, Loss: 0.10475433617830276\n",
      "Iteration: 150/214, Loss: 0.13306424021720886\n",
      "Iteration: 151/214, Loss: 0.057426441460847855\n",
      "Iteration: 152/214, Loss: 0.24036811292171478\n",
      "Iteration: 153/214, Loss: 0.08914203196763992\n",
      "Iteration: 154/214, Loss: 0.07283695042133331\n",
      "Iteration: 155/214, Loss: 0.7088404893875122\n",
      "Iteration: 156/214, Loss: 0.4177321791648865\n",
      "Iteration: 157/214, Loss: 0.009488052688539028\n",
      "Iteration: 158/214, Loss: 0.1868094801902771\n",
      "Iteration: 159/214, Loss: 0.08088546246290207\n",
      "Iteration: 160/214, Loss: 0.3463360667228699\n",
      "Iteration: 161/214, Loss: 0.6314951777458191\n",
      "Iteration: 162/214, Loss: 0.08380661904811859\n",
      "Iteration: 163/214, Loss: 0.19467125833034515\n",
      "Iteration: 164/214, Loss: 0.07031416893005371\n",
      "Iteration: 165/214, Loss: 0.5223144888877869\n",
      "Iteration: 166/214, Loss: 0.011708404868841171\n",
      "Iteration: 167/214, Loss: 0.06017112731933594\n",
      "Iteration: 168/214, Loss: 0.059029869735240936\n",
      "Iteration: 169/214, Loss: 0.4800901412963867\n",
      "Iteration: 170/214, Loss: 0.4990650415420532\n",
      "Iteration: 171/214, Loss: 0.7917486429214478\n",
      "Iteration: 172/214, Loss: 0.2535764276981354\n",
      "Iteration: 173/214, Loss: 0.12605051696300507\n",
      "Iteration: 174/214, Loss: 0.11257173120975494\n",
      "Iteration: 175/214, Loss: 0.3398262858390808\n",
      "Iteration: 176/214, Loss: 0.11058363318443298\n",
      "Iteration: 177/214, Loss: 0.3107811510562897\n",
      "Iteration: 178/214, Loss: 0.17994412779808044\n",
      "Iteration: 179/214, Loss: 0.04033459350466728\n",
      "Iteration: 180/214, Loss: 0.003283258993178606\n",
      "Iteration: 181/214, Loss: 0.38086041808128357\n",
      "Iteration: 182/214, Loss: 0.05624879151582718\n",
      "Iteration: 183/214, Loss: 0.6581434011459351\n",
      "Iteration: 184/214, Loss: 0.43294578790664673\n",
      "Iteration: 185/214, Loss: 0.17965944111347198\n",
      "Iteration: 186/214, Loss: 0.1283993273973465\n",
      "Iteration: 187/214, Loss: 0.19087158143520355\n",
      "Iteration: 188/214, Loss: 0.29921814799308777\n",
      "Iteration: 189/214, Loss: 0.8499888777732849\n",
      "Iteration: 190/214, Loss: 0.3632400929927826\n",
      "Iteration: 191/214, Loss: 0.968284547328949\n",
      "Iteration: 192/214, Loss: 0.13330456614494324\n",
      "Iteration: 193/214, Loss: 0.5578508973121643\n",
      "Iteration: 194/214, Loss: 0.05244161933660507\n",
      "Iteration: 195/214, Loss: 0.13763324916362762\n",
      "Iteration: 196/214, Loss: 1.0969301462173462\n",
      "Iteration: 197/214, Loss: 0.43227699398994446\n",
      "Iteration: 198/214, Loss: 0.2923263907432556\n",
      "Iteration: 199/214, Loss: 0.10916925221681595\n",
      "Iteration: 200/214, Loss: 0.4785855710506439\n",
      "Iteration: 201/214, Loss: 0.2631223499774933\n",
      "Iteration: 202/214, Loss: 0.5370343327522278\n",
      "Iteration: 203/214, Loss: 0.06518742442131042\n",
      "Iteration: 204/214, Loss: 0.19021233916282654\n",
      "Iteration: 205/214, Loss: 0.3251894414424896\n",
      "Iteration: 206/214, Loss: 0.09176202118396759\n",
      "Iteration: 207/214, Loss: 0.7013934850692749\n",
      "Iteration: 208/214, Loss: 0.15945792198181152\n",
      "Iteration: 209/214, Loss: 0.2255953997373581\n",
      "Iteration: 210/214, Loss: 0.44532886147499084\n",
      "Iteration: 211/214, Loss: 0.15800800919532776\n",
      "Iteration: 212/214, Loss: 0.18466895818710327\n",
      "Iteration: 213/214, Loss: 0.43802544474601746\n",
      "Iteration: 214/214, Loss: 0.2053259313106537\n",
      "tensor(59.3637, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Iteration: 1/214, Loss: 0.09922712296247482\n",
      "Iteration: 2/214, Loss: 0.10164760798215866\n",
      "Iteration: 3/214, Loss: 0.15530869364738464\n",
      "Iteration: 4/214, Loss: 0.9169385433197021\n",
      "Iteration: 5/214, Loss: 0.1061910092830658\n",
      "Iteration: 6/214, Loss: 0.09880844503641129\n",
      "Iteration: 7/214, Loss: 0.27237066626548767\n",
      "Iteration: 8/214, Loss: 0.1016278862953186\n",
      "Iteration: 9/214, Loss: 0.18734626471996307\n",
      "Iteration: 10/214, Loss: 0.5898347496986389\n",
      "Iteration: 11/214, Loss: 0.2135312259197235\n",
      "Iteration: 12/214, Loss: 0.31645604968070984\n",
      "Iteration: 13/214, Loss: 0.014602302573621273\n",
      "Iteration: 14/214, Loss: 0.8283609747886658\n",
      "Iteration: 15/214, Loss: 0.061052560806274414\n",
      "Iteration: 16/214, Loss: 0.07913170009851456\n",
      "Iteration: 17/214, Loss: 2.3124313354492188\n",
      "Iteration: 18/214, Loss: 0.004837822634726763\n",
      "Iteration: 19/214, Loss: 0.00818563997745514\n",
      "Iteration: 20/214, Loss: 0.4819490909576416\n",
      "Iteration: 21/214, Loss: 0.13432709872722626\n",
      "Iteration: 22/214, Loss: 0.2652917504310608\n",
      "Iteration: 23/214, Loss: 0.060617003589868546\n",
      "Iteration: 24/214, Loss: 0.5495071411132812\n",
      "Iteration: 25/214, Loss: 0.17844681441783905\n",
      "Iteration: 26/214, Loss: 0.317931205034256\n",
      "Iteration: 27/214, Loss: 0.3851412236690521\n",
      "Iteration: 28/214, Loss: 0.28192535042762756\n",
      "Iteration: 29/214, Loss: 0.25707170367240906\n",
      "Iteration: 30/214, Loss: 0.3277778923511505\n",
      "Iteration: 31/214, Loss: 0.04457306116819382\n",
      "Iteration: 32/214, Loss: 0.29287680983543396\n",
      "Iteration: 33/214, Loss: 0.004992697853595018\n",
      "Iteration: 34/214, Loss: 0.2600509226322174\n",
      "Iteration: 35/214, Loss: 0.0691416934132576\n",
      "Iteration: 36/214, Loss: 0.14088943600654602\n",
      "Iteration: 37/214, Loss: 0.24422869086265564\n",
      "Iteration: 38/214, Loss: 0.23812508583068848\n",
      "Iteration: 39/214, Loss: 0.34265515208244324\n",
      "Iteration: 40/214, Loss: 0.8469827175140381\n",
      "Iteration: 41/214, Loss: 0.12581296265125275\n",
      "Iteration: 42/214, Loss: 0.40940114855766296\n",
      "Iteration: 43/214, Loss: 0.433342844247818\n",
      "Iteration: 44/214, Loss: 0.013313224539160728\n",
      "Iteration: 45/214, Loss: 0.6579312086105347\n",
      "Iteration: 46/214, Loss: 0.06864433735609055\n",
      "Iteration: 47/214, Loss: 0.511425256729126\n",
      "Iteration: 48/214, Loss: 0.06881421059370041\n",
      "Iteration: 49/214, Loss: 0.08282923698425293\n",
      "Iteration: 50/214, Loss: 0.05382431298494339\n",
      "Iteration: 51/214, Loss: 0.21977955102920532\n",
      "Iteration: 52/214, Loss: 0.5965118408203125\n",
      "Iteration: 53/214, Loss: 0.12331075966358185\n",
      "Iteration: 54/214, Loss: 0.052580755203962326\n",
      "Iteration: 55/214, Loss: 0.30723118782043457\n",
      "Iteration: 56/214, Loss: 0.06964930891990662\n",
      "Iteration: 57/214, Loss: 0.09588377177715302\n",
      "Iteration: 58/214, Loss: 0.08646193146705627\n",
      "Iteration: 59/214, Loss: 0.6804056167602539\n",
      "Iteration: 60/214, Loss: 0.0053017232567071915\n",
      "Iteration: 61/214, Loss: 0.4974953532218933\n",
      "Iteration: 62/214, Loss: 0.06655970215797424\n",
      "Iteration: 63/214, Loss: 0.004538681358098984\n",
      "Iteration: 64/214, Loss: 0.08664520829916\n",
      "Iteration: 65/214, Loss: 0.4326763153076172\n",
      "Iteration: 66/214, Loss: 0.6458625793457031\n",
      "Iteration: 67/214, Loss: 0.4507721960544586\n",
      "Iteration: 68/214, Loss: 0.152495339512825\n",
      "Iteration: 69/214, Loss: 0.24044033885002136\n",
      "Iteration: 70/214, Loss: 0.2181447297334671\n",
      "Iteration: 71/214, Loss: 0.670561671257019\n",
      "Iteration: 72/214, Loss: 0.39259031414985657\n",
      "Iteration: 73/214, Loss: 0.30411186814308167\n",
      "Iteration: 74/214, Loss: 0.22641190886497498\n",
      "Iteration: 75/214, Loss: 0.5020068883895874\n",
      "Iteration: 76/214, Loss: 0.02904229797422886\n",
      "Iteration: 77/214, Loss: 0.16605964303016663\n",
      "Iteration: 78/214, Loss: 0.19384385645389557\n",
      "Iteration: 79/214, Loss: 0.24698571860790253\n",
      "Iteration: 80/214, Loss: 0.14033332467079163\n",
      "Iteration: 81/214, Loss: 0.21944454312324524\n",
      "Iteration: 82/214, Loss: 0.05892046540975571\n",
      "Iteration: 83/214, Loss: 0.036584317684173584\n",
      "Iteration: 84/214, Loss: 0.032511480152606964\n",
      "Iteration: 85/214, Loss: 0.33352985978126526\n",
      "Iteration: 86/214, Loss: 0.13062486052513123\n",
      "Iteration: 87/214, Loss: 0.3465292155742645\n",
      "Iteration: 88/214, Loss: 0.01425394881516695\n",
      "Iteration: 89/214, Loss: 0.0463566817343235\n",
      "Iteration: 90/214, Loss: 0.07189666479825974\n",
      "Iteration: 91/214, Loss: 0.05294245481491089\n",
      "Iteration: 92/214, Loss: 0.19134238362312317\n",
      "Iteration: 93/214, Loss: 0.4001040458679199\n",
      "Iteration: 94/214, Loss: 0.10753710567951202\n",
      "Iteration: 95/214, Loss: 0.4299553334712982\n",
      "Iteration: 96/214, Loss: 0.0824739933013916\n",
      "Iteration: 97/214, Loss: 0.19764962792396545\n",
      "Iteration: 98/214, Loss: 0.63779616355896\n",
      "Iteration: 99/214, Loss: 0.1279914528131485\n",
      "Iteration: 100/214, Loss: 0.1873352825641632\n",
      "Iteration: 101/214, Loss: 0.16977764666080475\n",
      "Iteration: 102/214, Loss: 0.009504131972789764\n",
      "Iteration: 103/214, Loss: 0.49202555418014526\n",
      "Iteration: 104/214, Loss: 0.18076692521572113\n",
      "Iteration: 105/214, Loss: 0.1989329308271408\n",
      "Iteration: 106/214, Loss: 0.004919903352856636\n",
      "Iteration: 107/214, Loss: 0.6195754408836365\n",
      "Iteration: 108/214, Loss: 0.7583548426628113\n",
      "Iteration: 109/214, Loss: 0.12274385988712311\n",
      "Iteration: 110/214, Loss: 0.12185671180486679\n",
      "Iteration: 111/214, Loss: 0.5929415822029114\n",
      "Iteration: 112/214, Loss: 0.21168193221092224\n",
      "Iteration: 113/214, Loss: 0.19474300742149353\n",
      "Iteration: 114/214, Loss: 0.12292077392339706\n",
      "Iteration: 115/214, Loss: 0.03812052309513092\n",
      "Iteration: 116/214, Loss: 0.5535475611686707\n",
      "Iteration: 117/214, Loss: 0.1917242407798767\n",
      "Iteration: 118/214, Loss: 0.06606156378984451\n",
      "Iteration: 119/214, Loss: 0.0783512070775032\n",
      "Iteration: 120/214, Loss: 0.4085284471511841\n",
      "Iteration: 121/214, Loss: 0.05208881199359894\n",
      "Iteration: 122/214, Loss: 0.22273948788642883\n",
      "Iteration: 123/214, Loss: 0.14373673498630524\n",
      "Iteration: 124/214, Loss: 0.0470907986164093\n",
      "Iteration: 125/214, Loss: 0.2738129496574402\n",
      "Iteration: 126/214, Loss: 0.0545722097158432\n",
      "Iteration: 127/214, Loss: 0.06605516374111176\n",
      "Iteration: 128/214, Loss: 0.24800825119018555\n",
      "Iteration: 129/214, Loss: 0.3773272633552551\n",
      "Iteration: 130/214, Loss: 0.28249022364616394\n",
      "Iteration: 131/214, Loss: 0.044501349329948425\n",
      "Iteration: 132/214, Loss: 0.4366145730018616\n",
      "Iteration: 133/214, Loss: 0.007236076518893242\n",
      "Iteration: 134/214, Loss: 0.04206531494855881\n",
      "Iteration: 135/214, Loss: 0.06112179532647133\n",
      "Iteration: 136/214, Loss: 0.023560725152492523\n",
      "Iteration: 137/214, Loss: 0.007642444223165512\n",
      "Iteration: 138/214, Loss: 0.12396823614835739\n",
      "Iteration: 139/214, Loss: 0.06517693400382996\n",
      "Iteration: 140/214, Loss: 0.5642213821411133\n",
      "Iteration: 141/214, Loss: 0.06128861755132675\n",
      "Iteration: 142/214, Loss: 0.7185530066490173\n",
      "Iteration: 143/214, Loss: 0.027465354651212692\n",
      "Iteration: 144/214, Loss: 0.31187015771865845\n",
      "Iteration: 145/214, Loss: 0.08371397107839584\n",
      "Iteration: 146/214, Loss: 0.043601855635643005\n",
      "Iteration: 147/214, Loss: 0.42024388909339905\n",
      "Iteration: 148/214, Loss: 0.21059824526309967\n",
      "Iteration: 149/214, Loss: 0.04943280667066574\n",
      "Iteration: 150/214, Loss: 0.06778962165117264\n",
      "Iteration: 151/214, Loss: 0.040375739336013794\n",
      "Iteration: 152/214, Loss: 0.17898447811603546\n",
      "Iteration: 153/214, Loss: 0.08944641798734665\n",
      "Iteration: 154/214, Loss: 0.06088545545935631\n",
      "Iteration: 155/214, Loss: 0.6719509959220886\n",
      "Iteration: 156/214, Loss: 0.43659642338752747\n",
      "Iteration: 157/214, Loss: 0.0038740518502891064\n",
      "Iteration: 158/214, Loss: 0.14257173240184784\n",
      "Iteration: 159/214, Loss: 0.06676513701677322\n",
      "Iteration: 160/214, Loss: 0.242417111992836\n",
      "Iteration: 161/214, Loss: 0.49831685423851013\n",
      "Iteration: 162/214, Loss: 0.07853996753692627\n",
      "Iteration: 163/214, Loss: 0.2326471507549286\n",
      "Iteration: 164/214, Loss: 0.06494685262441635\n",
      "Iteration: 165/214, Loss: 0.4307364225387573\n",
      "Iteration: 166/214, Loss: 0.00347621226683259\n",
      "Iteration: 167/214, Loss: 0.028451954945921898\n",
      "Iteration: 168/214, Loss: 0.035265568643808365\n",
      "Iteration: 169/214, Loss: 0.41403326392173767\n",
      "Iteration: 170/214, Loss: 0.3927210867404938\n",
      "Iteration: 171/214, Loss: 0.6459246873855591\n",
      "Iteration: 172/214, Loss: 0.19179841876029968\n",
      "Iteration: 173/214, Loss: 0.08063606917858124\n",
      "Iteration: 174/214, Loss: 0.07581880688667297\n",
      "Iteration: 175/214, Loss: 0.3244851529598236\n",
      "Iteration: 176/214, Loss: 0.09215213358402252\n",
      "Iteration: 177/214, Loss: 0.3774384558200836\n",
      "Iteration: 178/214, Loss: 0.11392869800329208\n",
      "Iteration: 179/214, Loss: 0.04114759713411331\n",
      "Iteration: 180/214, Loss: 0.001346886157989502\n",
      "Iteration: 181/214, Loss: 0.2926321029663086\n",
      "Iteration: 182/214, Loss: 0.027983322739601135\n",
      "Iteration: 183/214, Loss: 0.4963943362236023\n",
      "Iteration: 184/214, Loss: 0.3696059286594391\n",
      "Iteration: 185/214, Loss: 0.13134858012199402\n",
      "Iteration: 186/214, Loss: 0.0763077586889267\n",
      "Iteration: 187/214, Loss: 0.1375551074743271\n",
      "Iteration: 188/214, Loss: 0.24056504666805267\n",
      "Iteration: 189/214, Loss: 0.7318758964538574\n",
      "Iteration: 190/214, Loss: 0.3067132830619812\n",
      "Iteration: 191/214, Loss: 0.7458265423774719\n",
      "Iteration: 192/214, Loss: 0.05835656821727753\n",
      "Iteration: 193/214, Loss: 0.39178594946861267\n",
      "Iteration: 194/214, Loss: 0.021217012777924538\n",
      "Iteration: 195/214, Loss: 0.12218931317329407\n",
      "Iteration: 196/214, Loss: 0.6135738492012024\n",
      "Iteration: 197/214, Loss: 0.3336414694786072\n",
      "Iteration: 198/214, Loss: 0.22648081183433533\n",
      "Iteration: 199/214, Loss: 0.05195606127381325\n",
      "Iteration: 200/214, Loss: 0.47591432929039\n",
      "Iteration: 201/214, Loss: 0.3263885974884033\n",
      "Iteration: 202/214, Loss: 0.5147106647491455\n",
      "Iteration: 203/214, Loss: 0.045686572790145874\n",
      "Iteration: 204/214, Loss: 0.2712840735912323\n",
      "Iteration: 205/214, Loss: 0.39690402150154114\n",
      "Iteration: 206/214, Loss: 0.06450768560171127\n",
      "Iteration: 207/214, Loss: 0.5318924188613892\n",
      "Iteration: 208/214, Loss: 0.056715600192546844\n",
      "Iteration: 209/214, Loss: 0.18777132034301758\n",
      "Iteration: 210/214, Loss: 0.4287601709365845\n",
      "Iteration: 211/214, Loss: 0.20759016275405884\n",
      "Iteration: 212/214, Loss: 0.21542252600193024\n",
      "Iteration: 213/214, Loss: 0.40201419591903687\n",
      "Iteration: 214/214, Loss: 0.18323996663093567\n",
      "tensor(52.2464, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Iteration: 1/214, Loss: 0.06028728187084198\n",
      "Iteration: 2/214, Loss: 0.048903610557317734\n",
      "Iteration: 3/214, Loss: 0.053826749324798584\n",
      "Iteration: 4/214, Loss: 0.535703182220459\n",
      "Iteration: 5/214, Loss: 0.06458255648612976\n",
      "Iteration: 6/214, Loss: 0.05884163826704025\n",
      "Iteration: 7/214, Loss: 0.21128740906715393\n",
      "Iteration: 8/214, Loss: 0.0824398621916771\n",
      "Iteration: 9/214, Loss: 0.18633122742176056\n",
      "Iteration: 10/214, Loss: 0.6179928779602051\n",
      "Iteration: 11/214, Loss: 0.19455568492412567\n",
      "Iteration: 12/214, Loss: 0.3130767345428467\n",
      "Iteration: 13/214, Loss: 0.001923318486660719\n",
      "Iteration: 14/214, Loss: 0.7696413397789001\n",
      "Iteration: 15/214, Loss: 0.042701251804828644\n",
      "Iteration: 16/214, Loss: 0.059551503509283066\n",
      "Iteration: 17/214, Loss: 1.2740583419799805\n",
      "Iteration: 18/214, Loss: 0.0015561591135337949\n",
      "Iteration: 19/214, Loss: 0.0023786018136888742\n",
      "Iteration: 20/214, Loss: 0.5252413749694824\n",
      "Iteration: 21/214, Loss: 0.03338157385587692\n",
      "Iteration: 22/214, Loss: 0.2445552945137024\n",
      "Iteration: 23/214, Loss: 0.06310218572616577\n",
      "Iteration: 24/214, Loss: 0.40711599588394165\n",
      "Iteration: 25/214, Loss: 0.14265292882919312\n",
      "Iteration: 26/214, Loss: 0.2089010775089264\n",
      "Iteration: 27/214, Loss: 0.2205713987350464\n",
      "Iteration: 28/214, Loss: 0.277438759803772\n",
      "Iteration: 29/214, Loss: 0.2126360833644867\n",
      "Iteration: 30/214, Loss: 0.2918219268321991\n",
      "Iteration: 31/214, Loss: 0.06198487430810928\n",
      "Iteration: 32/214, Loss: 0.2052087038755417\n",
      "Iteration: 33/214, Loss: 0.002030411036685109\n",
      "Iteration: 34/214, Loss: 0.18932710587978363\n",
      "Iteration: 35/214, Loss: 0.05924283340573311\n",
      "Iteration: 36/214, Loss: 0.11210332065820694\n",
      "Iteration: 37/214, Loss: 0.25180521607398987\n",
      "Iteration: 38/214, Loss: 0.2807980179786682\n",
      "Iteration: 39/214, Loss: 0.47404342889785767\n",
      "Iteration: 40/214, Loss: 0.7159516215324402\n",
      "Iteration: 41/214, Loss: 0.10275054723024368\n",
      "Iteration: 42/214, Loss: 0.3800866901874542\n",
      "Iteration: 43/214, Loss: 0.2058943808078766\n",
      "Iteration: 44/214, Loss: 0.0009409411577507854\n",
      "Iteration: 45/214, Loss: 0.5749501585960388\n",
      "Iteration: 46/214, Loss: 0.03662881627678871\n",
      "Iteration: 47/214, Loss: 0.4759697914123535\n",
      "Iteration: 48/214, Loss: 0.057507582008838654\n",
      "Iteration: 49/214, Loss: 0.071494460105896\n",
      "Iteration: 50/214, Loss: 0.08530549705028534\n",
      "Iteration: 51/214, Loss: 0.16262087225914001\n",
      "Iteration: 52/214, Loss: 0.5215815901756287\n",
      "Iteration: 53/214, Loss: 0.128435418009758\n",
      "Iteration: 54/214, Loss: 0.058234743773937225\n",
      "Iteration: 55/214, Loss: 0.2621980309486389\n",
      "Iteration: 56/214, Loss: 0.07240279763936996\n",
      "Iteration: 57/214, Loss: 0.06474188715219498\n",
      "Iteration: 58/214, Loss: 0.056880686432123184\n",
      "Iteration: 59/214, Loss: 0.268602579832077\n",
      "Iteration: 60/214, Loss: 0.012765086255967617\n",
      "Iteration: 61/214, Loss: 0.33294007182121277\n",
      "Iteration: 62/214, Loss: 0.10997296124696732\n",
      "Iteration: 63/214, Loss: 0.006512870080769062\n",
      "Iteration: 64/214, Loss: 0.08209824562072754\n",
      "Iteration: 65/214, Loss: 0.5076537728309631\n",
      "Iteration: 66/214, Loss: 0.4482339322566986\n",
      "Iteration: 67/214, Loss: 0.4486670196056366\n",
      "Iteration: 68/214, Loss: 0.16969536244869232\n",
      "Iteration: 69/214, Loss: 0.15339195728302002\n",
      "Iteration: 70/214, Loss: 0.20011185109615326\n",
      "Iteration: 71/214, Loss: 0.6225244998931885\n",
      "Iteration: 72/214, Loss: 0.39898377656936646\n",
      "Iteration: 73/214, Loss: 0.22286933660507202\n",
      "Iteration: 74/214, Loss: 0.25623586773872375\n",
      "Iteration: 75/214, Loss: 0.4630652964115143\n",
      "Iteration: 76/214, Loss: 0.0351090207695961\n",
      "Iteration: 77/214, Loss: 0.13818135857582092\n",
      "Iteration: 78/214, Loss: 0.1608588844537735\n",
      "Iteration: 79/214, Loss: 0.20836538076400757\n",
      "Iteration: 80/214, Loss: 0.13824990391731262\n",
      "Iteration: 81/214, Loss: 0.15794388949871063\n",
      "Iteration: 82/214, Loss: 0.07612477988004684\n",
      "Iteration: 83/214, Loss: 0.05358579382300377\n",
      "Iteration: 84/214, Loss: 0.04694391041994095\n",
      "Iteration: 85/214, Loss: 0.29427123069763184\n",
      "Iteration: 86/214, Loss: 0.1662735491991043\n",
      "Iteration: 87/214, Loss: 0.27794528007507324\n",
      "Iteration: 88/214, Loss: 0.005711558274924755\n",
      "Iteration: 89/214, Loss: 0.07339993864297867\n",
      "Iteration: 90/214, Loss: 0.09865958243608475\n",
      "Iteration: 91/214, Loss: 0.07353035360574722\n",
      "Iteration: 92/214, Loss: 0.22610025107860565\n",
      "Iteration: 93/214, Loss: 0.4136117994785309\n",
      "Iteration: 94/214, Loss: 0.12184207886457443\n",
      "Iteration: 95/214, Loss: 0.4529152810573578\n",
      "Iteration: 96/214, Loss: 0.05836191400885582\n",
      "Iteration: 97/214, Loss: 0.17519758641719818\n",
      "Iteration: 98/214, Loss: 0.43519845604896545\n",
      "Iteration: 99/214, Loss: 0.11582571268081665\n",
      "Iteration: 100/214, Loss: 0.300284206867218\n",
      "Iteration: 101/214, Loss: 0.1997355818748474\n",
      "Iteration: 102/214, Loss: 0.0029929676093161106\n",
      "Iteration: 103/214, Loss: 0.47385138273239136\n",
      "Iteration: 104/214, Loss: 0.09818323701620102\n",
      "Iteration: 105/214, Loss: 0.2166849672794342\n",
      "Iteration: 106/214, Loss: 0.004827939905226231\n",
      "Iteration: 107/214, Loss: 0.5400275588035583\n",
      "Iteration: 108/214, Loss: 0.708594560623169\n",
      "Iteration: 109/214, Loss: 0.07411131262779236\n",
      "Iteration: 110/214, Loss: 0.16900168359279633\n",
      "Iteration: 111/214, Loss: 0.701272189617157\n",
      "Iteration: 112/214, Loss: 0.28228577971458435\n",
      "Iteration: 113/214, Loss: 0.3567582964897156\n",
      "Iteration: 114/214, Loss: 0.2873847782611847\n",
      "Iteration: 115/214, Loss: 0.05843693017959595\n",
      "Iteration: 116/214, Loss: 0.536592960357666\n",
      "Iteration: 117/214, Loss: 0.2370956987142563\n",
      "Iteration: 118/214, Loss: 0.06146428361535072\n",
      "Iteration: 119/214, Loss: 0.10911034792661667\n",
      "Iteration: 120/214, Loss: 0.40473005175590515\n",
      "Iteration: 121/214, Loss: 0.06294727325439453\n",
      "Iteration: 122/214, Loss: 0.24105864763259888\n",
      "Iteration: 123/214, Loss: 0.14935073256492615\n",
      "Iteration: 124/214, Loss: 0.0762961357831955\n",
      "Iteration: 125/214, Loss: 0.2291461080312729\n",
      "Iteration: 126/214, Loss: 0.07013171911239624\n",
      "Iteration: 127/214, Loss: 0.060226697474718094\n",
      "Iteration: 128/214, Loss: 0.29656726121902466\n",
      "Iteration: 129/214, Loss: 0.5224220156669617\n",
      "Iteration: 130/214, Loss: 0.44475868344306946\n",
      "Iteration: 131/214, Loss: 0.043047159910202026\n",
      "Iteration: 132/214, Loss: 0.41453421115875244\n",
      "Iteration: 133/214, Loss: 0.0031439554877579212\n",
      "Iteration: 134/214, Loss: 0.0898871049284935\n",
      "Iteration: 135/214, Loss: 0.10721860080957413\n",
      "Iteration: 136/214, Loss: 0.0018794736824929714\n",
      "Iteration: 137/214, Loss: 0.004631709307432175\n",
      "Iteration: 138/214, Loss: 0.10474509000778198\n",
      "Iteration: 139/214, Loss: 0.11584312468767166\n",
      "Iteration: 140/214, Loss: 0.5624249577522278\n",
      "Iteration: 141/214, Loss: 0.12519408762454987\n",
      "Iteration: 142/214, Loss: 0.8572673201560974\n",
      "Iteration: 143/214, Loss: 0.07668901979923248\n",
      "Iteration: 144/214, Loss: 0.30215564370155334\n",
      "Iteration: 145/214, Loss: 0.10892300307750702\n",
      "Iteration: 146/214, Loss: 0.0618971548974514\n",
      "Iteration: 147/214, Loss: 0.338834673166275\n",
      "Iteration: 148/214, Loss: 0.18631447851657867\n",
      "Iteration: 149/214, Loss: 0.08226083964109421\n",
      "Iteration: 150/214, Loss: 0.05996345356106758\n",
      "Iteration: 151/214, Loss: 0.06704141199588776\n",
      "Iteration: 152/214, Loss: 0.17316915094852448\n",
      "Iteration: 153/214, Loss: 0.07323678582906723\n",
      "Iteration: 154/214, Loss: 0.03968089073896408\n",
      "Iteration: 155/214, Loss: 0.5430388450622559\n",
      "Iteration: 156/214, Loss: 0.4296957552433014\n",
      "Iteration: 157/214, Loss: 0.002622353844344616\n",
      "Iteration: 158/214, Loss: 0.11020040512084961\n",
      "Iteration: 159/214, Loss: 0.04481988772749901\n",
      "Iteration: 160/214, Loss: 0.13381175696849823\n",
      "Iteration: 161/214, Loss: 0.4516459107398987\n",
      "Iteration: 162/214, Loss: 0.052571289241313934\n",
      "Iteration: 163/214, Loss: 0.17197075486183167\n",
      "Iteration: 164/214, Loss: 0.06997878104448318\n",
      "Iteration: 165/214, Loss: 0.46654143929481506\n",
      "Iteration: 166/214, Loss: 0.003401853609830141\n",
      "Iteration: 167/214, Loss: 0.06380681693553925\n",
      "Iteration: 168/214, Loss: 0.048370327800512314\n",
      "Iteration: 169/214, Loss: 0.4454883635044098\n",
      "Iteration: 170/214, Loss: 0.33725038170814514\n",
      "Iteration: 171/214, Loss: 0.5816066861152649\n",
      "Iteration: 172/214, Loss: 0.12011224031448364\n",
      "Iteration: 173/214, Loss: 0.052721694111824036\n",
      "Iteration: 174/214, Loss: 0.05274158716201782\n",
      "Iteration: 175/214, Loss: 0.32524919509887695\n",
      "Iteration: 176/214, Loss: 0.15024742484092712\n",
      "Iteration: 177/214, Loss: 0.26273271441459656\n",
      "Iteration: 178/214, Loss: 0.1180095374584198\n",
      "Iteration: 179/214, Loss: 0.030362920835614204\n",
      "Iteration: 180/214, Loss: 0.0008910991018638015\n",
      "Iteration: 181/214, Loss: 0.24241122603416443\n",
      "Iteration: 182/214, Loss: 0.03160545229911804\n",
      "Iteration: 183/214, Loss: 0.4049960672855377\n",
      "Iteration: 184/214, Loss: 0.32032978534698486\n",
      "Iteration: 185/214, Loss: 0.13018064200878143\n",
      "Iteration: 186/214, Loss: 0.1123572289943695\n",
      "Iteration: 187/214, Loss: 0.09848485141992569\n",
      "Iteration: 188/214, Loss: 0.20853182673454285\n",
      "Iteration: 189/214, Loss: 0.6505903601646423\n",
      "Iteration: 190/214, Loss: 0.3216083347797394\n",
      "Iteration: 191/214, Loss: 0.4722214639186859\n",
      "Iteration: 192/214, Loss: 0.07371589541435242\n",
      "Iteration: 193/214, Loss: 0.32267630100250244\n",
      "Iteration: 194/214, Loss: 0.011628328822553158\n",
      "Iteration: 195/214, Loss: 0.08989103138446808\n",
      "Iteration: 196/214, Loss: 0.4418002963066101\n",
      "Iteration: 197/214, Loss: 0.3204638659954071\n",
      "Iteration: 198/214, Loss: 0.2050488293170929\n",
      "Iteration: 199/214, Loss: 0.04039112105965614\n",
      "Iteration: 200/214, Loss: 0.3249000310897827\n",
      "Iteration: 201/214, Loss: 0.23547692596912384\n",
      "Iteration: 202/214, Loss: 0.4626052677631378\n",
      "Iteration: 203/214, Loss: 0.039873555302619934\n",
      "Iteration: 204/214, Loss: 0.17539864778518677\n",
      "Iteration: 205/214, Loss: 0.3382890522480011\n",
      "Iteration: 206/214, Loss: 0.04682542011141777\n",
      "Iteration: 207/214, Loss: 0.4714491367340088\n",
      "Iteration: 208/214, Loss: 0.0528888925909996\n",
      "Iteration: 209/214, Loss: 0.1850268840789795\n",
      "Iteration: 210/214, Loss: 0.39984455704689026\n",
      "Iteration: 211/214, Loss: 0.13852128386497498\n",
      "Iteration: 212/214, Loss: 0.13158507645130157\n",
      "Iteration: 213/214, Loss: 0.3848002552986145\n",
      "Iteration: 214/214, Loss: 0.1271529495716095\n",
      "tensor(46.6465, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Iteration: 1/214, Loss: 0.05914133042097092\n",
      "Iteration: 2/214, Loss: 0.0475754514336586\n",
      "Iteration: 3/214, Loss: 0.07248269766569138\n",
      "Iteration: 4/214, Loss: 0.5169919729232788\n",
      "Iteration: 5/214, Loss: 0.04055920988321304\n",
      "Iteration: 6/214, Loss: 0.04052494466304779\n",
      "Iteration: 7/214, Loss: 0.23879919946193695\n",
      "Iteration: 8/214, Loss: 0.06259540468454361\n",
      "Iteration: 9/214, Loss: 0.15115711092948914\n",
      "Iteration: 10/214, Loss: 0.48370569944381714\n",
      "Iteration: 11/214, Loss: 0.20182925462722778\n",
      "Iteration: 12/214, Loss: 0.262315034866333\n",
      "Iteration: 13/214, Loss: 0.0021011431235820055\n",
      "Iteration: 14/214, Loss: 0.6935428977012634\n",
      "Iteration: 15/214, Loss: 0.04913008213043213\n",
      "Iteration: 16/214, Loss: 0.044713620096445084\n",
      "Iteration: 17/214, Loss: 0.9414187073707581\n",
      "Iteration: 18/214, Loss: 0.002113986061885953\n",
      "Iteration: 19/214, Loss: 0.0010318562854081392\n",
      "Iteration: 20/214, Loss: 0.3923933506011963\n",
      "Iteration: 21/214, Loss: 0.04805333539843559\n",
      "Iteration: 22/214, Loss: 0.2020740509033203\n",
      "Iteration: 23/214, Loss: 0.04413142800331116\n",
      "Iteration: 24/214, Loss: 0.32657796144485474\n",
      "Iteration: 25/214, Loss: 0.10266178101301193\n",
      "Iteration: 26/214, Loss: 0.18327613174915314\n",
      "Iteration: 27/214, Loss: 0.16227702796459198\n",
      "Iteration: 28/214, Loss: 0.22099056839942932\n",
      "Iteration: 29/214, Loss: 0.20400302112102509\n",
      "Iteration: 30/214, Loss: 0.281669020652771\n",
      "Iteration: 31/214, Loss: 0.02774067036807537\n",
      "Iteration: 32/214, Loss: 0.15270563960075378\n",
      "Iteration: 33/214, Loss: 0.0022335550747811794\n",
      "Iteration: 34/214, Loss: 0.1798212081193924\n",
      "Iteration: 35/214, Loss: 0.04238258674740791\n",
      "Iteration: 36/214, Loss: 0.09058284014463425\n",
      "Iteration: 37/214, Loss: 0.17056682705879211\n",
      "Iteration: 38/214, Loss: 0.24587145447731018\n",
      "Iteration: 39/214, Loss: 0.29279494285583496\n",
      "Iteration: 40/214, Loss: 0.6233378648757935\n",
      "Iteration: 41/214, Loss: 0.09679292142391205\n",
      "Iteration: 42/214, Loss: 0.24854391813278198\n",
      "Iteration: 43/214, Loss: 0.17972783744335175\n",
      "Iteration: 44/214, Loss: 0.0012573811691254377\n",
      "Iteration: 45/214, Loss: 0.4411214292049408\n",
      "Iteration: 46/214, Loss: 0.02971324510872364\n",
      "Iteration: 47/214, Loss: 0.44638895988464355\n",
      "Iteration: 48/214, Loss: 0.034563228487968445\n",
      "Iteration: 49/214, Loss: 0.04315496236085892\n",
      "Iteration: 50/214, Loss: 0.04853716492652893\n",
      "Iteration: 51/214, Loss: 0.11224088817834854\n",
      "Iteration: 52/214, Loss: 0.2974330186843872\n",
      "Iteration: 53/214, Loss: 0.07915730774402618\n",
      "Iteration: 54/214, Loss: 0.06196364387869835\n",
      "Iteration: 55/214, Loss: 0.2221694141626358\n",
      "Iteration: 56/214, Loss: 0.05378549173474312\n",
      "Iteration: 57/214, Loss: 0.10985258966684341\n",
      "Iteration: 58/214, Loss: 0.04627135023474693\n",
      "Iteration: 59/214, Loss: 0.26731932163238525\n",
      "Iteration: 60/214, Loss: 0.014367622323334217\n",
      "Iteration: 61/214, Loss: 0.24418005347251892\n",
      "Iteration: 62/214, Loss: 0.09142762422561646\n",
      "Iteration: 63/214, Loss: 0.0022965082898736\n",
      "Iteration: 64/214, Loss: 0.07559695094823837\n",
      "Iteration: 65/214, Loss: 0.3534901440143585\n",
      "Iteration: 66/214, Loss: 0.5282658338546753\n",
      "Iteration: 67/214, Loss: 0.3178839683532715\n",
      "Iteration: 68/214, Loss: 0.057286035269498825\n",
      "Iteration: 69/214, Loss: 0.11252067983150482\n",
      "Iteration: 70/214, Loss: 0.1291383057832718\n",
      "Iteration: 71/214, Loss: 0.47523579001426697\n",
      "Iteration: 72/214, Loss: 0.3006628751754761\n",
      "Iteration: 73/214, Loss: 0.22117163240909576\n",
      "Iteration: 74/214, Loss: 0.1786612570285797\n",
      "Iteration: 75/214, Loss: 0.3826165199279785\n",
      "Iteration: 76/214, Loss: 0.03253442049026489\n",
      "Iteration: 77/214, Loss: 0.10733456909656525\n",
      "Iteration: 78/214, Loss: 0.1657957136631012\n",
      "Iteration: 79/214, Loss: 0.11544639617204666\n",
      "Iteration: 80/214, Loss: 0.13962745666503906\n",
      "Iteration: 81/214, Loss: 0.10960711538791656\n",
      "Iteration: 82/214, Loss: 0.07593470066785812\n",
      "Iteration: 83/214, Loss: 0.06772688031196594\n",
      "Iteration: 84/214, Loss: 0.04397255554795265\n",
      "Iteration: 85/214, Loss: 0.26834341883659363\n",
      "Iteration: 86/214, Loss: 0.22697113454341888\n",
      "Iteration: 87/214, Loss: 0.26803845167160034\n",
      "Iteration: 88/214, Loss: 0.0018668662523850799\n",
      "Iteration: 89/214, Loss: 0.051413729786872864\n",
      "Iteration: 90/214, Loss: 0.058642514050006866\n",
      "Iteration: 91/214, Loss: 0.037237975746393204\n",
      "Iteration: 92/214, Loss: 0.10153219848871231\n",
      "Iteration: 93/214, Loss: 0.2909087836742401\n",
      "Iteration: 94/214, Loss: 0.09771799296140671\n",
      "Iteration: 95/214, Loss: 0.39035406708717346\n",
      "Iteration: 96/214, Loss: 0.06020670756697655\n",
      "Iteration: 97/214, Loss: 0.11395994573831558\n",
      "Iteration: 98/214, Loss: 0.41813331842422485\n",
      "Iteration: 99/214, Loss: 0.07275326550006866\n",
      "Iteration: 100/214, Loss: 0.1506207138299942\n",
      "Iteration: 101/214, Loss: 0.09567766636610031\n",
      "Iteration: 102/214, Loss: 0.0033863140270113945\n",
      "Iteration: 103/214, Loss: 0.35728052258491516\n",
      "Iteration: 104/214, Loss: 0.08322681486606598\n",
      "Iteration: 105/214, Loss: 0.22150440514087677\n",
      "Iteration: 106/214, Loss: 0.0014982444699853659\n",
      "Iteration: 107/214, Loss: 0.5456526279449463\n",
      "Iteration: 108/214, Loss: 0.5767527222633362\n",
      "Iteration: 109/214, Loss: 0.058897070586681366\n",
      "Iteration: 110/214, Loss: 0.12190724909305573\n",
      "Iteration: 111/214, Loss: 0.5151103734970093\n",
      "Iteration: 112/214, Loss: 0.16107578575611115\n",
      "Iteration: 113/214, Loss: 0.29650989174842834\n",
      "Iteration: 114/214, Loss: 0.15748977661132812\n",
      "Iteration: 115/214, Loss: 0.0670771524310112\n",
      "Iteration: 116/214, Loss: 0.4188555181026459\n",
      "Iteration: 117/214, Loss: 0.1903184950351715\n",
      "Iteration: 118/214, Loss: 0.06203749403357506\n",
      "Iteration: 119/214, Loss: 0.09217670559883118\n",
      "Iteration: 120/214, Loss: 0.35114672780036926\n",
      "Iteration: 121/214, Loss: 0.03346231207251549\n",
      "Iteration: 122/214, Loss: 0.18061864376068115\n",
      "Iteration: 123/214, Loss: 0.0767534077167511\n",
      "Iteration: 124/214, Loss: 0.04810895025730133\n",
      "Iteration: 125/214, Loss: 0.15943869948387146\n",
      "Iteration: 126/214, Loss: 0.04097084328532219\n",
      "Iteration: 127/214, Loss: 0.06227780133485794\n",
      "Iteration: 128/214, Loss: 0.2601277530193329\n",
      "Iteration: 129/214, Loss: 0.34532904624938965\n",
      "Iteration: 130/214, Loss: 0.3136071562767029\n",
      "Iteration: 131/214, Loss: 0.030224716290831566\n",
      "Iteration: 132/214, Loss: 0.3762243986129761\n",
      "Iteration: 133/214, Loss: 0.0026661395095288754\n",
      "Iteration: 134/214, Loss: 0.05108746886253357\n",
      "Iteration: 135/214, Loss: 0.06600189954042435\n",
      "Iteration: 136/214, Loss: 0.0010094003519043326\n",
      "Iteration: 137/214, Loss: 0.0020538419485092163\n",
      "Iteration: 138/214, Loss: 0.10087524354457855\n",
      "Iteration: 139/214, Loss: 0.0829019621014595\n",
      "Iteration: 140/214, Loss: 0.3929291367530823\n",
      "Iteration: 141/214, Loss: 0.06082633137702942\n",
      "Iteration: 142/214, Loss: 0.5200999975204468\n",
      "Iteration: 143/214, Loss: 0.06417930126190186\n",
      "Iteration: 144/214, Loss: 0.22827351093292236\n",
      "Iteration: 145/214, Loss: 0.0738515853881836\n",
      "Iteration: 146/214, Loss: 0.036324143409729004\n",
      "Iteration: 147/214, Loss: 0.31151071190834045\n",
      "Iteration: 148/214, Loss: 0.16088107228279114\n",
      "Iteration: 149/214, Loss: 0.022756651043891907\n",
      "Iteration: 150/214, Loss: 0.08822016417980194\n",
      "Iteration: 151/214, Loss: 0.04133865237236023\n",
      "Iteration: 152/214, Loss: 0.15423409640789032\n",
      "Iteration: 153/214, Loss: 0.05932832509279251\n",
      "Iteration: 154/214, Loss: 0.031828176230192184\n",
      "Iteration: 155/214, Loss: 0.49431705474853516\n",
      "Iteration: 156/214, Loss: 0.30647900700569153\n",
      "Iteration: 157/214, Loss: 0.003035571426153183\n",
      "Iteration: 158/214, Loss: 0.11908835172653198\n",
      "Iteration: 159/214, Loss: 0.046574994921684265\n",
      "Iteration: 160/214, Loss: 0.18907754123210907\n",
      "Iteration: 161/214, Loss: 0.39203739166259766\n",
      "Iteration: 162/214, Loss: 0.04412524774670601\n",
      "Iteration: 163/214, Loss: 0.15916450321674347\n",
      "Iteration: 164/214, Loss: 0.05243157967925072\n",
      "Iteration: 165/214, Loss: 0.39655834436416626\n",
      "Iteration: 166/214, Loss: 0.004482093267142773\n",
      "Iteration: 167/214, Loss: 0.04820283129811287\n",
      "Iteration: 168/214, Loss: 0.028428424149751663\n",
      "Iteration: 169/214, Loss: 0.39652180671691895\n",
      "Iteration: 170/214, Loss: 0.2753046751022339\n",
      "Iteration: 171/214, Loss: 0.4022650718688965\n",
      "Iteration: 172/214, Loss: 0.13173489272594452\n",
      "Iteration: 173/214, Loss: 0.06465771794319153\n",
      "Iteration: 174/214, Loss: 0.04384981468319893\n",
      "Iteration: 175/214, Loss: 0.36317959427833557\n",
      "Iteration: 176/214, Loss: 0.0887005627155304\n",
      "Iteration: 177/214, Loss: 0.21208429336547852\n",
      "Iteration: 178/214, Loss: 0.07922551780939102\n",
      "Iteration: 179/214, Loss: 0.022985117509961128\n",
      "Iteration: 180/214, Loss: 0.0003440515720285475\n",
      "Iteration: 181/214, Loss: 0.1733422428369522\n",
      "Iteration: 182/214, Loss: 0.03464341163635254\n",
      "Iteration: 183/214, Loss: 0.3669007420539856\n",
      "Iteration: 184/214, Loss: 0.28093162178993225\n",
      "Iteration: 185/214, Loss: 0.08867934346199036\n",
      "Iteration: 186/214, Loss: 0.09935475140810013\n",
      "Iteration: 187/214, Loss: 0.10190509259700775\n",
      "Iteration: 188/214, Loss: 0.20972439646720886\n",
      "Iteration: 189/214, Loss: 0.5144216418266296\n",
      "Iteration: 190/214, Loss: 0.25022831559181213\n",
      "Iteration: 191/214, Loss: 0.524229884147644\n",
      "Iteration: 192/214, Loss: 0.05324956402182579\n",
      "Iteration: 193/214, Loss: 0.30222028493881226\n",
      "Iteration: 194/214, Loss: 0.01065063662827015\n",
      "Iteration: 195/214, Loss: 0.11431267857551575\n",
      "Iteration: 196/214, Loss: 0.3199089467525482\n",
      "Iteration: 197/214, Loss: 0.28569740056991577\n",
      "Iteration: 198/214, Loss: 0.24092786014080048\n",
      "Iteration: 199/214, Loss: 0.043365053832530975\n",
      "Iteration: 200/214, Loss: 0.28954562544822693\n",
      "Iteration: 201/214, Loss: 0.1695862114429474\n",
      "Iteration: 202/214, Loss: 0.411349892616272\n",
      "Iteration: 203/214, Loss: 0.03869924694299698\n",
      "Iteration: 204/214, Loss: 0.2647462785243988\n",
      "Iteration: 205/214, Loss: 0.349176824092865\n",
      "Iteration: 206/214, Loss: 0.03586870804429054\n",
      "Iteration: 207/214, Loss: 0.45301440358161926\n",
      "Iteration: 208/214, Loss: 0.042073335498571396\n",
      "Iteration: 209/214, Loss: 0.12104204297065735\n",
      "Iteration: 210/214, Loss: 0.2621874511241913\n",
      "Iteration: 211/214, Loss: 0.12113989889621735\n",
      "Iteration: 212/214, Loss: 0.09770895540714264\n",
      "Iteration: 213/214, Loss: 0.2781524956226349\n",
      "Iteration: 214/214, Loss: 0.0749502182006836\n",
      "tensor(37.9261, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Iteration: 1/214, Loss: 0.07536940276622772\n",
      "Iteration: 2/214, Loss: 0.06961444765329361\n",
      "Iteration: 3/214, Loss: 0.06596968322992325\n",
      "Iteration: 4/214, Loss: 0.5406741499900818\n",
      "Iteration: 5/214, Loss: 0.06482281535863876\n",
      "Iteration: 6/214, Loss: 0.060435011982917786\n",
      "Iteration: 7/214, Loss: 0.21607226133346558\n",
      "Iteration: 8/214, Loss: 0.03574327379465103\n",
      "Iteration: 9/214, Loss: 0.11931776255369186\n",
      "Iteration: 10/214, Loss: 0.40648773312568665\n",
      "Iteration: 11/214, Loss: 0.16848787665367126\n",
      "Iteration: 12/214, Loss: 0.24292688071727753\n",
      "Iteration: 13/214, Loss: 0.001594801084138453\n",
      "Iteration: 14/214, Loss: 0.7271807789802551\n",
      "Iteration: 15/214, Loss: 0.0638648122549057\n",
      "Iteration: 16/214, Loss: 0.06115799769759178\n",
      "Iteration: 17/214, Loss: 1.123814582824707\n",
      "Iteration: 18/214, Loss: 0.0009668200509622693\n",
      "Iteration: 19/214, Loss: 0.0007286024047061801\n",
      "Iteration: 20/214, Loss: 0.37790077924728394\n",
      "Iteration: 21/214, Loss: 0.04833615571260452\n",
      "Iteration: 22/214, Loss: 0.14955706894397736\n",
      "Iteration: 23/214, Loss: 0.03929903730750084\n",
      "Iteration: 24/214, Loss: 0.2907361388206482\n",
      "Iteration: 25/214, Loss: 0.0973806083202362\n",
      "Iteration: 26/214, Loss: 0.12446610629558563\n",
      "Iteration: 27/214, Loss: 0.16934651136398315\n",
      "Iteration: 28/214, Loss: 0.18650855123996735\n",
      "Iteration: 29/214, Loss: 0.1673678755760193\n",
      "Iteration: 30/214, Loss: 0.32828593254089355\n",
      "Iteration: 31/214, Loss: 0.0596337616443634\n",
      "Iteration: 32/214, Loss: 0.12876161932945251\n",
      "Iteration: 33/214, Loss: 0.002844511065632105\n",
      "Iteration: 34/214, Loss: 0.1704062968492508\n",
      "Iteration: 35/214, Loss: 0.05393707752227783\n",
      "Iteration: 36/214, Loss: 0.07642871886491776\n",
      "Iteration: 37/214, Loss: 0.26954013109207153\n",
      "Iteration: 38/214, Loss: 0.20584332942962646\n",
      "Iteration: 39/214, Loss: 0.3789902329444885\n",
      "Iteration: 40/214, Loss: 0.5665116906166077\n",
      "Iteration: 41/214, Loss: 0.09341879189014435\n",
      "Iteration: 42/214, Loss: 0.2569253742694855\n",
      "Iteration: 43/214, Loss: 0.1517021656036377\n",
      "Iteration: 44/214, Loss: 0.0008397611090913415\n",
      "Iteration: 45/214, Loss: 0.36236974596977234\n",
      "Iteration: 46/214, Loss: 0.05117302015423775\n",
      "Iteration: 47/214, Loss: 0.4013197720050812\n",
      "Iteration: 48/214, Loss: 0.03947383165359497\n",
      "Iteration: 49/214, Loss: 0.019248243421316147\n",
      "Iteration: 50/214, Loss: 0.036265283823013306\n",
      "Iteration: 51/214, Loss: 0.1387648582458496\n",
      "Iteration: 52/214, Loss: 0.3145655691623688\n",
      "Iteration: 53/214, Loss: 0.10502111166715622\n",
      "Iteration: 54/214, Loss: 0.05906436964869499\n",
      "Iteration: 55/214, Loss: 0.23133760690689087\n",
      "Iteration: 56/214, Loss: 0.0295401681214571\n",
      "Iteration: 57/214, Loss: 0.0570598803460598\n",
      "Iteration: 58/214, Loss: 0.03708260878920555\n",
      "Iteration: 59/214, Loss: 0.27269813418388367\n",
      "Iteration: 60/214, Loss: 0.002918716287240386\n",
      "Iteration: 61/214, Loss: 0.18298402428627014\n",
      "Iteration: 62/214, Loss: 0.0880771204829216\n",
      "Iteration: 63/214, Loss: 0.0020627269987016916\n",
      "Iteration: 64/214, Loss: 0.08937626332044601\n",
      "Iteration: 65/214, Loss: 0.36021316051483154\n",
      "Iteration: 66/214, Loss: 0.5038896799087524\n",
      "Iteration: 67/214, Loss: 0.3832907974720001\n",
      "Iteration: 68/214, Loss: 0.06755362451076508\n",
      "Iteration: 69/214, Loss: 0.11633571237325668\n",
      "Iteration: 70/214, Loss: 0.17414872348308563\n",
      "Iteration: 71/214, Loss: 0.5979487299919128\n",
      "Iteration: 72/214, Loss: 0.3119890093803406\n",
      "Iteration: 73/214, Loss: 0.20926010608673096\n",
      "Iteration: 74/214, Loss: 0.12029525637626648\n",
      "Iteration: 75/214, Loss: 0.3252972364425659\n",
      "Iteration: 76/214, Loss: 0.036572977900505066\n",
      "Iteration: 77/214, Loss: 0.08925360441207886\n",
      "Iteration: 78/214, Loss: 0.21067596971988678\n",
      "Iteration: 79/214, Loss: 0.13809572160243988\n",
      "Iteration: 80/214, Loss: 0.18817786872386932\n",
      "Iteration: 81/214, Loss: 0.16011369228363037\n",
      "Iteration: 82/214, Loss: 0.04303841292858124\n",
      "Iteration: 83/214, Loss: 0.06374324858188629\n",
      "Iteration: 84/214, Loss: 0.03427770733833313\n",
      "Iteration: 85/214, Loss: 0.3281102478504181\n",
      "Iteration: 86/214, Loss: 0.1792050153017044\n",
      "Iteration: 87/214, Loss: 0.2891600728034973\n",
      "Iteration: 88/214, Loss: 0.0016881909687072039\n",
      "Iteration: 89/214, Loss: 0.05314384773373604\n",
      "Iteration: 90/214, Loss: 0.05586744844913483\n",
      "Iteration: 91/214, Loss: 0.08241517096757889\n",
      "Iteration: 92/214, Loss: 0.255861759185791\n",
      "Iteration: 93/214, Loss: 0.44210222363471985\n",
      "Iteration: 94/214, Loss: 0.1338750272989273\n",
      "Iteration: 95/214, Loss: 0.3258036673069\n",
      "Iteration: 96/214, Loss: 0.07430239766836166\n",
      "Iteration: 97/214, Loss: 0.1317840963602066\n",
      "Iteration: 98/214, Loss: 0.5127092599868774\n",
      "Iteration: 99/214, Loss: 0.11131422966718674\n",
      "Iteration: 100/214, Loss: 0.2365736961364746\n",
      "Iteration: 101/214, Loss: 0.11104241013526917\n",
      "Iteration: 102/214, Loss: 0.001095110783353448\n",
      "Iteration: 103/214, Loss: 0.39588290452957153\n",
      "Iteration: 104/214, Loss: 0.08488242328166962\n",
      "Iteration: 105/214, Loss: 0.22971390187740326\n",
      "Iteration: 106/214, Loss: 0.0019942184444516897\n",
      "Iteration: 107/214, Loss: 0.47048813104629517\n",
      "Iteration: 108/214, Loss: 0.41502517461776733\n",
      "Iteration: 109/214, Loss: 0.06849958002567291\n",
      "Iteration: 110/214, Loss: 0.08794358372688293\n",
      "Iteration: 111/214, Loss: 0.4585188329219818\n",
      "Iteration: 112/214, Loss: 0.20974642038345337\n",
      "Iteration: 113/214, Loss: 0.13087131083011627\n",
      "Iteration: 114/214, Loss: 0.1865847110748291\n",
      "Iteration: 115/214, Loss: 0.04460987076163292\n",
      "Iteration: 116/214, Loss: 0.49152103066444397\n",
      "Iteration: 117/214, Loss: 0.19014112651348114\n",
      "Iteration: 118/214, Loss: 0.08196143805980682\n",
      "Iteration: 119/214, Loss: 0.08726613223552704\n",
      "Iteration: 120/214, Loss: 0.3168959319591522\n",
      "Iteration: 121/214, Loss: 0.044543515890836716\n",
      "Iteration: 122/214, Loss: 0.12888072431087494\n",
      "Iteration: 123/214, Loss: 0.11499512940645218\n",
      "Iteration: 124/214, Loss: 0.058340683579444885\n",
      "Iteration: 125/214, Loss: 0.181677907705307\n",
      "Iteration: 126/214, Loss: 0.028599867597222328\n",
      "Iteration: 127/214, Loss: 0.04426540806889534\n",
      "Iteration: 128/214, Loss: 0.16498902440071106\n",
      "Iteration: 129/214, Loss: 0.3181138038635254\n",
      "Iteration: 130/214, Loss: 0.29443278908729553\n",
      "Iteration: 131/214, Loss: 0.028014376759529114\n",
      "Iteration: 132/214, Loss: 0.2917821407318115\n",
      "Iteration: 133/214, Loss: 0.0013912664726376534\n",
      "Iteration: 134/214, Loss: 0.058477338403463364\n",
      "Iteration: 135/214, Loss: 0.044288698583841324\n",
      "Iteration: 136/214, Loss: 0.0010443709325045347\n",
      "Iteration: 137/214, Loss: 0.001469864509999752\n",
      "Iteration: 138/214, Loss: 0.03531161695718765\n",
      "Iteration: 139/214, Loss: 0.08239875733852386\n",
      "Iteration: 140/214, Loss: 0.3481840491294861\n",
      "Iteration: 141/214, Loss: 0.053929027169942856\n",
      "Iteration: 142/214, Loss: 0.5225602388381958\n",
      "Iteration: 143/214, Loss: 0.07343323528766632\n",
      "Iteration: 144/214, Loss: 0.2552832365036011\n",
      "Iteration: 145/214, Loss: 0.05254984274506569\n",
      "Iteration: 146/214, Loss: 0.028070950880646706\n",
      "Iteration: 147/214, Loss: 0.22419056296348572\n",
      "Iteration: 148/214, Loss: 0.13484813272953033\n",
      "Iteration: 149/214, Loss: 0.031158341094851494\n",
      "Iteration: 150/214, Loss: 0.037470635026693344\n",
      "Iteration: 151/214, Loss: 0.041142337024211884\n",
      "Iteration: 152/214, Loss: 0.09979411959648132\n",
      "Iteration: 153/214, Loss: 0.037430692464113235\n",
      "Iteration: 154/214, Loss: 0.0361197330057621\n",
      "Iteration: 155/214, Loss: 0.43297287821769714\n",
      "Iteration: 156/214, Loss: 0.35533350706100464\n",
      "Iteration: 157/214, Loss: 0.0022232451010495424\n",
      "Iteration: 158/214, Loss: 0.08696523308753967\n",
      "Iteration: 159/214, Loss: 0.033379118889570236\n",
      "Iteration: 160/214, Loss: 0.1883956640958786\n",
      "Iteration: 161/214, Loss: 0.333881676197052\n",
      "Iteration: 162/214, Loss: 0.05392060801386833\n",
      "Iteration: 163/214, Loss: 0.147314190864563\n",
      "Iteration: 164/214, Loss: 0.035076480358839035\n",
      "Iteration: 165/214, Loss: 0.3690839111804962\n",
      "Iteration: 166/214, Loss: 0.0016062111826613545\n",
      "Iteration: 167/214, Loss: 0.027230998501181602\n",
      "Iteration: 168/214, Loss: 0.03316039964556694\n",
      "Iteration: 169/214, Loss: 0.30923694372177124\n",
      "Iteration: 170/214, Loss: 0.29001808166503906\n",
      "Iteration: 171/214, Loss: 0.47842544317245483\n",
      "Iteration: 172/214, Loss: 0.12641216814517975\n",
      "Iteration: 173/214, Loss: 0.037777647376060486\n",
      "Iteration: 174/214, Loss: 0.02504870854318142\n",
      "Iteration: 175/214, Loss: 0.24153010547161102\n",
      "Iteration: 176/214, Loss: 0.0872991755604744\n",
      "Iteration: 177/214, Loss: 0.15992887318134308\n",
      "Iteration: 178/214, Loss: 0.09479829668998718\n",
      "Iteration: 179/214, Loss: 0.02682814560830593\n",
      "Iteration: 180/214, Loss: 0.0005966058233752847\n",
      "Iteration: 181/214, Loss: 0.17896129190921783\n",
      "Iteration: 182/214, Loss: 0.03485172241926193\n",
      "Iteration: 183/214, Loss: 0.5103887915611267\n",
      "Iteration: 184/214, Loss: 0.2472047358751297\n",
      "Iteration: 185/214, Loss: 0.09456746280193329\n",
      "Iteration: 186/214, Loss: 0.07382072508335114\n",
      "Iteration: 187/214, Loss: 0.10540066659450531\n",
      "Iteration: 188/214, Loss: 0.1544187366962433\n",
      "Iteration: 189/214, Loss: 0.7269542813301086\n",
      "Iteration: 190/214, Loss: 0.23457418382167816\n",
      "Iteration: 191/214, Loss: 0.4094962775707245\n",
      "Iteration: 192/214, Loss: 0.0875464603304863\n",
      "Iteration: 193/214, Loss: 0.3407341241836548\n",
      "Iteration: 194/214, Loss: 0.004460924305021763\n",
      "Iteration: 195/214, Loss: 0.07609731703996658\n",
      "Iteration: 196/214, Loss: 0.34563305974006653\n",
      "Iteration: 197/214, Loss: 0.2931494414806366\n",
      "Iteration: 198/214, Loss: 0.1863371729850769\n",
      "Iteration: 199/214, Loss: 0.05015115812420845\n",
      "Iteration: 200/214, Loss: 0.32976922392845154\n",
      "Iteration: 201/214, Loss: 0.2929159104824066\n",
      "Iteration: 202/214, Loss: 0.4816659390926361\n",
      "Iteration: 203/214, Loss: 0.040027882903814316\n",
      "Iteration: 204/214, Loss: 0.12392902374267578\n",
      "Iteration: 205/214, Loss: 0.24798041582107544\n",
      "Iteration: 206/214, Loss: 0.03004561923444271\n",
      "Iteration: 207/214, Loss: 0.4161890149116516\n",
      "Iteration: 208/214, Loss: 0.03700345754623413\n",
      "Iteration: 209/214, Loss: 0.10542016476392746\n",
      "Iteration: 210/214, Loss: 0.30546021461486816\n",
      "Iteration: 211/214, Loss: 0.12832483649253845\n",
      "Iteration: 212/214, Loss: 0.1562521755695343\n",
      "Iteration: 213/214, Loss: 0.5306724309921265\n",
      "Iteration: 214/214, Loss: 0.07100076973438263\n",
      "tensor(37.7024, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Iteration: 1/214, Loss: 0.06730814278125763\n",
      "Iteration: 2/214, Loss: 0.045247502624988556\n",
      "Iteration: 3/214, Loss: 0.043353646993637085\n",
      "Iteration: 4/214, Loss: 0.4180571734905243\n",
      "Iteration: 5/214, Loss: 0.053664881736040115\n",
      "Iteration: 6/214, Loss: 0.037862978875637054\n",
      "Iteration: 7/214, Loss: 0.14776328206062317\n",
      "Iteration: 8/214, Loss: 0.02954719401896\n",
      "Iteration: 9/214, Loss: 0.20335257053375244\n",
      "Iteration: 10/214, Loss: 0.36314600706100464\n",
      "Iteration: 11/214, Loss: 0.16935139894485474\n",
      "Iteration: 12/214, Loss: 0.1621929258108139\n",
      "Iteration: 13/214, Loss: 0.0009981330949813128\n",
      "Iteration: 14/214, Loss: 0.6029743552207947\n",
      "Iteration: 15/214, Loss: 0.05704270303249359\n",
      "Iteration: 16/214, Loss: 0.05866197869181633\n",
      "Iteration: 17/214, Loss: 1.6265989542007446\n",
      "Iteration: 18/214, Loss: 0.0007022448116913438\n",
      "Iteration: 19/214, Loss: 0.0008733881404623389\n",
      "Iteration: 20/214, Loss: 0.3251885771751404\n",
      "Iteration: 21/214, Loss: 0.058088622987270355\n",
      "Iteration: 22/214, Loss: 0.21273553371429443\n",
      "Iteration: 23/214, Loss: 0.036579832434654236\n",
      "Iteration: 24/214, Loss: 0.32064637541770935\n",
      "Iteration: 25/214, Loss: 0.05908737704157829\n",
      "Iteration: 26/214, Loss: 0.10193166881799698\n",
      "Iteration: 27/214, Loss: 0.10828983783721924\n",
      "Iteration: 28/214, Loss: 0.22307418286800385\n",
      "Iteration: 29/214, Loss: 0.14947998523712158\n",
      "Iteration: 30/214, Loss: 0.23506242036819458\n",
      "Iteration: 31/214, Loss: 0.0529639795422554\n",
      "Iteration: 32/214, Loss: 0.11113737523555756\n",
      "Iteration: 33/214, Loss: 0.0021801446564495564\n",
      "Iteration: 34/214, Loss: 0.15233224630355835\n",
      "Iteration: 35/214, Loss: 0.03180449455976486\n",
      "Iteration: 36/214, Loss: 0.05119023844599724\n",
      "Iteration: 37/214, Loss: 0.15550856292247772\n",
      "Iteration: 38/214, Loss: 0.16770721971988678\n",
      "Iteration: 39/214, Loss: 0.24062182009220123\n",
      "Iteration: 40/214, Loss: 0.5511503219604492\n",
      "Iteration: 41/214, Loss: 0.07637232542037964\n",
      "Iteration: 42/214, Loss: 0.19007964432239532\n",
      "Iteration: 43/214, Loss: 0.1646224707365036\n",
      "Iteration: 44/214, Loss: 0.0017002932727336884\n",
      "Iteration: 45/214, Loss: 0.3100690543651581\n",
      "Iteration: 46/214, Loss: 0.0498807430267334\n",
      "Iteration: 47/214, Loss: 0.3342072069644928\n",
      "Iteration: 48/214, Loss: 0.02812919020652771\n",
      "Iteration: 49/214, Loss: 0.02682102844119072\n",
      "Iteration: 50/214, Loss: 0.028794825077056885\n",
      "Iteration: 51/214, Loss: 0.08592921495437622\n",
      "Iteration: 52/214, Loss: 0.2747425138950348\n",
      "Iteration: 53/214, Loss: 0.08053874224424362\n",
      "Iteration: 54/214, Loss: 0.049890145659446716\n",
      "Iteration: 55/214, Loss: 0.14363348484039307\n",
      "Iteration: 56/214, Loss: 0.032286081463098526\n",
      "Iteration: 57/214, Loss: 0.046617377549409866\n",
      "Iteration: 58/214, Loss: 0.020604267716407776\n",
      "Iteration: 59/214, Loss: 0.21512261033058167\n",
      "Iteration: 60/214, Loss: 0.0018472725059837103\n",
      "Iteration: 61/214, Loss: 0.1783650666475296\n",
      "Iteration: 62/214, Loss: 0.07056384533643723\n",
      "Iteration: 63/214, Loss: 0.0020659181755036116\n",
      "Iteration: 64/214, Loss: 0.06652560830116272\n",
      "Iteration: 65/214, Loss: 0.3807632327079773\n",
      "Iteration: 66/214, Loss: 0.37773987650871277\n",
      "Iteration: 67/214, Loss: 0.2268642783164978\n",
      "Iteration: 68/214, Loss: 0.055487051606178284\n",
      "Iteration: 69/214, Loss: 0.10666858404874802\n",
      "Iteration: 70/214, Loss: 0.13708682358264923\n",
      "Iteration: 71/214, Loss: 0.3967741131782532\n",
      "Iteration: 72/214, Loss: 0.2779318392276764\n",
      "Iteration: 73/214, Loss: 0.14254768192768097\n",
      "Iteration: 74/214, Loss: 0.13369883596897125\n",
      "Iteration: 75/214, Loss: 0.2995336949825287\n",
      "Iteration: 76/214, Loss: 0.02075493521988392\n",
      "Iteration: 77/214, Loss: 0.09786016494035721\n",
      "Iteration: 78/214, Loss: 0.15849393606185913\n",
      "Iteration: 79/214, Loss: 0.11510275304317474\n",
      "Iteration: 80/214, Loss: 0.10296061635017395\n",
      "Iteration: 81/214, Loss: 0.09917730838060379\n",
      "Iteration: 82/214, Loss: 0.024471160024404526\n",
      "Iteration: 83/214, Loss: 0.025330867618322372\n",
      "Iteration: 84/214, Loss: 0.034842319786548615\n",
      "Iteration: 85/214, Loss: 0.17635589838027954\n",
      "Iteration: 86/214, Loss: 0.09793522208929062\n",
      "Iteration: 87/214, Loss: 0.19276505708694458\n",
      "Iteration: 88/214, Loss: 0.001120834844186902\n",
      "Iteration: 89/214, Loss: 0.05464731901884079\n",
      "Iteration: 90/214, Loss: 0.023349527269601822\n",
      "Iteration: 91/214, Loss: 0.05213189870119095\n",
      "Iteration: 92/214, Loss: 0.1420450061559677\n",
      "Iteration: 93/214, Loss: 0.2488616406917572\n",
      "Iteration: 94/214, Loss: 0.08352740854024887\n",
      "Iteration: 95/214, Loss: 0.31395354866981506\n",
      "Iteration: 96/214, Loss: 0.04953845590353012\n",
      "Iteration: 97/214, Loss: 0.13918350636959076\n",
      "Iteration: 98/214, Loss: 0.33973196148872375\n",
      "Iteration: 99/214, Loss: 0.06682288646697998\n",
      "Iteration: 100/214, Loss: 0.15765002369880676\n",
      "Iteration: 101/214, Loss: 0.09516847133636475\n",
      "Iteration: 102/214, Loss: 0.0011155640240758657\n",
      "Iteration: 103/214, Loss: 0.31896331906318665\n",
      "Iteration: 104/214, Loss: 0.0776023268699646\n",
      "Iteration: 105/214, Loss: 0.18914508819580078\n",
      "Iteration: 106/214, Loss: 0.0013655272778123617\n",
      "Iteration: 107/214, Loss: 0.343178927898407\n",
      "Iteration: 108/214, Loss: 0.37349840998649597\n",
      "Iteration: 109/214, Loss: 0.05130381137132645\n",
      "Iteration: 110/214, Loss: 0.05485893040895462\n",
      "Iteration: 111/214, Loss: 0.4093053936958313\n",
      "Iteration: 112/214, Loss: 0.14895682036876678\n",
      "Iteration: 113/214, Loss: 0.12704946100711823\n",
      "Iteration: 114/214, Loss: 0.08559788763523102\n",
      "Iteration: 115/214, Loss: 0.030580593273043633\n",
      "Iteration: 116/214, Loss: 0.38451385498046875\n",
      "Iteration: 117/214, Loss: 0.16919741034507751\n",
      "Iteration: 118/214, Loss: 0.06254995614290237\n",
      "Iteration: 119/214, Loss: 0.04992959648370743\n",
      "Iteration: 120/214, Loss: 0.2298557162284851\n",
      "Iteration: 121/214, Loss: 0.02047220803797245\n",
      "Iteration: 122/214, Loss: 0.13205738365650177\n",
      "Iteration: 123/214, Loss: 0.062587670981884\n",
      "Iteration: 124/214, Loss: 0.05250130221247673\n",
      "Iteration: 125/214, Loss: 0.11419489979743958\n",
      "Iteration: 126/214, Loss: 0.03144226595759392\n",
      "Iteration: 127/214, Loss: 0.057287029922008514\n",
      "Iteration: 128/214, Loss: 0.13045644760131836\n",
      "Iteration: 129/214, Loss: 0.22243119776248932\n",
      "Iteration: 130/214, Loss: 0.18529140949249268\n",
      "Iteration: 131/214, Loss: 0.020532755181193352\n",
      "Iteration: 132/214, Loss: 0.2559700012207031\n",
      "Iteration: 133/214, Loss: 0.0020370343700051308\n",
      "Iteration: 134/214, Loss: 0.051445748656988144\n",
      "Iteration: 135/214, Loss: 0.07827083766460419\n",
      "Iteration: 136/214, Loss: 0.0010259469272568822\n",
      "Iteration: 137/214, Loss: 0.002409958280622959\n",
      "Iteration: 138/214, Loss: 0.04306450113654137\n",
      "Iteration: 139/214, Loss: 0.06087736040353775\n",
      "Iteration: 140/214, Loss: 0.366259902715683\n",
      "Iteration: 141/214, Loss: 0.04741709679365158\n",
      "Iteration: 142/214, Loss: 0.441436231136322\n",
      "Iteration: 143/214, Loss: 0.03012883849442005\n",
      "Iteration: 144/214, Loss: 0.15517035126686096\n",
      "Iteration: 145/214, Loss: 0.04253953695297241\n",
      "Iteration: 146/214, Loss: 0.03434893116354942\n",
      "Iteration: 147/214, Loss: 0.17505940794944763\n",
      "Iteration: 148/214, Loss: 0.11299774050712585\n",
      "Iteration: 149/214, Loss: 0.03520122915506363\n",
      "Iteration: 150/214, Loss: 0.031886614859104156\n",
      "Iteration: 151/214, Loss: 0.04057940095663071\n",
      "Iteration: 152/214, Loss: 0.06531517952680588\n",
      "Iteration: 153/214, Loss: 0.03988157585263252\n",
      "Iteration: 154/214, Loss: 0.029716547578573227\n",
      "Iteration: 155/214, Loss: 0.35648393630981445\n",
      "Iteration: 156/214, Loss: 0.27507421374320984\n",
      "Iteration: 157/214, Loss: 0.0028335419483482838\n",
      "Iteration: 158/214, Loss: 0.07988060265779495\n",
      "Iteration: 159/214, Loss: 0.04052139073610306\n",
      "Iteration: 160/214, Loss: 0.13768251240253448\n",
      "Iteration: 161/214, Loss: 0.32103830575942993\n",
      "Iteration: 162/214, Loss: 0.04228924959897995\n",
      "Iteration: 163/214, Loss: 0.16098399460315704\n",
      "Iteration: 164/214, Loss: 0.030939048156142235\n",
      "Iteration: 165/214, Loss: 0.2871548533439636\n",
      "Iteration: 166/214, Loss: 0.006003187503665686\n",
      "Iteration: 167/214, Loss: 0.021325984969735146\n",
      "Iteration: 168/214, Loss: 0.041059717535972595\n",
      "Iteration: 169/214, Loss: 0.28632158041000366\n",
      "Iteration: 170/214, Loss: 0.27478858828544617\n",
      "Iteration: 171/214, Loss: 0.3787376880645752\n",
      "Iteration: 172/214, Loss: 0.09208984673023224\n",
      "Iteration: 173/214, Loss: 0.03834459185600281\n",
      "Iteration: 174/214, Loss: 0.02913467213511467\n",
      "Iteration: 175/214, Loss: 0.17861545085906982\n",
      "Iteration: 176/214, Loss: 0.06895444542169571\n",
      "Iteration: 177/214, Loss: 0.20858226716518402\n",
      "Iteration: 178/214, Loss: 0.07390903681516647\n",
      "Iteration: 179/214, Loss: 0.01785670779645443\n",
      "Iteration: 180/214, Loss: 0.0006300504319369793\n",
      "Iteration: 181/214, Loss: 0.195264995098114\n",
      "Iteration: 182/214, Loss: 0.041376933455467224\n",
      "Iteration: 183/214, Loss: 0.34862425923347473\n",
      "Iteration: 184/214, Loss: 0.19978740811347961\n",
      "Iteration: 185/214, Loss: 0.07021088898181915\n",
      "Iteration: 186/214, Loss: 0.059409528970718384\n",
      "Iteration: 187/214, Loss: 0.0883844867348671\n",
      "Iteration: 188/214, Loss: 0.16258837282657623\n",
      "Iteration: 189/214, Loss: 0.5205864906311035\n",
      "Iteration: 190/214, Loss: 0.19327616691589355\n",
      "Iteration: 191/214, Loss: 0.38265547156333923\n",
      "Iteration: 192/214, Loss: 0.049539659172296524\n",
      "Iteration: 193/214, Loss: 0.2788051664829254\n",
      "Iteration: 194/214, Loss: 0.009885109961032867\n",
      "Iteration: 195/214, Loss: 0.08181650936603546\n",
      "Iteration: 196/214, Loss: 0.2518450915813446\n",
      "Iteration: 197/214, Loss: 0.2451842576265335\n",
      "Iteration: 198/214, Loss: 0.12574169039726257\n",
      "Iteration: 199/214, Loss: 0.024872969835996628\n",
      "Iteration: 200/214, Loss: 0.2249508947134018\n",
      "Iteration: 201/214, Loss: 0.14705045521259308\n",
      "Iteration: 202/214, Loss: 0.32706618309020996\n",
      "Iteration: 203/214, Loss: 0.0347626768052578\n",
      "Iteration: 204/214, Loss: 0.1743699461221695\n",
      "Iteration: 205/214, Loss: 0.21811443567276\n",
      "Iteration: 206/214, Loss: 0.034935206174850464\n",
      "Iteration: 207/214, Loss: 0.38155031204223633\n",
      "Iteration: 208/214, Loss: 0.030471980571746826\n",
      "Iteration: 209/214, Loss: 0.09244732558727264\n",
      "Iteration: 210/214, Loss: 0.2800180912017822\n",
      "Iteration: 211/214, Loss: 0.10542348027229309\n",
      "Iteration: 212/214, Loss: 0.08376570791006088\n",
      "Iteration: 213/214, Loss: 0.31223854422569275\n",
      "Iteration: 214/214, Loss: 0.06576208025217056\n",
      "tensor(30.9386, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Iteration: 1/214, Loss: 0.04616100713610649\n",
      "Iteration: 2/214, Loss: 0.02779010683298111\n",
      "Iteration: 3/214, Loss: 0.04458854719996452\n",
      "Iteration: 4/214, Loss: 0.426241934299469\n",
      "Iteration: 5/214, Loss: 0.05382673069834709\n",
      "Iteration: 6/214, Loss: 0.02893528714776039\n",
      "Iteration: 7/214, Loss: 0.06909846514463425\n",
      "Iteration: 8/214, Loss: 0.021820640191435814\n",
      "Iteration: 9/214, Loss: 0.0959145724773407\n",
      "Iteration: 10/214, Loss: 0.2685302793979645\n",
      "Iteration: 11/214, Loss: 0.146728053689003\n",
      "Iteration: 12/214, Loss: 0.17644691467285156\n",
      "Iteration: 13/214, Loss: 0.014026429504156113\n",
      "Iteration: 14/214, Loss: 0.5615460872650146\n",
      "Iteration: 15/214, Loss: 0.04759141430258751\n",
      "Iteration: 16/214, Loss: 0.06452630460262299\n",
      "Iteration: 17/214, Loss: 1.2039711475372314\n",
      "Iteration: 18/214, Loss: 0.0027793990448117256\n",
      "Iteration: 19/214, Loss: 0.007567719556391239\n",
      "Iteration: 20/214, Loss: 0.286024808883667\n",
      "Iteration: 21/214, Loss: 0.04796302691102028\n",
      "Iteration: 22/214, Loss: 0.13162362575531006\n",
      "Iteration: 23/214, Loss: 0.03315325081348419\n",
      "Iteration: 24/214, Loss: 0.22690996527671814\n",
      "Iteration: 25/214, Loss: 0.050207361578941345\n",
      "Iteration: 26/214, Loss: 0.08308521658182144\n",
      "Iteration: 27/214, Loss: 0.09891603142023087\n",
      "Iteration: 28/214, Loss: 0.11723357439041138\n",
      "Iteration: 29/214, Loss: 0.08338117599487305\n",
      "Iteration: 30/214, Loss: 0.16367828845977783\n",
      "Iteration: 31/214, Loss: 0.03179918974637985\n",
      "Iteration: 32/214, Loss: 0.10375958681106567\n",
      "Iteration: 33/214, Loss: 0.0044606090523302555\n",
      "Iteration: 34/214, Loss: 0.08867746591567993\n",
      "Iteration: 35/214, Loss: 0.03002450242638588\n",
      "Iteration: 36/214, Loss: 0.05544886365532875\n",
      "Iteration: 37/214, Loss: 0.09695607423782349\n",
      "Iteration: 38/214, Loss: 0.1369103491306305\n",
      "Iteration: 39/214, Loss: 0.20260290801525116\n",
      "Iteration: 40/214, Loss: 0.5301780104637146\n",
      "Iteration: 41/214, Loss: 0.07113754749298096\n",
      "Iteration: 42/214, Loss: 0.18387217819690704\n",
      "Iteration: 43/214, Loss: 0.11530350148677826\n",
      "Iteration: 44/214, Loss: 0.0006838034023530781\n",
      "Iteration: 45/214, Loss: 0.28513216972351074\n",
      "Iteration: 46/214, Loss: 0.017660588026046753\n",
      "Iteration: 47/214, Loss: 0.4067278504371643\n",
      "Iteration: 48/214, Loss: 0.03979743644595146\n",
      "Iteration: 49/214, Loss: 0.013706834986805916\n",
      "Iteration: 50/214, Loss: 0.03178546577692032\n",
      "Iteration: 51/214, Loss: 0.07251473516225815\n",
      "Iteration: 52/214, Loss: 0.25270918011665344\n",
      "Iteration: 53/214, Loss: 0.04684208333492279\n",
      "Iteration: 54/214, Loss: 0.04447545111179352\n",
      "Iteration: 55/214, Loss: 0.12074699252843857\n",
      "Iteration: 56/214, Loss: 0.02277078479528427\n",
      "Iteration: 57/214, Loss: 0.03783288598060608\n",
      "Iteration: 58/214, Loss: 0.027875658124685287\n",
      "Iteration: 59/214, Loss: 0.320992648601532\n",
      "Iteration: 60/214, Loss: 0.0017044034320861101\n",
      "Iteration: 61/214, Loss: 0.1680019348859787\n",
      "Iteration: 62/214, Loss: 0.051910609006881714\n",
      "Iteration: 63/214, Loss: 0.001574002904817462\n",
      "Iteration: 64/214, Loss: 0.03385896980762482\n",
      "Iteration: 65/214, Loss: 0.24969074130058289\n",
      "Iteration: 66/214, Loss: 0.38522377610206604\n",
      "Iteration: 67/214, Loss: 0.1826387494802475\n",
      "Iteration: 68/214, Loss: 0.06701261550188065\n",
      "Iteration: 69/214, Loss: 0.1117047592997551\n",
      "Iteration: 70/214, Loss: 0.09338954836130142\n",
      "Iteration: 71/214, Loss: 0.5001941323280334\n",
      "Iteration: 72/214, Loss: 0.23588325083255768\n",
      "Iteration: 73/214, Loss: 0.20026633143424988\n",
      "Iteration: 74/214, Loss: 0.09921213984489441\n",
      "Iteration: 75/214, Loss: 0.23740223050117493\n",
      "Iteration: 76/214, Loss: 0.023786483332514763\n",
      "Iteration: 77/214, Loss: 0.08487853407859802\n",
      "Iteration: 78/214, Loss: 0.12934596836566925\n",
      "Iteration: 79/214, Loss: 0.10315589606761932\n",
      "Iteration: 80/214, Loss: 0.11386997252702713\n",
      "Iteration: 81/214, Loss: 0.11175510287284851\n",
      "Iteration: 82/214, Loss: 0.04051666706800461\n",
      "Iteration: 83/214, Loss: 0.0363754965364933\n",
      "Iteration: 84/214, Loss: 0.02382393181324005\n",
      "Iteration: 85/214, Loss: 0.1862667351961136\n",
      "Iteration: 86/214, Loss: 0.09286956489086151\n",
      "Iteration: 87/214, Loss: 0.161300927400589\n",
      "Iteration: 88/214, Loss: 0.001053705345839262\n",
      "Iteration: 89/214, Loss: 0.035139523446559906\n",
      "Iteration: 90/214, Loss: 0.03754037991166115\n",
      "Iteration: 91/214, Loss: 0.062411367893218994\n",
      "Iteration: 92/214, Loss: 0.22696256637573242\n",
      "Iteration: 93/214, Loss: 0.3061169683933258\n",
      "Iteration: 94/214, Loss: 0.09158994257450104\n",
      "Iteration: 95/214, Loss: 0.22550097107887268\n",
      "Iteration: 96/214, Loss: 0.028015416115522385\n",
      "Iteration: 97/214, Loss: 0.0817335695028305\n",
      "Iteration: 98/214, Loss: 0.4037151038646698\n",
      "Iteration: 99/214, Loss: 0.05501252040266991\n",
      "Iteration: 100/214, Loss: 0.2032235562801361\n",
      "Iteration: 101/214, Loss: 0.13935132324695587\n",
      "Iteration: 102/214, Loss: 0.00200467836111784\n",
      "Iteration: 103/214, Loss: 0.28847694396972656\n",
      "Iteration: 104/214, Loss: 0.05258656293153763\n",
      "Iteration: 105/214, Loss: 0.1176566630601883\n",
      "Iteration: 106/214, Loss: 0.0019114678725600243\n",
      "Iteration: 107/214, Loss: 0.405353844165802\n",
      "Iteration: 108/214, Loss: 0.35778704285621643\n",
      "Iteration: 109/214, Loss: 0.036686938256025314\n",
      "Iteration: 110/214, Loss: 0.09877936542034149\n",
      "Iteration: 111/214, Loss: 0.40982306003570557\n",
      "Iteration: 112/214, Loss: 0.1559256762266159\n",
      "Iteration: 113/214, Loss: 0.164454385638237\n",
      "Iteration: 114/214, Loss: 0.14163346588611603\n",
      "Iteration: 115/214, Loss: 0.0334436260163784\n",
      "Iteration: 116/214, Loss: 0.4107978045940399\n",
      "Iteration: 117/214, Loss: 0.16040144860744476\n",
      "Iteration: 118/214, Loss: 0.026400137692689896\n",
      "Iteration: 119/214, Loss: 0.054702967405319214\n",
      "Iteration: 120/214, Loss: 0.2984653115272522\n",
      "Iteration: 121/214, Loss: 0.028158482164144516\n",
      "Iteration: 122/214, Loss: 0.08904699236154556\n",
      "Iteration: 123/214, Loss: 0.07609028369188309\n",
      "Iteration: 124/214, Loss: 0.035233575850725174\n",
      "Iteration: 125/214, Loss: 0.07581505924463272\n",
      "Iteration: 126/214, Loss: 0.031066879630088806\n",
      "Iteration: 127/214, Loss: 0.03211564943194389\n",
      "Iteration: 128/214, Loss: 0.1463196724653244\n",
      "Iteration: 129/214, Loss: 0.26347747445106506\n",
      "Iteration: 130/214, Loss: 0.24635037779808044\n",
      "Iteration: 131/214, Loss: 0.037728019058704376\n",
      "Iteration: 132/214, Loss: 0.329639196395874\n",
      "Iteration: 133/214, Loss: 0.0017480248352512717\n",
      "Iteration: 134/214, Loss: 0.09140291810035706\n",
      "Iteration: 135/214, Loss: 0.06760042905807495\n",
      "Iteration: 136/214, Loss: 0.0017020199447870255\n",
      "Iteration: 137/214, Loss: 0.0016566549893468618\n",
      "Iteration: 138/214, Loss: 0.09151733666658401\n",
      "Iteration: 139/214, Loss: 0.1180468499660492\n",
      "Iteration: 140/214, Loss: 0.42083418369293213\n",
      "Iteration: 141/214, Loss: 0.04222678765654564\n",
      "Iteration: 142/214, Loss: 0.45535901188850403\n",
      "Iteration: 143/214, Loss: 0.044792842119932175\n",
      "Iteration: 144/214, Loss: 0.18624070286750793\n",
      "Iteration: 145/214, Loss: 0.0481233075261116\n",
      "Iteration: 146/214, Loss: 0.026123175397515297\n",
      "Iteration: 147/214, Loss: 0.28632259368896484\n",
      "Iteration: 148/214, Loss: 0.18009714782238007\n",
      "Iteration: 149/214, Loss: 0.0251776110380888\n",
      "Iteration: 150/214, Loss: 0.0298327449709177\n",
      "Iteration: 151/214, Loss: 0.039703235030174255\n",
      "Iteration: 152/214, Loss: 0.06329675018787384\n",
      "Iteration: 153/214, Loss: 0.043972477316856384\n",
      "Iteration: 154/214, Loss: 0.023315556347370148\n",
      "Iteration: 155/214, Loss: 0.31612423062324524\n",
      "Iteration: 156/214, Loss: 0.2860835790634155\n",
      "Iteration: 157/214, Loss: 0.0009694846230559051\n",
      "Iteration: 158/214, Loss: 0.06398079544305801\n",
      "Iteration: 159/214, Loss: 0.02117275260388851\n",
      "Iteration: 160/214, Loss: 0.16164855659008026\n",
      "Iteration: 161/214, Loss: 0.3030715882778168\n",
      "Iteration: 162/214, Loss: 0.03435102850198746\n",
      "Iteration: 163/214, Loss: 0.13215304911136627\n",
      "Iteration: 164/214, Loss: 0.02825320139527321\n",
      "Iteration: 165/214, Loss: 0.283965528011322\n",
      "Iteration: 166/214, Loss: 0.0016196264186874032\n",
      "Iteration: 167/214, Loss: 0.041348837316036224\n",
      "Iteration: 168/214, Loss: 0.0181963462382555\n",
      "Iteration: 169/214, Loss: 0.2633882164955139\n",
      "Iteration: 170/214, Loss: 0.28291264176368713\n",
      "Iteration: 171/214, Loss: 0.2536553740501404\n",
      "Iteration: 172/214, Loss: 0.056786105036735535\n",
      "Iteration: 173/214, Loss: 0.05552463233470917\n",
      "Iteration: 174/214, Loss: 0.040630217641592026\n",
      "Iteration: 175/214, Loss: 0.24587133526802063\n",
      "Iteration: 176/214, Loss: 0.11011900007724762\n",
      "Iteration: 177/214, Loss: 0.2618844509124756\n",
      "Iteration: 178/214, Loss: 0.09421409666538239\n",
      "Iteration: 179/214, Loss: 0.017597993835806847\n",
      "Iteration: 180/214, Loss: 0.000634643598459661\n",
      "Iteration: 181/214, Loss: 0.2036810964345932\n",
      "Iteration: 182/214, Loss: 0.03221946209669113\n",
      "Iteration: 183/214, Loss: 0.34980517625808716\n",
      "Iteration: 184/214, Loss: 0.27837368845939636\n",
      "Iteration: 185/214, Loss: 0.09469962865114212\n",
      "Iteration: 186/214, Loss: 0.07376240938901901\n",
      "Iteration: 187/214, Loss: 0.09298611432313919\n",
      "Iteration: 188/214, Loss: 0.12425251305103302\n",
      "Iteration: 189/214, Loss: 0.45837169885635376\n",
      "Iteration: 190/214, Loss: 0.15362268686294556\n",
      "Iteration: 191/214, Loss: 0.3197023868560791\n",
      "Iteration: 192/214, Loss: 0.06223011389374733\n",
      "Iteration: 193/214, Loss: 0.2869836091995239\n",
      "Iteration: 194/214, Loss: 0.02167995274066925\n",
      "Iteration: 195/214, Loss: 0.10827019065618515\n",
      "Iteration: 196/214, Loss: 0.39845433831214905\n",
      "Iteration: 197/214, Loss: 0.22991995513439178\n",
      "Iteration: 198/214, Loss: 0.1220158115029335\n",
      "Iteration: 199/214, Loss: 0.03785514086484909\n",
      "Iteration: 200/214, Loss: 0.30403658747673035\n",
      "Iteration: 201/214, Loss: 0.23086173832416534\n",
      "Iteration: 202/214, Loss: 0.3366987407207489\n",
      "Iteration: 203/214, Loss: 0.0504031777381897\n",
      "Iteration: 204/214, Loss: 0.20816753804683685\n",
      "Iteration: 205/214, Loss: 0.20208840072155\n",
      "Iteration: 206/214, Loss: 0.03934444114565849\n",
      "Iteration: 207/214, Loss: 0.34507957100868225\n",
      "Iteration: 208/214, Loss: 0.034493930637836456\n",
      "Iteration: 209/214, Loss: 0.07987161725759506\n",
      "Iteration: 210/214, Loss: 0.31867706775665283\n",
      "Iteration: 211/214, Loss: 0.11978849023580551\n",
      "Iteration: 212/214, Loss: 0.1181350126862526\n",
      "Iteration: 213/214, Loss: 0.3278721570968628\n",
      "Iteration: 214/214, Loss: 0.07818742096424103\n",
      "tensor(30.1648, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Iteration: 1/214, Loss: 0.04685746878385544\n",
      "Iteration: 2/214, Loss: 0.036535751074552536\n",
      "Iteration: 3/214, Loss: 0.042994093149900436\n",
      "Iteration: 4/214, Loss: 0.4350869953632355\n",
      "Iteration: 5/214, Loss: 0.059226423501968384\n",
      "Iteration: 6/214, Loss: 0.05503569543361664\n",
      "Iteration: 7/214, Loss: 0.1012699231505394\n",
      "Iteration: 8/214, Loss: 0.04514244943857193\n",
      "Iteration: 9/214, Loss: 0.15320681035518646\n",
      "Iteration: 10/214, Loss: 0.3373717963695526\n",
      "Iteration: 11/214, Loss: 0.16313767433166504\n",
      "Iteration: 12/214, Loss: 0.20708374679088593\n",
      "Iteration: 13/214, Loss: 0.001125973416492343\n",
      "Iteration: 14/214, Loss: 0.5307976007461548\n",
      "Iteration: 15/214, Loss: 0.049773357808589935\n",
      "Iteration: 16/214, Loss: 0.07525699585676193\n",
      "Iteration: 17/214, Loss: 1.0784119367599487\n",
      "Iteration: 18/214, Loss: 0.0004560794332064688\n",
      "Iteration: 19/214, Loss: 0.00093695311807096\n",
      "Iteration: 20/214, Loss: 0.3698066771030426\n",
      "Iteration: 21/214, Loss: 0.07546492666006088\n",
      "Iteration: 22/214, Loss: 0.2654205560684204\n",
      "Iteration: 23/214, Loss: 0.036868661642074585\n",
      "Iteration: 24/214, Loss: 0.34205976128578186\n",
      "Iteration: 25/214, Loss: 0.09806521981954575\n",
      "Iteration: 26/214, Loss: 0.1416400820016861\n",
      "Iteration: 27/214, Loss: 0.08321074396371841\n",
      "Iteration: 28/214, Loss: 0.18503189086914062\n",
      "Iteration: 29/214, Loss: 0.10130556672811508\n",
      "Iteration: 30/214, Loss: 0.2671513557434082\n",
      "Iteration: 31/214, Loss: 0.06334003806114197\n",
      "Iteration: 32/214, Loss: 0.1437320113182068\n",
      "Iteration: 33/214, Loss: 0.006125480867922306\n",
      "Iteration: 34/214, Loss: 0.07336807996034622\n",
      "Iteration: 35/214, Loss: 0.03519180789589882\n",
      "Iteration: 36/214, Loss: 0.09702803194522858\n",
      "Iteration: 37/214, Loss: 0.17782410979270935\n",
      "Iteration: 38/214, Loss: 0.1712321639060974\n",
      "Iteration: 39/214, Loss: 0.27043575048446655\n",
      "Iteration: 40/214, Loss: 0.3698071539402008\n",
      "Iteration: 41/214, Loss: 0.07464521378278732\n",
      "Iteration: 42/214, Loss: 0.20493842661380768\n",
      "Iteration: 43/214, Loss: 0.16061200201511383\n",
      "Iteration: 44/214, Loss: 0.0014749657129868865\n",
      "Iteration: 45/214, Loss: 0.3705540895462036\n",
      "Iteration: 46/214, Loss: 0.053821537643671036\n",
      "Iteration: 47/214, Loss: 0.29535844922065735\n",
      "Iteration: 48/214, Loss: 0.0556349903345108\n",
      "Iteration: 49/214, Loss: 0.03633824363350868\n",
      "Iteration: 50/214, Loss: 0.04403810575604439\n",
      "Iteration: 51/214, Loss: 0.12886765599250793\n",
      "Iteration: 52/214, Loss: 0.21939845383167267\n",
      "Iteration: 53/214, Loss: 0.08671726286411285\n",
      "Iteration: 54/214, Loss: 0.038962215185165405\n",
      "Iteration: 55/214, Loss: 0.1736098825931549\n",
      "Iteration: 56/214, Loss: 0.023208124563097954\n",
      "Iteration: 57/214, Loss: 0.03516499325633049\n",
      "Iteration: 58/214, Loss: 0.026620546355843544\n",
      "Iteration: 59/214, Loss: 0.24882912635803223\n",
      "Iteration: 60/214, Loss: 0.001822856953367591\n",
      "Iteration: 61/214, Loss: 0.1669357419013977\n",
      "Iteration: 62/214, Loss: 0.05213388428092003\n",
      "Iteration: 63/214, Loss: 0.0012121527688577771\n",
      "Iteration: 64/214, Loss: 0.06502962857484818\n",
      "Iteration: 65/214, Loss: 0.3742547631263733\n",
      "Iteration: 66/214, Loss: 0.4450529217720032\n",
      "Iteration: 67/214, Loss: 0.27422550320625305\n",
      "Iteration: 68/214, Loss: 0.0845615491271019\n",
      "Iteration: 69/214, Loss: 0.09422066807746887\n",
      "Iteration: 70/214, Loss: 0.10273195803165436\n",
      "Iteration: 71/214, Loss: 0.3667357861995697\n",
      "Iteration: 72/214, Loss: 0.2638464868068695\n",
      "Iteration: 73/214, Loss: 0.1478108912706375\n",
      "Iteration: 74/214, Loss: 0.12758472561836243\n",
      "Iteration: 75/214, Loss: 0.31049445271492004\n",
      "Iteration: 76/214, Loss: 0.027402926236391068\n",
      "Iteration: 77/214, Loss: 0.07940484583377838\n",
      "Iteration: 78/214, Loss: 0.12141983956098557\n",
      "Iteration: 79/214, Loss: 0.1761380434036255\n",
      "Iteration: 80/214, Loss: 0.12027385830879211\n",
      "Iteration: 81/214, Loss: 0.09515923261642456\n",
      "Iteration: 82/214, Loss: 0.05244464427232742\n",
      "Iteration: 83/214, Loss: 0.027018088847398758\n",
      "Iteration: 84/214, Loss: 0.03676137700676918\n",
      "Iteration: 85/214, Loss: 0.2304370403289795\n",
      "Iteration: 86/214, Loss: 0.12400991469621658\n",
      "Iteration: 87/214, Loss: 0.20065198838710785\n",
      "Iteration: 88/214, Loss: 0.0012176746968179941\n",
      "Iteration: 89/214, Loss: 0.015153702348470688\n",
      "Iteration: 90/214, Loss: 0.042638860642910004\n",
      "Iteration: 91/214, Loss: 0.04081590101122856\n",
      "Iteration: 92/214, Loss: 0.10018876940011978\n",
      "Iteration: 93/214, Loss: 0.2646876275539398\n",
      "Iteration: 94/214, Loss: 0.10187806934118271\n",
      "Iteration: 95/214, Loss: 0.26671382784843445\n",
      "Iteration: 96/214, Loss: 0.03837530314922333\n",
      "Iteration: 97/214, Loss: 0.15760475397109985\n",
      "Iteration: 98/214, Loss: 0.28052881360054016\n",
      "Iteration: 99/214, Loss: 0.05365482717752457\n",
      "Iteration: 100/214, Loss: 0.13321153819561005\n",
      "Iteration: 101/214, Loss: 0.14573033154010773\n",
      "Iteration: 102/214, Loss: 0.001122020184993744\n",
      "Iteration: 103/214, Loss: 0.24634765088558197\n",
      "Iteration: 104/214, Loss: 0.045186709612607956\n",
      "Iteration: 105/214, Loss: 0.1248302012681961\n",
      "Iteration: 106/214, Loss: 0.0024844910949468613\n",
      "Iteration: 107/214, Loss: 0.4608748257160187\n",
      "Iteration: 108/214, Loss: 0.44229191541671753\n",
      "Iteration: 109/214, Loss: 0.026934964582324028\n",
      "Iteration: 110/214, Loss: 0.06589074432849884\n",
      "Iteration: 111/214, Loss: 0.35393860936164856\n",
      "Iteration: 112/214, Loss: 0.14008916914463043\n",
      "Iteration: 113/214, Loss: 0.16497132182121277\n",
      "Iteration: 114/214, Loss: 0.11024048179388046\n",
      "Iteration: 115/214, Loss: 0.028908103704452515\n",
      "Iteration: 116/214, Loss: 0.36924782395362854\n",
      "Iteration: 117/214, Loss: 0.24567387998104095\n",
      "Iteration: 118/214, Loss: 0.0188998244702816\n",
      "Iteration: 119/214, Loss: 0.04860324785113335\n",
      "Iteration: 120/214, Loss: 0.3691958487033844\n",
      "Iteration: 121/214, Loss: 0.033849574625492096\n",
      "Iteration: 122/214, Loss: 0.09881481528282166\n",
      "Iteration: 123/214, Loss: 0.08093177527189255\n",
      "Iteration: 124/214, Loss: 0.046247802674770355\n",
      "Iteration: 125/214, Loss: 0.03350532427430153\n",
      "Iteration: 126/214, Loss: 0.03900802880525589\n",
      "Iteration: 127/214, Loss: 0.03730287402868271\n",
      "Iteration: 128/214, Loss: 0.22507283091545105\n",
      "Iteration: 129/214, Loss: 0.3459756672382355\n",
      "Iteration: 130/214, Loss: 0.3045690655708313\n",
      "Iteration: 131/214, Loss: 0.037942301481962204\n",
      "Iteration: 132/214, Loss: 0.30934906005859375\n",
      "Iteration: 133/214, Loss: 0.0014415880432352424\n",
      "Iteration: 134/214, Loss: 0.0382189117372036\n",
      "Iteration: 135/214, Loss: 0.04676872491836548\n",
      "Iteration: 136/214, Loss: 0.0013189035234972835\n",
      "Iteration: 137/214, Loss: 0.0015617676544934511\n",
      "Iteration: 138/214, Loss: 0.0577668771147728\n",
      "Iteration: 139/214, Loss: 0.07626815885305405\n",
      "Iteration: 140/214, Loss: 0.39123573899269104\n",
      "Iteration: 141/214, Loss: 0.04134604334831238\n",
      "Iteration: 142/214, Loss: 0.39316415786743164\n",
      "Iteration: 143/214, Loss: 0.07262936234474182\n",
      "Iteration: 144/214, Loss: 0.1684868335723877\n",
      "Iteration: 145/214, Loss: 0.06970150023698807\n",
      "Iteration: 146/214, Loss: 0.028408806771039963\n",
      "Iteration: 147/214, Loss: 0.29410606622695923\n",
      "Iteration: 148/214, Loss: 0.15204550325870514\n",
      "Iteration: 149/214, Loss: 0.05128934234380722\n",
      "Iteration: 150/214, Loss: 0.06914250552654266\n",
      "Iteration: 151/214, Loss: 0.02694098837673664\n",
      "Iteration: 152/214, Loss: 0.10137288272380829\n",
      "Iteration: 153/214, Loss: 0.04942500963807106\n",
      "Iteration: 154/214, Loss: 0.02991897240281105\n",
      "Iteration: 155/214, Loss: 0.42017626762390137\n",
      "Iteration: 156/214, Loss: 0.20177417993545532\n",
      "Iteration: 157/214, Loss: 0.00236441008746624\n",
      "Iteration: 158/214, Loss: 0.07644926011562347\n",
      "Iteration: 159/214, Loss: 0.026109931990504265\n",
      "Iteration: 160/214, Loss: 0.09186875820159912\n",
      "Iteration: 161/214, Loss: 0.24610716104507446\n",
      "Iteration: 162/214, Loss: 0.036473702639341354\n",
      "Iteration: 163/214, Loss: 0.14758075773715973\n",
      "Iteration: 164/214, Loss: 0.03774343430995941\n",
      "Iteration: 165/214, Loss: 0.28218021988868713\n",
      "Iteration: 166/214, Loss: 0.0009025220642797649\n",
      "Iteration: 167/214, Loss: 0.041668400168418884\n",
      "Iteration: 168/214, Loss: 0.026074320077896118\n",
      "Iteration: 169/214, Loss: 0.1723639965057373\n",
      "Iteration: 170/214, Loss: 0.2271217256784439\n",
      "Iteration: 171/214, Loss: 0.28596970438957214\n",
      "Iteration: 172/214, Loss: 0.11060232669115067\n",
      "Iteration: 173/214, Loss: 0.030627597123384476\n",
      "Iteration: 174/214, Loss: 0.02256176434457302\n",
      "Iteration: 175/214, Loss: 0.25713738799095154\n",
      "Iteration: 176/214, Loss: 0.06720282137393951\n",
      "Iteration: 177/214, Loss: 0.20152217149734497\n",
      "Iteration: 178/214, Loss: 0.0750759094953537\n",
      "Iteration: 179/214, Loss: 0.02298518642783165\n",
      "Iteration: 180/214, Loss: 0.00047725444892421365\n",
      "Iteration: 181/214, Loss: 0.17337150871753693\n",
      "Iteration: 182/214, Loss: 0.04274475947022438\n",
      "Iteration: 183/214, Loss: 0.5416486263275146\n",
      "Iteration: 184/214, Loss: 0.1629771590232849\n",
      "Iteration: 185/214, Loss: 0.0961378961801529\n",
      "Iteration: 186/214, Loss: 0.08615119010210037\n",
      "Iteration: 187/214, Loss: 0.1128707155585289\n",
      "Iteration: 188/214, Loss: 0.15493778884410858\n",
      "Iteration: 189/214, Loss: 0.46120771765708923\n",
      "Iteration: 190/214, Loss: 0.2154468446969986\n",
      "Iteration: 191/214, Loss: 0.2548675835132599\n",
      "Iteration: 192/214, Loss: 0.12650294601917267\n",
      "Iteration: 193/214, Loss: 0.3059808015823364\n",
      "Iteration: 194/214, Loss: 0.00790976919233799\n",
      "Iteration: 195/214, Loss: 0.09694229066371918\n",
      "Iteration: 196/214, Loss: 0.38965779542922974\n",
      "Iteration: 197/214, Loss: 0.27254819869995117\n",
      "Iteration: 198/214, Loss: 0.10714143514633179\n",
      "Iteration: 199/214, Loss: 0.045702312141656876\n",
      "Iteration: 200/214, Loss: 0.32997262477874756\n",
      "Iteration: 201/214, Loss: 0.2101285457611084\n",
      "Iteration: 202/214, Loss: 0.40064120292663574\n",
      "Iteration: 203/214, Loss: 0.02680649422109127\n",
      "Iteration: 204/214, Loss: 0.1250661164522171\n",
      "Iteration: 205/214, Loss: 0.2036043405532837\n",
      "Iteration: 206/214, Loss: 0.03316080570220947\n",
      "Iteration: 207/214, Loss: 0.3364305794239044\n",
      "Iteration: 208/214, Loss: 0.019879931584000587\n",
      "Iteration: 209/214, Loss: 0.10514281690120697\n",
      "Iteration: 210/214, Loss: 0.2636898458003998\n",
      "Iteration: 211/214, Loss: 0.09327712655067444\n",
      "Iteration: 212/214, Loss: 0.09560894966125488\n",
      "Iteration: 213/214, Loss: 0.32629305124282837\n",
      "Iteration: 214/214, Loss: 0.08515149354934692\n",
      "tensor(31.3264, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Iteration: 1/214, Loss: 0.05207490548491478\n",
      "Iteration: 2/214, Loss: 0.03540276736021042\n",
      "Iteration: 3/214, Loss: 0.033539485186338425\n",
      "Iteration: 4/214, Loss: 0.31398501992225647\n",
      "Iteration: 5/214, Loss: 0.06481797248125076\n",
      "Iteration: 6/214, Loss: 0.018244324252009392\n",
      "Iteration: 7/214, Loss: 0.060961414128541946\n",
      "Iteration: 8/214, Loss: 0.0334908552467823\n",
      "Iteration: 9/214, Loss: 0.11973262578248978\n",
      "Iteration: 10/214, Loss: 0.33941128849983215\n",
      "Iteration: 11/214, Loss: 0.1821025162935257\n",
      "Iteration: 12/214, Loss: 0.14434197545051575\n",
      "Iteration: 13/214, Loss: 0.0012923611793667078\n",
      "Iteration: 14/214, Loss: 0.4732391834259033\n",
      "Iteration: 15/214, Loss: 0.0330667681992054\n",
      "Iteration: 16/214, Loss: 0.04474833235144615\n",
      "Iteration: 17/214, Loss: 0.7246644496917725\n",
      "Iteration: 18/214, Loss: 0.0006613319274038076\n",
      "Iteration: 19/214, Loss: 0.0012250165455043316\n",
      "Iteration: 20/214, Loss: 0.22644725441932678\n",
      "Iteration: 21/214, Loss: 0.06346451491117477\n",
      "Iteration: 22/214, Loss: 0.2140495628118515\n",
      "Iteration: 23/214, Loss: 0.04028624668717384\n",
      "Iteration: 24/214, Loss: 0.30525535345077515\n",
      "Iteration: 25/214, Loss: 0.08961337059736252\n",
      "Iteration: 26/214, Loss: 0.11751735955476761\n",
      "Iteration: 27/214, Loss: 0.07756348699331284\n",
      "Iteration: 28/214, Loss: 0.10398885607719421\n",
      "Iteration: 29/214, Loss: 0.08515296131372452\n",
      "Iteration: 30/214, Loss: 0.20581567287445068\n",
      "Iteration: 31/214, Loss: 0.05681098997592926\n",
      "Iteration: 32/214, Loss: 0.10293304175138474\n",
      "Iteration: 33/214, Loss: 0.005730419885367155\n",
      "Iteration: 34/214, Loss: 0.0814051479101181\n",
      "Iteration: 35/214, Loss: 0.02407057210803032\n",
      "Iteration: 36/214, Loss: 0.10621251165866852\n",
      "Iteration: 37/214, Loss: 0.13462917506694794\n",
      "Iteration: 38/214, Loss: 0.13333575427532196\n",
      "Iteration: 39/214, Loss: 0.1829744130373001\n",
      "Iteration: 40/214, Loss: 0.38461992144584656\n",
      "Iteration: 41/214, Loss: 0.09221995621919632\n",
      "Iteration: 42/214, Loss: 0.19365210831165314\n",
      "Iteration: 43/214, Loss: 0.12078282237052917\n",
      "Iteration: 44/214, Loss: 0.0012749365996569395\n",
      "Iteration: 45/214, Loss: 0.3150380849838257\n",
      "Iteration: 46/214, Loss: 0.030765235424041748\n",
      "Iteration: 47/214, Loss: 0.3003620207309723\n",
      "Iteration: 48/214, Loss: 0.04890623688697815\n",
      "Iteration: 49/214, Loss: 0.05292172357439995\n",
      "Iteration: 50/214, Loss: 0.04507097974419594\n",
      "Iteration: 51/214, Loss: 0.06720371544361115\n",
      "Iteration: 52/214, Loss: 0.1925571858882904\n",
      "Iteration: 53/214, Loss: 0.06349290907382965\n",
      "Iteration: 54/214, Loss: 0.03689137473702431\n",
      "Iteration: 55/214, Loss: 0.21879902482032776\n",
      "Iteration: 56/214, Loss: 0.02912537194788456\n",
      "Iteration: 57/214, Loss: 0.042319223284721375\n",
      "Iteration: 58/214, Loss: 0.03200427442789078\n",
      "Iteration: 59/214, Loss: 0.19306378066539764\n",
      "Iteration: 60/214, Loss: 0.002399235498160124\n",
      "Iteration: 61/214, Loss: 0.13635824620723724\n",
      "Iteration: 62/214, Loss: 0.058989010751247406\n",
      "Iteration: 63/214, Loss: 0.00284199183806777\n",
      "Iteration: 64/214, Loss: 0.040554191917181015\n",
      "Iteration: 65/214, Loss: 0.3868882656097412\n",
      "Iteration: 66/214, Loss: 0.38223305344581604\n",
      "Iteration: 67/214, Loss: 0.27974680066108704\n",
      "Iteration: 68/214, Loss: 0.08036854863166809\n",
      "Iteration: 69/214, Loss: 0.10672217607498169\n",
      "Iteration: 70/214, Loss: 0.0707826018333435\n",
      "Iteration: 71/214, Loss: 0.36952894926071167\n",
      "Iteration: 72/214, Loss: 0.2049771249294281\n",
      "Iteration: 73/214, Loss: 0.16767893731594086\n",
      "Iteration: 74/214, Loss: 0.15902696549892426\n",
      "Iteration: 75/214, Loss: 0.2649138867855072\n",
      "Iteration: 76/214, Loss: 0.031550243496894836\n",
      "Iteration: 77/214, Loss: 0.06249820068478584\n",
      "Iteration: 78/214, Loss: 0.06925640255212784\n",
      "Iteration: 79/214, Loss: 0.09371111541986465\n",
      "Iteration: 80/214, Loss: 0.08283240348100662\n",
      "Iteration: 81/214, Loss: 0.08640892803668976\n",
      "Iteration: 82/214, Loss: 0.02270163781940937\n",
      "Iteration: 83/214, Loss: 0.03088253177702427\n",
      "Iteration: 84/214, Loss: 0.026619596406817436\n",
      "Iteration: 85/214, Loss: 0.20061179995536804\n",
      "Iteration: 86/214, Loss: 0.1046791598200798\n",
      "Iteration: 87/214, Loss: 0.17199471592903137\n",
      "Iteration: 88/214, Loss: 0.0013045133091509342\n",
      "Iteration: 89/214, Loss: 0.02456948533654213\n",
      "Iteration: 90/214, Loss: 0.025398654863238335\n",
      "Iteration: 91/214, Loss: 0.046481356024742126\n",
      "Iteration: 92/214, Loss: 0.10698212683200836\n",
      "Iteration: 93/214, Loss: 0.2499900460243225\n",
      "Iteration: 94/214, Loss: 0.07279091328382492\n",
      "Iteration: 95/214, Loss: 0.2384500801563263\n",
      "Iteration: 96/214, Loss: 0.0418236181139946\n",
      "Iteration: 97/214, Loss: 0.14626416563987732\n",
      "Iteration: 98/214, Loss: 0.3777657747268677\n",
      "Iteration: 99/214, Loss: 0.055998679250478745\n",
      "Iteration: 100/214, Loss: 0.2448842078447342\n",
      "Iteration: 101/214, Loss: 0.16814307868480682\n",
      "Iteration: 102/214, Loss: 0.0016297033289447427\n",
      "Iteration: 103/214, Loss: 0.2856602072715759\n",
      "Iteration: 104/214, Loss: 0.1021927148103714\n",
      "Iteration: 105/214, Loss: 0.16439074277877808\n",
      "Iteration: 106/214, Loss: 0.0031011910177767277\n",
      "Iteration: 107/214, Loss: 0.4175265431404114\n",
      "Iteration: 108/214, Loss: 0.4398094117641449\n",
      "Iteration: 109/214, Loss: 0.056592922657728195\n",
      "Iteration: 110/214, Loss: 0.10676271468400955\n",
      "Iteration: 111/214, Loss: 0.3520721197128296\n",
      "Iteration: 112/214, Loss: 0.12159370630979538\n",
      "Iteration: 113/214, Loss: 0.1407206505537033\n",
      "Iteration: 114/214, Loss: 0.129415825009346\n",
      "Iteration: 115/214, Loss: 0.05047266185283661\n",
      "Iteration: 116/214, Loss: 0.3668087422847748\n",
      "Iteration: 117/214, Loss: 0.2132980227470398\n",
      "Iteration: 118/214, Loss: 0.02905435673892498\n",
      "Iteration: 119/214, Loss: 0.06920210272073746\n",
      "Iteration: 120/214, Loss: 0.22976602613925934\n",
      "Iteration: 121/214, Loss: 0.033661507070064545\n",
      "Iteration: 122/214, Loss: 0.10118159651756287\n",
      "Iteration: 123/214, Loss: 0.06667060405015945\n",
      "Iteration: 124/214, Loss: 0.0324455201625824\n",
      "Iteration: 125/214, Loss: 0.03924930840730667\n",
      "Iteration: 126/214, Loss: 0.037776656448841095\n",
      "Iteration: 127/214, Loss: 0.049448784440755844\n",
      "Iteration: 128/214, Loss: 0.17309589684009552\n",
      "Iteration: 129/214, Loss: 0.3328452706336975\n",
      "Iteration: 130/214, Loss: 0.2398044764995575\n",
      "Iteration: 131/214, Loss: 0.02089957892894745\n",
      "Iteration: 132/214, Loss: 0.2897707223892212\n",
      "Iteration: 133/214, Loss: 0.0009639126947149634\n",
      "Iteration: 134/214, Loss: 0.017310794442892075\n",
      "Iteration: 135/214, Loss: 0.02711016871035099\n",
      "Iteration: 136/214, Loss: 0.0016249911859631538\n",
      "Iteration: 137/214, Loss: 0.0012104845372959971\n",
      "Iteration: 138/214, Loss: 0.05236174538731575\n",
      "Iteration: 139/214, Loss: 0.046456094831228256\n",
      "Iteration: 140/214, Loss: 0.32093480229377747\n",
      "Iteration: 141/214, Loss: 0.03198583796620369\n",
      "Iteration: 142/214, Loss: 0.37534067034721375\n",
      "Iteration: 143/214, Loss: 0.03818090632557869\n",
      "Iteration: 144/214, Loss: 0.15256184339523315\n",
      "Iteration: 145/214, Loss: 0.05071048438549042\n",
      "Iteration: 146/214, Loss: 0.025344526395201683\n",
      "Iteration: 147/214, Loss: 0.195734903216362\n",
      "Iteration: 148/214, Loss: 0.11535558104515076\n",
      "Iteration: 149/214, Loss: 0.031869255006313324\n",
      "Iteration: 150/214, Loss: 0.035644471645355225\n",
      "Iteration: 151/214, Loss: 0.04515579715371132\n",
      "Iteration: 152/214, Loss: 0.07401500642299652\n",
      "Iteration: 153/214, Loss: 0.042111702263355255\n",
      "Iteration: 154/214, Loss: 0.03305617347359657\n",
      "Iteration: 155/214, Loss: 0.3380148410797119\n",
      "Iteration: 156/214, Loss: 0.19378367066383362\n",
      "Iteration: 157/214, Loss: 0.0008622243185527623\n",
      "Iteration: 158/214, Loss: 0.06442499905824661\n",
      "Iteration: 159/214, Loss: 0.04506312683224678\n",
      "Iteration: 160/214, Loss: 0.09641017764806747\n",
      "Iteration: 161/214, Loss: 0.295444518327713\n",
      "Iteration: 162/214, Loss: 0.04568181186914444\n",
      "Iteration: 163/214, Loss: 0.12450059503316879\n",
      "Iteration: 164/214, Loss: 0.04736842215061188\n",
      "Iteration: 165/214, Loss: 0.2934737503528595\n",
      "Iteration: 166/214, Loss: 0.0009878643322736025\n",
      "Iteration: 167/214, Loss: 0.0162209440022707\n",
      "Iteration: 168/214, Loss: 0.021190427243709564\n",
      "Iteration: 169/214, Loss: 0.16869240999221802\n",
      "Iteration: 170/214, Loss: 0.28089025616645813\n",
      "Iteration: 171/214, Loss: 0.27007433772087097\n",
      "Iteration: 172/214, Loss: 0.11400080472230911\n",
      "Iteration: 173/214, Loss: 0.015068196691572666\n",
      "Iteration: 174/214, Loss: 0.021092597395181656\n",
      "Iteration: 175/214, Loss: 0.2272176295518875\n",
      "Iteration: 176/214, Loss: 0.05674665793776512\n",
      "Iteration: 177/214, Loss: 0.17853431403636932\n",
      "Iteration: 178/214, Loss: 0.055267829447984695\n",
      "Iteration: 179/214, Loss: 0.031016230583190918\n",
      "Iteration: 180/214, Loss: 0.000613517826423049\n",
      "Iteration: 181/214, Loss: 0.1556752771139145\n",
      "Iteration: 182/214, Loss: 0.04453154653310776\n",
      "Iteration: 183/214, Loss: 0.2768922448158264\n",
      "Iteration: 184/214, Loss: 0.21808384358882904\n",
      "Iteration: 185/214, Loss: 0.07138289511203766\n",
      "Iteration: 186/214, Loss: 0.052454836666584015\n",
      "Iteration: 187/214, Loss: 0.05219506844878197\n",
      "Iteration: 188/214, Loss: 0.06030474230647087\n",
      "Iteration: 189/214, Loss: 0.33131009340286255\n",
      "Iteration: 190/214, Loss: 0.09556876122951508\n",
      "Iteration: 191/214, Loss: 0.2143416553735733\n",
      "Iteration: 192/214, Loss: 0.049025923013687134\n",
      "Iteration: 193/214, Loss: 0.26635968685150146\n",
      "Iteration: 194/214, Loss: 0.005188245791941881\n",
      "Iteration: 195/214, Loss: 0.1232474073767662\n",
      "Iteration: 196/214, Loss: 0.2823667526245117\n",
      "Iteration: 197/214, Loss: 0.258387953042984\n",
      "Iteration: 198/214, Loss: 0.13879315555095673\n",
      "Iteration: 199/214, Loss: 0.025831308215856552\n",
      "Iteration: 200/214, Loss: 0.25623011589050293\n",
      "Iteration: 201/214, Loss: 0.23752403259277344\n",
      "Iteration: 202/214, Loss: 0.3888028562068939\n",
      "Iteration: 203/214, Loss: 0.03906268998980522\n",
      "Iteration: 204/214, Loss: 0.13319046795368195\n",
      "Iteration: 205/214, Loss: 0.24186350405216217\n",
      "Iteration: 206/214, Loss: 0.03747808188199997\n",
      "Iteration: 207/214, Loss: 0.2784769535064697\n",
      "Iteration: 208/214, Loss: 0.02771761454641819\n",
      "Iteration: 209/214, Loss: 0.05455642566084862\n",
      "Iteration: 210/214, Loss: 0.2507101893424988\n",
      "Iteration: 211/214, Loss: 0.0890720933675766\n",
      "Iteration: 212/214, Loss: 0.08318162709474564\n",
      "Iteration: 213/214, Loss: 0.21160317957401276\n",
      "Iteration: 214/214, Loss: 0.09579170495271683\n",
      "tensor(27.5557, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Iteration: 1/214, Loss: 0.039836812764406204\n",
      "Iteration: 2/214, Loss: 0.051631998270750046\n",
      "Iteration: 3/214, Loss: 0.03690920025110245\n",
      "Iteration: 4/214, Loss: 0.3533686399459839\n",
      "Iteration: 5/214, Loss: 0.0340665765106678\n",
      "Iteration: 6/214, Loss: 0.04197732359170914\n",
      "Iteration: 7/214, Loss: 0.04868519678711891\n",
      "Iteration: 8/214, Loss: 0.028570976108312607\n",
      "Iteration: 9/214, Loss: 0.11541431397199631\n",
      "Iteration: 10/214, Loss: 0.29198336601257324\n",
      "Iteration: 11/214, Loss: 0.13366523385047913\n",
      "Iteration: 12/214, Loss: 0.1433139443397522\n",
      "Iteration: 13/214, Loss: 0.0008639533771201968\n",
      "Iteration: 14/214, Loss: 0.5571444630622864\n",
      "Iteration: 15/214, Loss: 0.048799362033605576\n",
      "Iteration: 16/214, Loss: 0.037091486155986786\n",
      "Iteration: 17/214, Loss: 0.7740651369094849\n",
      "Iteration: 18/214, Loss: 0.0001995843049371615\n",
      "Iteration: 19/214, Loss: 0.0005219095619395375\n",
      "Iteration: 20/214, Loss: 0.2259940207004547\n",
      "Iteration: 21/214, Loss: 0.05166640877723694\n",
      "Iteration: 22/214, Loss: 0.1693086475133896\n",
      "Iteration: 23/214, Loss: 0.04015953466296196\n",
      "Iteration: 24/214, Loss: 0.2770318388938904\n",
      "Iteration: 25/214, Loss: 0.08259649574756622\n",
      "Iteration: 26/214, Loss: 0.14240653812885284\n",
      "Iteration: 27/214, Loss: 0.0861653983592987\n",
      "Iteration: 28/214, Loss: 0.12979687750339508\n",
      "Iteration: 29/214, Loss: 0.053325120359659195\n",
      "Iteration: 30/214, Loss: 0.1580440253019333\n",
      "Iteration: 31/214, Loss: 0.0544310137629509\n",
      "Iteration: 32/214, Loss: 0.08840424567461014\n",
      "Iteration: 33/214, Loss: 0.0014639620203524828\n",
      "Iteration: 34/214, Loss: 0.052675195038318634\n",
      "Iteration: 35/214, Loss: 0.029883038252592087\n",
      "Iteration: 36/214, Loss: 0.09748867154121399\n",
      "Iteration: 37/214, Loss: 0.11200594902038574\n",
      "Iteration: 38/214, Loss: 0.14495274424552917\n",
      "Iteration: 39/214, Loss: 0.22365806996822357\n",
      "Iteration: 40/214, Loss: 0.34628647565841675\n",
      "Iteration: 41/214, Loss: 0.08732525259256363\n",
      "Iteration: 42/214, Loss: 0.1292761117219925\n",
      "Iteration: 43/214, Loss: 0.11760524660348892\n",
      "Iteration: 44/214, Loss: 0.0004890062264166772\n",
      "Iteration: 45/214, Loss: 0.29615211486816406\n",
      "Iteration: 46/214, Loss: 0.03306342661380768\n",
      "Iteration: 47/214, Loss: 0.30231937766075134\n",
      "Iteration: 48/214, Loss: 0.04862440750002861\n",
      "Iteration: 49/214, Loss: 0.029283026233315468\n",
      "Iteration: 50/214, Loss: 0.033981770277023315\n",
      "Iteration: 51/214, Loss: 0.1058214008808136\n",
      "Iteration: 52/214, Loss: 0.21519799530506134\n",
      "Iteration: 53/214, Loss: 0.05668056383728981\n",
      "Iteration: 54/214, Loss: 0.027346912771463394\n",
      "Iteration: 55/214, Loss: 0.13699570298194885\n",
      "Iteration: 56/214, Loss: 0.02486877702176571\n",
      "Iteration: 57/214, Loss: 0.03927231207489967\n",
      "Iteration: 58/214, Loss: 0.019684864208102226\n",
      "Iteration: 59/214, Loss: 0.1315063089132309\n",
      "Iteration: 60/214, Loss: 0.002890177071094513\n",
      "Iteration: 61/214, Loss: 0.11757341027259827\n",
      "Iteration: 62/214, Loss: 0.05111627280712128\n",
      "Iteration: 63/214, Loss: 0.0008476421935483813\n",
      "Iteration: 64/214, Loss: 0.06267478317022324\n",
      "Iteration: 65/214, Loss: 0.2705538272857666\n",
      "Iteration: 66/214, Loss: 0.3452226519584656\n",
      "Iteration: 67/214, Loss: 0.31856921315193176\n",
      "Iteration: 68/214, Loss: 0.10572392493486404\n",
      "Iteration: 69/214, Loss: 0.0630284920334816\n",
      "Iteration: 70/214, Loss: 0.037765566259622574\n",
      "Iteration: 71/214, Loss: 0.3675905764102936\n",
      "Iteration: 72/214, Loss: 0.2730783224105835\n",
      "Iteration: 73/214, Loss: 0.14565734565258026\n",
      "Iteration: 74/214, Loss: 0.12136673182249069\n",
      "Iteration: 75/214, Loss: 0.23100624978542328\n",
      "Iteration: 76/214, Loss: 0.01910235360264778\n",
      "Iteration: 77/214, Loss: 0.06739043444395065\n",
      "Iteration: 78/214, Loss: 0.13928671181201935\n",
      "Iteration: 79/214, Loss: 0.12569734454154968\n",
      "Iteration: 80/214, Loss: 0.13266099989414215\n",
      "Iteration: 81/214, Loss: 0.175784170627594\n",
      "Iteration: 82/214, Loss: 0.03216318041086197\n",
      "Iteration: 83/214, Loss: 0.028134308755397797\n",
      "Iteration: 84/214, Loss: 0.03752380982041359\n",
      "Iteration: 85/214, Loss: 0.26402547955513\n",
      "Iteration: 86/214, Loss: 0.10966933518648148\n",
      "Iteration: 87/214, Loss: 0.20156103372573853\n",
      "Iteration: 88/214, Loss: 0.0008112838841043413\n",
      "Iteration: 89/214, Loss: 0.021732492372393608\n",
      "Iteration: 90/214, Loss: 0.03476198762655258\n",
      "Iteration: 91/214, Loss: 0.0592014454305172\n",
      "Iteration: 92/214, Loss: 0.1452643722295761\n",
      "Iteration: 93/214, Loss: 0.2839670777320862\n",
      "Iteration: 94/214, Loss: 0.07770194113254547\n",
      "Iteration: 95/214, Loss: 0.21705283224582672\n",
      "Iteration: 96/214, Loss: 0.04918738082051277\n",
      "Iteration: 97/214, Loss: 0.0717412605881691\n",
      "Iteration: 98/214, Loss: 0.24364073574543\n",
      "Iteration: 99/214, Loss: 0.050666600465774536\n",
      "Iteration: 100/214, Loss: 0.1444431096315384\n",
      "Iteration: 101/214, Loss: 0.11468273401260376\n",
      "Iteration: 102/214, Loss: 0.0007946811383590102\n",
      "Iteration: 103/214, Loss: 0.22048945724964142\n",
      "Iteration: 104/214, Loss: 0.062286362051963806\n",
      "Iteration: 105/214, Loss: 0.13105617463588715\n",
      "Iteration: 106/214, Loss: 0.0020621023140847683\n",
      "Iteration: 107/214, Loss: 0.2969304323196411\n",
      "Iteration: 108/214, Loss: 0.32847118377685547\n",
      "Iteration: 109/214, Loss: 0.042588334530591965\n",
      "Iteration: 110/214, Loss: 0.0685587078332901\n",
      "Iteration: 111/214, Loss: 0.24774739146232605\n",
      "Iteration: 112/214, Loss: 0.1419670134782791\n",
      "Iteration: 113/214, Loss: 0.17289762198925018\n",
      "Iteration: 114/214, Loss: 0.15463273227214813\n",
      "Iteration: 115/214, Loss: 0.03821825608611107\n",
      "Iteration: 116/214, Loss: 0.2930675446987152\n",
      "Iteration: 117/214, Loss: 0.0995202362537384\n",
      "Iteration: 118/214, Loss: 0.03842921555042267\n",
      "Iteration: 119/214, Loss: 0.06297449767589569\n",
      "Iteration: 120/214, Loss: 0.13917402923107147\n",
      "Iteration: 121/214, Loss: 0.037064261734485626\n",
      "Iteration: 122/214, Loss: 0.1467275768518448\n",
      "Iteration: 123/214, Loss: 0.08756489306688309\n",
      "Iteration: 124/214, Loss: 0.040044598281383514\n",
      "Iteration: 125/214, Loss: 0.049762025475502014\n",
      "Iteration: 126/214, Loss: 0.04923253878951073\n",
      "Iteration: 127/214, Loss: 0.054387468844652176\n",
      "Iteration: 128/214, Loss: 0.1761431246995926\n",
      "Iteration: 129/214, Loss: 0.34664803743362427\n",
      "Iteration: 130/214, Loss: 0.27490749955177307\n",
      "Iteration: 131/214, Loss: 0.026605241000652313\n",
      "Iteration: 132/214, Loss: 0.23726972937583923\n",
      "Iteration: 133/214, Loss: 0.0015191743150353432\n",
      "Iteration: 134/214, Loss: 0.05689531937241554\n",
      "Iteration: 135/214, Loss: 0.042481712996959686\n",
      "Iteration: 136/214, Loss: 0.0004404888313729316\n",
      "Iteration: 137/214, Loss: 0.0012441470753401518\n",
      "Iteration: 138/214, Loss: 0.060209233313798904\n",
      "Iteration: 139/214, Loss: 0.09853602200746536\n",
      "Iteration: 140/214, Loss: 0.5809348225593567\n",
      "Iteration: 141/214, Loss: 0.08347589522600174\n",
      "Iteration: 142/214, Loss: 0.6226363182067871\n",
      "Iteration: 143/214, Loss: 0.0399133674800396\n",
      "Iteration: 144/214, Loss: 0.3122825026512146\n",
      "Iteration: 145/214, Loss: 0.0871010273694992\n",
      "Iteration: 146/214, Loss: 0.04212770238518715\n",
      "Iteration: 147/214, Loss: 0.2149420976638794\n",
      "Iteration: 148/214, Loss: 0.15046840906143188\n",
      "Iteration: 149/214, Loss: 0.036156609654426575\n",
      "Iteration: 150/214, Loss: 0.04080875590443611\n",
      "Iteration: 151/214, Loss: 0.03043290041387081\n",
      "Iteration: 152/214, Loss: 0.10670634359121323\n",
      "Iteration: 153/214, Loss: 0.08029432594776154\n",
      "Iteration: 154/214, Loss: 0.05167997255921364\n",
      "Iteration: 155/214, Loss: 0.29774796962738037\n",
      "Iteration: 156/214, Loss: 0.2716004252433777\n",
      "Iteration: 157/214, Loss: 0.002520886715501547\n",
      "Iteration: 158/214, Loss: 0.09946499019861221\n",
      "Iteration: 159/214, Loss: 0.04866539686918259\n",
      "Iteration: 160/214, Loss: 0.09539590775966644\n",
      "Iteration: 161/214, Loss: 0.26858779788017273\n",
      "Iteration: 162/214, Loss: 0.03220390900969505\n",
      "Iteration: 163/214, Loss: 0.1063946932554245\n",
      "Iteration: 164/214, Loss: 0.02485596388578415\n",
      "Iteration: 165/214, Loss: 0.3442537784576416\n",
      "Iteration: 166/214, Loss: 0.0026438282802700996\n",
      "Iteration: 167/214, Loss: 0.054287511855363846\n",
      "Iteration: 168/214, Loss: 0.03982781246304512\n",
      "Iteration: 169/214, Loss: 0.2584953308105469\n",
      "Iteration: 170/214, Loss: 0.25373533368110657\n",
      "Iteration: 171/214, Loss: 0.2461174577474594\n",
      "Iteration: 172/214, Loss: 0.09430884569883347\n",
      "Iteration: 173/214, Loss: 0.026355715468525887\n",
      "Iteration: 174/214, Loss: 0.015806131064891815\n",
      "Iteration: 175/214, Loss: 0.15390193462371826\n",
      "Iteration: 176/214, Loss: 0.10770411789417267\n",
      "Iteration: 177/214, Loss: 0.20016524195671082\n",
      "Iteration: 178/214, Loss: 0.06998255103826523\n",
      "Iteration: 179/214, Loss: 0.023663969710469246\n",
      "Iteration: 180/214, Loss: 0.00027518393471837044\n",
      "Iteration: 181/214, Loss: 0.12894561886787415\n",
      "Iteration: 182/214, Loss: 0.026080705225467682\n",
      "Iteration: 183/214, Loss: 0.23819474875926971\n",
      "Iteration: 184/214, Loss: 0.13412438333034515\n",
      "Iteration: 185/214, Loss: 0.04526854678988457\n",
      "Iteration: 186/214, Loss: 0.04019488766789436\n",
      "Iteration: 187/214, Loss: 0.04283873364329338\n",
      "Iteration: 188/214, Loss: 0.07492806762456894\n",
      "Iteration: 189/214, Loss: 0.37819215655326843\n",
      "Iteration: 190/214, Loss: 0.12290742248296738\n",
      "Iteration: 191/214, Loss: 0.2024790495634079\n",
      "Iteration: 192/214, Loss: 0.059795208275318146\n",
      "Iteration: 193/214, Loss: 0.20920559763908386\n",
      "Iteration: 194/214, Loss: 0.004406662657856941\n",
      "Iteration: 195/214, Loss: 0.07188231498003006\n",
      "Iteration: 196/214, Loss: 0.2943330407142639\n",
      "Iteration: 197/214, Loss: 0.2896276116371155\n",
      "Iteration: 198/214, Loss: 0.1427578330039978\n",
      "Iteration: 199/214, Loss: 0.042247965931892395\n",
      "Iteration: 200/214, Loss: 0.3084149658679962\n",
      "Iteration: 201/214, Loss: 0.2209843099117279\n",
      "Iteration: 202/214, Loss: 0.3178282678127289\n",
      "Iteration: 203/214, Loss: 0.04414272680878639\n",
      "Iteration: 204/214, Loss: 0.17451657354831696\n",
      "Iteration: 205/214, Loss: 0.3413851857185364\n",
      "Iteration: 206/214, Loss: 0.05767018347978592\n",
      "Iteration: 207/214, Loss: 0.29851892590522766\n",
      "Iteration: 208/214, Loss: 0.048608072102069855\n",
      "Iteration: 209/214, Loss: 0.06619821488857269\n",
      "Iteration: 210/214, Loss: 0.29789048433303833\n",
      "Iteration: 211/214, Loss: 0.08390028774738312\n",
      "Iteration: 212/214, Loss: 0.1090230867266655\n",
      "Iteration: 213/214, Loss: 0.27670818567276\n",
      "Iteration: 214/214, Loss: 0.09784924983978271\n",
      "tensor(27.7444, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Iteration: 1/214, Loss: 0.060366373509168625\n",
      "Iteration: 2/214, Loss: 0.05409645661711693\n",
      "Iteration: 3/214, Loss: 0.06626410782337189\n",
      "Iteration: 4/214, Loss: 0.3870653212070465\n",
      "Iteration: 5/214, Loss: 0.05250959470868111\n",
      "Iteration: 6/214, Loss: 0.06633301079273224\n",
      "Iteration: 7/214, Loss: 0.11181852966547012\n",
      "Iteration: 8/214, Loss: 0.06511577218770981\n",
      "Iteration: 9/214, Loss: 0.1371055394411087\n",
      "Iteration: 10/214, Loss: 0.4158340394496918\n",
      "Iteration: 11/214, Loss: 0.21954567730426788\n",
      "Iteration: 12/214, Loss: 0.12819845974445343\n",
      "Iteration: 13/214, Loss: 0.0005984118906781077\n",
      "Iteration: 14/214, Loss: 0.5250830054283142\n",
      "Iteration: 15/214, Loss: 0.06135464087128639\n",
      "Iteration: 16/214, Loss: 0.03596276789903641\n",
      "Iteration: 17/214, Loss: 0.9215630888938904\n",
      "Iteration: 18/214, Loss: 0.0002456913935020566\n",
      "Iteration: 19/214, Loss: 0.0007412837585434318\n",
      "Iteration: 20/214, Loss: 0.21995528042316437\n",
      "Iteration: 21/214, Loss: 0.03509548678994179\n",
      "Iteration: 22/214, Loss: 0.13133777678012848\n",
      "Iteration: 23/214, Loss: 0.05682755261659622\n",
      "Iteration: 24/214, Loss: 0.3039124608039856\n",
      "Iteration: 25/214, Loss: 0.06318605691194534\n",
      "Iteration: 26/214, Loss: 0.1595800369977951\n",
      "Iteration: 27/214, Loss: 0.08087392896413803\n",
      "Iteration: 28/214, Loss: 0.18869464099407196\n",
      "Iteration: 29/214, Loss: 0.04994773119688034\n",
      "Iteration: 30/214, Loss: 0.2622365355491638\n",
      "Iteration: 31/214, Loss: 0.044907618314027786\n",
      "Iteration: 32/214, Loss: 0.132288858294487\n",
      "Iteration: 33/214, Loss: 0.00255406997166574\n",
      "Iteration: 34/214, Loss: 0.06013709679245949\n",
      "Iteration: 35/214, Loss: 0.04445155709981918\n",
      "Iteration: 36/214, Loss: 0.08244787156581879\n",
      "Iteration: 37/214, Loss: 0.11165034025907516\n",
      "Iteration: 38/214, Loss: 0.12552353739738464\n",
      "Iteration: 39/214, Loss: 0.23221321403980255\n",
      "Iteration: 40/214, Loss: 0.3413196802139282\n",
      "Iteration: 41/214, Loss: 0.08439233899116516\n",
      "Iteration: 42/214, Loss: 0.2474995255470276\n",
      "Iteration: 43/214, Loss: 0.16069500148296356\n",
      "Iteration: 44/214, Loss: 0.00068391248350963\n",
      "Iteration: 45/214, Loss: 0.39823535084724426\n",
      "Iteration: 46/214, Loss: 0.033553026616573334\n",
      "Iteration: 47/214, Loss: 0.26275232434272766\n",
      "Iteration: 48/214, Loss: 0.03769891336560249\n",
      "Iteration: 49/214, Loss: 0.033531706780195236\n",
      "Iteration: 50/214, Loss: 0.031754620373249054\n",
      "Iteration: 51/214, Loss: 0.1272643655538559\n",
      "Iteration: 52/214, Loss: 0.2935691177845001\n",
      "Iteration: 53/214, Loss: 0.1142740324139595\n",
      "Iteration: 54/214, Loss: 0.03184736520051956\n",
      "Iteration: 55/214, Loss: 0.18199287354946136\n",
      "Iteration: 56/214, Loss: 0.04906489700078964\n",
      "Iteration: 57/214, Loss: 0.04272857680916786\n",
      "Iteration: 58/214, Loss: 0.03478085994720459\n",
      "Iteration: 59/214, Loss: 0.16642406582832336\n",
      "Iteration: 60/214, Loss: 0.0009580644546076655\n",
      "Iteration: 61/214, Loss: 0.16666385531425476\n",
      "Iteration: 62/214, Loss: 0.05487758666276932\n",
      "Iteration: 63/214, Loss: 0.0009244403918273747\n",
      "Iteration: 64/214, Loss: 0.07509621232748032\n",
      "Iteration: 65/214, Loss: 0.2130446434020996\n",
      "Iteration: 66/214, Loss: 0.31773045659065247\n",
      "Iteration: 67/214, Loss: 0.23275434970855713\n",
      "Iteration: 68/214, Loss: 0.09036790579557419\n",
      "Iteration: 69/214, Loss: 0.049094557762145996\n",
      "Iteration: 70/214, Loss: 0.04804443567991257\n",
      "Iteration: 71/214, Loss: 0.3366526663303375\n",
      "Iteration: 72/214, Loss: 0.18493440747261047\n",
      "Iteration: 73/214, Loss: 0.10319159179925919\n",
      "Iteration: 74/214, Loss: 0.09083390235900879\n",
      "Iteration: 75/214, Loss: 0.21270620822906494\n",
      "Iteration: 76/214, Loss: 0.028369378298521042\n",
      "Iteration: 77/214, Loss: 0.050354667007923126\n",
      "Iteration: 78/214, Loss: 0.075811006128788\n",
      "Iteration: 79/214, Loss: 0.11302012205123901\n",
      "Iteration: 80/214, Loss: 0.10144319385290146\n",
      "Iteration: 81/214, Loss: 0.09903199970722198\n",
      "Iteration: 82/214, Loss: 0.02086014300584793\n",
      "Iteration: 83/214, Loss: 0.03255559876561165\n",
      "Iteration: 84/214, Loss: 0.03726458549499512\n",
      "Iteration: 85/214, Loss: 0.17784550786018372\n",
      "Iteration: 86/214, Loss: 0.05671016126871109\n",
      "Iteration: 87/214, Loss: 0.14639975130558014\n",
      "Iteration: 88/214, Loss: 0.0005745948292315006\n",
      "Iteration: 89/214, Loss: 0.029259897768497467\n",
      "Iteration: 90/214, Loss: 0.028608035296201706\n",
      "Iteration: 91/214, Loss: 0.04532264545559883\n",
      "Iteration: 92/214, Loss: 0.09937430173158646\n",
      "Iteration: 93/214, Loss: 0.22187557816505432\n",
      "Iteration: 94/214, Loss: 0.06349233537912369\n",
      "Iteration: 95/214, Loss: 0.1991238296031952\n",
      "Iteration: 96/214, Loss: 0.0327281728386879\n",
      "Iteration: 97/214, Loss: 0.07644163072109222\n",
      "Iteration: 98/214, Loss: 0.27569299936294556\n",
      "Iteration: 99/214, Loss: 0.03718913346529007\n",
      "Iteration: 100/214, Loss: 0.18239246308803558\n",
      "Iteration: 101/214, Loss: 0.09897638857364655\n",
      "Iteration: 102/214, Loss: 0.0011958342511206865\n",
      "Iteration: 103/214, Loss: 0.21999280154705048\n",
      "Iteration: 104/214, Loss: 0.07265173643827438\n",
      "Iteration: 105/214, Loss: 0.11853419989347458\n",
      "Iteration: 106/214, Loss: 0.001600069459527731\n",
      "Iteration: 107/214, Loss: 0.2629319727420807\n",
      "Iteration: 108/214, Loss: 0.30475690960884094\n",
      "Iteration: 109/214, Loss: 0.04374919831752777\n",
      "Iteration: 110/214, Loss: 0.08991703391075134\n",
      "Iteration: 111/214, Loss: 0.24750947952270508\n",
      "Iteration: 112/214, Loss: 0.13198749721050262\n",
      "Iteration: 113/214, Loss: 0.12908324599266052\n",
      "Iteration: 114/214, Loss: 0.09455928951501846\n",
      "Iteration: 115/214, Loss: 0.024460643529891968\n",
      "Iteration: 116/214, Loss: 0.23517780005931854\n",
      "Iteration: 117/214, Loss: 0.12245305627584457\n",
      "Iteration: 118/214, Loss: 0.04829173535108566\n",
      "Iteration: 119/214, Loss: 0.050911422818899155\n",
      "Iteration: 120/214, Loss: 0.193834125995636\n",
      "Iteration: 121/214, Loss: 0.03827504813671112\n",
      "Iteration: 122/214, Loss: 0.07496560364961624\n",
      "Iteration: 123/214, Loss: 0.06844275444746017\n",
      "Iteration: 124/214, Loss: 0.020082803443074226\n",
      "Iteration: 125/214, Loss: 0.013506263494491577\n",
      "Iteration: 126/214, Loss: 0.022069083526730537\n",
      "Iteration: 127/214, Loss: 0.04298568144440651\n",
      "Iteration: 128/214, Loss: 0.13103578984737396\n",
      "Iteration: 129/214, Loss: 0.20096909999847412\n",
      "Iteration: 130/214, Loss: 0.1773177981376648\n",
      "Iteration: 131/214, Loss: 0.03353647515177727\n",
      "Iteration: 132/214, Loss: 0.19753023982048035\n",
      "Iteration: 133/214, Loss: 0.001202194020152092\n",
      "Iteration: 134/214, Loss: 0.02982150949537754\n",
      "Iteration: 135/214, Loss: 0.027592143043875694\n",
      "Iteration: 136/214, Loss: 0.00038465866236947477\n",
      "Iteration: 137/214, Loss: 0.0010274150408804417\n",
      "Iteration: 138/214, Loss: 0.02783985063433647\n",
      "Iteration: 139/214, Loss: 0.04740377888083458\n",
      "Iteration: 140/214, Loss: 0.2961832582950592\n",
      "Iteration: 141/214, Loss: 0.05539198964834213\n",
      "Iteration: 142/214, Loss: 0.3514340817928314\n",
      "Iteration: 143/214, Loss: 0.03380933776497841\n",
      "Iteration: 144/214, Loss: 0.16441889107227325\n",
      "Iteration: 145/214, Loss: 0.06563900411128998\n",
      "Iteration: 146/214, Loss: 0.018188243731856346\n",
      "Iteration: 147/214, Loss: 0.15020780265331268\n",
      "Iteration: 148/214, Loss: 0.08915853500366211\n",
      "Iteration: 149/214, Loss: 0.03460153192281723\n",
      "Iteration: 150/214, Loss: 0.03201304376125336\n",
      "Iteration: 151/214, Loss: 0.019880736246705055\n",
      "Iteration: 152/214, Loss: 0.05825177952647209\n",
      "Iteration: 153/214, Loss: 0.06647704541683197\n",
      "Iteration: 154/214, Loss: 0.02149958163499832\n",
      "Iteration: 155/214, Loss: 0.22396230697631836\n",
      "Iteration: 156/214, Loss: 0.20669448375701904\n",
      "Iteration: 157/214, Loss: 0.0007666645105928183\n",
      "Iteration: 158/214, Loss: 0.06431976705789566\n",
      "Iteration: 159/214, Loss: 0.03535070642828941\n",
      "Iteration: 160/214, Loss: 0.0733863040804863\n",
      "Iteration: 161/214, Loss: 0.190965935587883\n",
      "Iteration: 162/214, Loss: 0.02254772186279297\n",
      "Iteration: 163/214, Loss: 0.09640603512525558\n",
      "Iteration: 164/214, Loss: 0.026019060984253883\n",
      "Iteration: 165/214, Loss: 0.2538394629955292\n",
      "Iteration: 166/214, Loss: 0.0011671282118186355\n",
      "Iteration: 167/214, Loss: 0.022067442536354065\n",
      "Iteration: 168/214, Loss: 0.0320967361330986\n",
      "Iteration: 169/214, Loss: 0.1461140513420105\n",
      "Iteration: 170/214, Loss: 0.19051498174667358\n",
      "Iteration: 171/214, Loss: 0.2257632315158844\n",
      "Iteration: 172/214, Loss: 0.06867754459381104\n",
      "Iteration: 173/214, Loss: 0.02867140993475914\n",
      "Iteration: 174/214, Loss: 0.024428190663456917\n",
      "Iteration: 175/214, Loss: 0.16484735906124115\n",
      "Iteration: 176/214, Loss: 0.056689999997615814\n",
      "Iteration: 177/214, Loss: 0.1686687171459198\n",
      "Iteration: 178/214, Loss: 0.0672081857919693\n",
      "Iteration: 179/214, Loss: 0.02751307561993599\n",
      "Iteration: 180/214, Loss: 0.0004047435359098017\n",
      "Iteration: 181/214, Loss: 0.09322638064622879\n",
      "Iteration: 182/214, Loss: 0.017801571637392044\n",
      "Iteration: 183/214, Loss: 0.1607915759086609\n",
      "Iteration: 184/214, Loss: 0.10401506721973419\n",
      "Iteration: 185/214, Loss: 0.041312672197818756\n",
      "Iteration: 186/214, Loss: 0.032841943204402924\n",
      "Iteration: 187/214, Loss: 0.06423559039831161\n",
      "Iteration: 188/214, Loss: 0.06294069439172745\n",
      "Iteration: 189/214, Loss: 0.3913380801677704\n",
      "Iteration: 190/214, Loss: 0.09725382179021835\n",
      "Iteration: 191/214, Loss: 0.1433473527431488\n",
      "Iteration: 192/214, Loss: 0.04383184760808945\n",
      "Iteration: 193/214, Loss: 0.2782028615474701\n",
      "Iteration: 194/214, Loss: 0.0034421535674482584\n",
      "Iteration: 195/214, Loss: 0.06463698297739029\n",
      "Iteration: 196/214, Loss: 0.2122797816991806\n",
      "Iteration: 197/214, Loss: 0.19229969382286072\n",
      "Iteration: 198/214, Loss: 0.13410717248916626\n",
      "Iteration: 199/214, Loss: 0.034674737602472305\n",
      "Iteration: 200/214, Loss: 0.35211414098739624\n",
      "Iteration: 201/214, Loss: 0.22652478516101837\n",
      "Iteration: 202/214, Loss: 0.3053256571292877\n",
      "Iteration: 203/214, Loss: 0.028388220816850662\n",
      "Iteration: 204/214, Loss: 0.1362619549036026\n",
      "Iteration: 205/214, Loss: 0.14482639729976654\n",
      "Iteration: 206/214, Loss: 0.036792073398828506\n",
      "Iteration: 207/214, Loss: 0.3355783224105835\n",
      "Iteration: 208/214, Loss: 0.040018659085035324\n",
      "Iteration: 209/214, Loss: 0.038988031446933746\n",
      "Iteration: 210/214, Loss: 0.35397422313690186\n",
      "Iteration: 211/214, Loss: 0.11223999410867691\n",
      "Iteration: 212/214, Loss: 0.10472294688224792\n",
      "Iteration: 213/214, Loss: 0.2753012478351593\n",
      "Iteration: 214/214, Loss: 0.06261695921421051\n",
      "tensor(25.0324, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Iteration: 1/214, Loss: 0.04459444433450699\n",
      "Iteration: 2/214, Loss: 0.04329493269324303\n",
      "Iteration: 3/214, Loss: 0.05160338804125786\n",
      "Iteration: 4/214, Loss: 0.36441245675086975\n",
      "Iteration: 5/214, Loss: 0.04301793500781059\n",
      "Iteration: 6/214, Loss: 0.05293193832039833\n",
      "Iteration: 7/214, Loss: 0.07465391606092453\n",
      "Iteration: 8/214, Loss: 0.060746707022190094\n",
      "Iteration: 9/214, Loss: 0.1183605045080185\n",
      "Iteration: 10/214, Loss: 0.3757482171058655\n",
      "Iteration: 11/214, Loss: 0.1831786036491394\n",
      "Iteration: 12/214, Loss: 0.1322278380393982\n",
      "Iteration: 13/214, Loss: 0.001816106727346778\n",
      "Iteration: 14/214, Loss: 0.6269184350967407\n",
      "Iteration: 15/214, Loss: 0.03719998523592949\n",
      "Iteration: 16/214, Loss: 0.030726058408617973\n",
      "Iteration: 17/214, Loss: 0.6805088520050049\n",
      "Iteration: 18/214, Loss: 0.00012916259584017098\n",
      "Iteration: 19/214, Loss: 0.0008543023141101003\n",
      "Iteration: 20/214, Loss: 0.19805437326431274\n",
      "Iteration: 21/214, Loss: 0.03046785667538643\n",
      "Iteration: 22/214, Loss: 0.13459999859333038\n",
      "Iteration: 23/214, Loss: 0.045331038534641266\n",
      "Iteration: 24/214, Loss: 0.21669211983680725\n",
      "Iteration: 25/214, Loss: 0.039040397852659225\n",
      "Iteration: 26/214, Loss: 0.14212390780448914\n",
      "Iteration: 27/214, Loss: 0.06687105447053909\n",
      "Iteration: 28/214, Loss: 0.12002743035554886\n",
      "Iteration: 29/214, Loss: 0.0544343963265419\n",
      "Iteration: 30/214, Loss: 0.18584749102592468\n",
      "Iteration: 31/214, Loss: 0.027688967064023018\n",
      "Iteration: 32/214, Loss: 0.08912090957164764\n",
      "Iteration: 33/214, Loss: 0.0007312851957976818\n",
      "Iteration: 34/214, Loss: 0.07283327728509903\n",
      "Iteration: 35/214, Loss: 0.018862128257751465\n",
      "Iteration: 36/214, Loss: 0.05914696678519249\n",
      "Iteration: 37/214, Loss: 0.14019343256950378\n",
      "Iteration: 38/214, Loss: 0.11345329135656357\n",
      "Iteration: 39/214, Loss: 0.1671810895204544\n",
      "Iteration: 40/214, Loss: 0.25933730602264404\n",
      "Iteration: 41/214, Loss: 0.0602397546172142\n",
      "Iteration: 42/214, Loss: 0.1805829405784607\n",
      "Iteration: 43/214, Loss: 0.11297468841075897\n",
      "Iteration: 44/214, Loss: 0.0003531545808073133\n",
      "Iteration: 45/214, Loss: 0.224159374833107\n",
      "Iteration: 46/214, Loss: 0.05194494500756264\n",
      "Iteration: 47/214, Loss: 0.26007258892059326\n",
      "Iteration: 48/214, Loss: 0.027776820585131645\n",
      "Iteration: 49/214, Loss: 0.026121284812688828\n",
      "Iteration: 50/214, Loss: 0.04632578045129776\n",
      "Iteration: 51/214, Loss: 0.10828736424446106\n",
      "Iteration: 52/214, Loss: 0.2455306053161621\n",
      "Iteration: 53/214, Loss: 0.06827061623334885\n",
      "Iteration: 54/214, Loss: 0.02486240863800049\n",
      "Iteration: 55/214, Loss: 0.14077648520469666\n",
      "Iteration: 56/214, Loss: 0.04908987507224083\n",
      "Iteration: 57/214, Loss: 0.04679745063185692\n",
      "Iteration: 58/214, Loss: 0.039580088108778\n",
      "Iteration: 59/214, Loss: 0.13292303681373596\n",
      "Iteration: 60/214, Loss: 0.0011072459165006876\n",
      "Iteration: 61/214, Loss: 0.1036323830485344\n",
      "Iteration: 62/214, Loss: 0.059458423405885696\n",
      "Iteration: 63/214, Loss: 0.0008517651003785431\n",
      "Iteration: 64/214, Loss: 0.04467617720365524\n",
      "Iteration: 65/214, Loss: 0.2249273806810379\n",
      "Iteration: 66/214, Loss: 0.2973943054676056\n",
      "Iteration: 67/214, Loss: 0.24461773037910461\n",
      "Iteration: 68/214, Loss: 0.05095934867858887\n",
      "Iteration: 69/214, Loss: 0.05023033171892166\n",
      "Iteration: 70/214, Loss: 0.05045926943421364\n",
      "Iteration: 71/214, Loss: 0.35576072335243225\n",
      "Iteration: 72/214, Loss: 0.20294824242591858\n",
      "Iteration: 73/214, Loss: 0.09798040241003036\n",
      "Iteration: 74/214, Loss: 0.10662396997213364\n",
      "Iteration: 75/214, Loss: 0.18797263503074646\n",
      "Iteration: 76/214, Loss: 0.025874514132738113\n",
      "Iteration: 77/214, Loss: 0.04607280343770981\n",
      "Iteration: 78/214, Loss: 0.08383948355913162\n",
      "Iteration: 79/214, Loss: 0.11248521506786346\n",
      "Iteration: 80/214, Loss: 0.10379712283611298\n",
      "Iteration: 81/214, Loss: 0.09764034301042557\n",
      "Iteration: 82/214, Loss: 0.026163697242736816\n",
      "Iteration: 83/214, Loss: 0.03189922124147415\n",
      "Iteration: 84/214, Loss: 0.034346845000982285\n",
      "Iteration: 85/214, Loss: 0.1959933042526245\n",
      "Iteration: 86/214, Loss: 0.08284951746463776\n",
      "Iteration: 87/214, Loss: 0.12823215126991272\n",
      "Iteration: 88/214, Loss: 0.0009765657596290112\n",
      "Iteration: 89/214, Loss: 0.025097012519836426\n",
      "Iteration: 90/214, Loss: 0.027164209634065628\n",
      "Iteration: 91/214, Loss: 0.04561104252934456\n",
      "Iteration: 92/214, Loss: 0.10077514499425888\n",
      "Iteration: 93/214, Loss: 0.2357318103313446\n",
      "Iteration: 94/214, Loss: 0.07192067056894302\n",
      "Iteration: 95/214, Loss: 0.20087561011314392\n",
      "Iteration: 96/214, Loss: 0.044286180287599564\n",
      "Iteration: 97/214, Loss: 0.13727004826068878\n",
      "Iteration: 98/214, Loss: 0.25322413444519043\n",
      "Iteration: 99/214, Loss: 0.04394720494747162\n",
      "Iteration: 100/214, Loss: 0.108336441218853\n",
      "Iteration: 101/214, Loss: 0.09608600288629532\n",
      "Iteration: 102/214, Loss: 0.0007324060425162315\n",
      "Iteration: 103/214, Loss: 0.2084747552871704\n",
      "Iteration: 104/214, Loss: 0.07128732651472092\n",
      "Iteration: 105/214, Loss: 0.19674625992774963\n",
      "Iteration: 106/214, Loss: 0.0011068121530115604\n",
      "Iteration: 107/214, Loss: 0.39515921473503113\n",
      "Iteration: 108/214, Loss: 0.28841379284858704\n",
      "Iteration: 109/214, Loss: 0.04730485379695892\n",
      "Iteration: 110/214, Loss: 0.07737022638320923\n",
      "Iteration: 111/214, Loss: 0.24775148928165436\n",
      "Iteration: 112/214, Loss: 0.16463227570056915\n",
      "Iteration: 113/214, Loss: 0.12676073610782623\n",
      "Iteration: 114/214, Loss: 0.08208512514829636\n",
      "Iteration: 115/214, Loss: 0.024507425725460052\n",
      "Iteration: 116/214, Loss: 0.2453223019838333\n",
      "Iteration: 117/214, Loss: 0.11752195656299591\n",
      "Iteration: 118/214, Loss: 0.04852018132805824\n",
      "Iteration: 119/214, Loss: 0.04076845943927765\n",
      "Iteration: 120/214, Loss: 0.21085265278816223\n",
      "Iteration: 121/214, Loss: 0.030501123517751694\n",
      "Iteration: 122/214, Loss: 0.04826238751411438\n",
      "Iteration: 123/214, Loss: 0.07320467382669449\n",
      "Iteration: 124/214, Loss: 0.04077613353729248\n",
      "Iteration: 125/214, Loss: 0.022799784317612648\n",
      "Iteration: 126/214, Loss: 0.03237728029489517\n",
      "Iteration: 127/214, Loss: 0.02791244350373745\n",
      "Iteration: 128/214, Loss: 0.14787395298480988\n",
      "Iteration: 129/214, Loss: 0.22467142343521118\n",
      "Iteration: 130/214, Loss: 0.1856621503829956\n",
      "Iteration: 131/214, Loss: 0.03293541818857193\n",
      "Iteration: 132/214, Loss: 0.23536066710948944\n",
      "Iteration: 133/214, Loss: 0.0010160224046558142\n",
      "Iteration: 134/214, Loss: 0.03649483621120453\n",
      "Iteration: 135/214, Loss: 0.04175054654479027\n",
      "Iteration: 136/214, Loss: 0.0010775302071124315\n",
      "Iteration: 137/214, Loss: 0.0012262981617823243\n",
      "Iteration: 138/214, Loss: 0.04102258011698723\n",
      "Iteration: 139/214, Loss: 0.06240890547633171\n",
      "Iteration: 140/214, Loss: 0.3149459660053253\n",
      "Iteration: 141/214, Loss: 0.04149521142244339\n",
      "Iteration: 142/214, Loss: 0.3335665762424469\n",
      "Iteration: 143/214, Loss: 0.053109798580408096\n",
      "Iteration: 144/214, Loss: 0.1823197454214096\n",
      "Iteration: 145/214, Loss: 0.0782085582613945\n",
      "Iteration: 146/214, Loss: 0.03435194492340088\n",
      "Iteration: 147/214, Loss: 0.16691622138023376\n",
      "Iteration: 148/214, Loss: 0.12110825628042221\n",
      "Iteration: 149/214, Loss: 0.03767717629671097\n",
      "Iteration: 150/214, Loss: 0.03189915418624878\n",
      "Iteration: 151/214, Loss: 0.030462952330708504\n",
      "Iteration: 152/214, Loss: 0.10445656627416611\n",
      "Iteration: 153/214, Loss: 0.05418679490685463\n",
      "Iteration: 154/214, Loss: 0.038102928549051285\n",
      "Iteration: 155/214, Loss: 0.40952691435813904\n",
      "Iteration: 156/214, Loss: 0.2767379879951477\n",
      "Iteration: 157/214, Loss: 0.0006642275839112699\n",
      "Iteration: 158/214, Loss: 0.10180950164794922\n",
      "Iteration: 159/214, Loss: 0.02804252877831459\n",
      "Iteration: 160/214, Loss: 0.09581407904624939\n",
      "Iteration: 161/214, Loss: 0.22702261805534363\n",
      "Iteration: 162/214, Loss: 0.037926603108644485\n",
      "Iteration: 163/214, Loss: 0.10222519934177399\n",
      "Iteration: 164/214, Loss: 0.05056082457304001\n",
      "Iteration: 165/214, Loss: 0.3760562539100647\n",
      "Iteration: 166/214, Loss: 0.0008606496267020702\n",
      "Iteration: 167/214, Loss: 0.03574322164058685\n",
      "Iteration: 168/214, Loss: 0.038309916853904724\n",
      "Iteration: 169/214, Loss: 0.15059787034988403\n",
      "Iteration: 170/214, Loss: 0.25226733088493347\n",
      "Iteration: 171/214, Loss: 0.26996520161628723\n",
      "Iteration: 172/214, Loss: 0.08504723012447357\n",
      "Iteration: 173/214, Loss: 0.023441389203071594\n",
      "Iteration: 174/214, Loss: 0.01838923804461956\n",
      "Iteration: 175/214, Loss: 0.2723003625869751\n",
      "Iteration: 176/214, Loss: 0.0988837331533432\n",
      "Iteration: 177/214, Loss: 0.14613047242164612\n",
      "Iteration: 178/214, Loss: 0.0633482038974762\n",
      "Iteration: 179/214, Loss: 0.02055985853075981\n",
      "Iteration: 180/214, Loss: 0.00042167602805420756\n",
      "Iteration: 181/214, Loss: 0.15905790030956268\n",
      "Iteration: 182/214, Loss: 0.04126819223165512\n",
      "Iteration: 183/214, Loss: 0.3198346495628357\n",
      "Iteration: 184/214, Loss: 0.2564920485019684\n",
      "Iteration: 185/214, Loss: 0.0745842233300209\n",
      "Iteration: 186/214, Loss: 0.04789833724498749\n",
      "Iteration: 187/214, Loss: 0.057482361793518066\n",
      "Iteration: 188/214, Loss: 0.05290617048740387\n",
      "Iteration: 189/214, Loss: 0.35713139176368713\n",
      "Iteration: 190/214, Loss: 0.10111074894666672\n",
      "Iteration: 191/214, Loss: 0.19457288086414337\n",
      "Iteration: 192/214, Loss: 0.04239598289132118\n",
      "Iteration: 193/214, Loss: 0.18537667393684387\n",
      "Iteration: 194/214, Loss: 0.002841603010892868\n",
      "Iteration: 195/214, Loss: 0.06772217899560928\n",
      "Iteration: 196/214, Loss: 0.26384782791137695\n",
      "Iteration: 197/214, Loss: 0.21043072640895844\n",
      "Iteration: 198/214, Loss: 0.14833088219165802\n",
      "Iteration: 199/214, Loss: 0.03683772310614586\n",
      "Iteration: 200/214, Loss: 0.21794795989990234\n",
      "Iteration: 201/214, Loss: 0.17700636386871338\n",
      "Iteration: 202/214, Loss: 0.23991045355796814\n",
      "Iteration: 203/214, Loss: 0.046357251703739166\n",
      "Iteration: 204/214, Loss: 0.11337658017873764\n",
      "Iteration: 205/214, Loss: 0.19250479340553284\n",
      "Iteration: 206/214, Loss: 0.03213467448949814\n",
      "Iteration: 207/214, Loss: 0.3476353883743286\n",
      "Iteration: 208/214, Loss: 0.022935891523957253\n",
      "Iteration: 209/214, Loss: 0.11107763648033142\n",
      "Iteration: 210/214, Loss: 0.2610344886779785\n",
      "Iteration: 211/214, Loss: 0.0986471176147461\n",
      "Iteration: 212/214, Loss: 0.08885737508535385\n",
      "Iteration: 213/214, Loss: 0.2568364143371582\n",
      "Iteration: 214/214, Loss: 0.04093680530786514\n",
      "tensor(24.9472, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Iteration: 1/214, Loss: 0.05807671695947647\n",
      "Iteration: 2/214, Loss: 0.03981933742761612\n",
      "Iteration: 3/214, Loss: 0.05482446774840355\n",
      "Iteration: 4/214, Loss: 0.4206673502922058\n",
      "Iteration: 5/214, Loss: 0.049461159855127335\n",
      "Iteration: 6/214, Loss: 0.03193880617618561\n",
      "Iteration: 7/214, Loss: 0.07087328284978867\n",
      "Iteration: 8/214, Loss: 0.03039214201271534\n",
      "Iteration: 9/214, Loss: 0.12542498111724854\n",
      "Iteration: 10/214, Loss: 0.2580536901950836\n",
      "Iteration: 11/214, Loss: 0.136911079287529\n",
      "Iteration: 12/214, Loss: 0.10119188576936722\n",
      "Iteration: 13/214, Loss: 0.0012032617814838886\n",
      "Iteration: 14/214, Loss: 0.47501176595687866\n",
      "Iteration: 15/214, Loss: 0.02812976948916912\n",
      "Iteration: 16/214, Loss: 0.04278749227523804\n",
      "Iteration: 17/214, Loss: 0.6588565111160278\n",
      "Iteration: 18/214, Loss: 0.00045827796566300094\n",
      "Iteration: 19/214, Loss: 0.0007215776713564992\n",
      "Iteration: 20/214, Loss: 0.21501891314983368\n",
      "Iteration: 21/214, Loss: 0.0382174514234066\n",
      "Iteration: 22/214, Loss: 0.15401610732078552\n",
      "Iteration: 23/214, Loss: 0.0425761342048645\n",
      "Iteration: 24/214, Loss: 0.21448251605033875\n",
      "Iteration: 25/214, Loss: 0.042017724364995956\n",
      "Iteration: 26/214, Loss: 0.10934735089540482\n",
      "Iteration: 27/214, Loss: 0.08896387368440628\n",
      "Iteration: 28/214, Loss: 0.0967317521572113\n",
      "Iteration: 29/214, Loss: 0.08981512486934662\n",
      "Iteration: 30/214, Loss: 0.21450400352478027\n",
      "Iteration: 31/214, Loss: 0.04182995483279228\n",
      "Iteration: 32/214, Loss: 0.07970765233039856\n",
      "Iteration: 33/214, Loss: 0.001150226453319192\n",
      "Iteration: 34/214, Loss: 0.05096102133393288\n",
      "Iteration: 35/214, Loss: 0.023877369239926338\n",
      "Iteration: 36/214, Loss: 0.05581272020936012\n",
      "Iteration: 37/214, Loss: 0.12317489832639694\n",
      "Iteration: 38/214, Loss: 0.12455515563488007\n",
      "Iteration: 39/214, Loss: 0.20313136279582977\n",
      "Iteration: 40/214, Loss: 0.3107025623321533\n",
      "Iteration: 41/214, Loss: 0.061287831515073776\n",
      "Iteration: 42/214, Loss: 0.13585463166236877\n",
      "Iteration: 43/214, Loss: 0.07787591964006424\n",
      "Iteration: 44/214, Loss: 0.002245895564556122\n",
      "Iteration: 45/214, Loss: 0.21753773093223572\n",
      "Iteration: 46/214, Loss: 0.04792993888258934\n",
      "Iteration: 47/214, Loss: 0.33041447401046753\n",
      "Iteration: 48/214, Loss: 0.04832858219742775\n",
      "Iteration: 49/214, Loss: 0.022744888439774513\n",
      "Iteration: 50/214, Loss: 0.05275747925043106\n",
      "Iteration: 51/214, Loss: 0.10783761739730835\n",
      "Iteration: 52/214, Loss: 0.20549283921718597\n",
      "Iteration: 53/214, Loss: 0.06671148538589478\n",
      "Iteration: 54/214, Loss: 0.026969334110617638\n",
      "Iteration: 55/214, Loss: 0.1277049332857132\n",
      "Iteration: 56/214, Loss: 0.04451973736286163\n",
      "Iteration: 57/214, Loss: 0.06026330590248108\n",
      "Iteration: 58/214, Loss: 0.04116412624716759\n",
      "Iteration: 59/214, Loss: 0.18124032020568848\n",
      "Iteration: 60/214, Loss: 0.0010108218993991613\n",
      "Iteration: 61/214, Loss: 0.10356497764587402\n",
      "Iteration: 62/214, Loss: 0.09280785173177719\n",
      "Iteration: 63/214, Loss: 0.003863496705889702\n",
      "Iteration: 64/214, Loss: 0.06296657025814056\n",
      "Iteration: 65/214, Loss: 0.3303447365760803\n",
      "Iteration: 66/214, Loss: 0.3074207901954651\n",
      "Iteration: 67/214, Loss: 0.37033531069755554\n",
      "Iteration: 68/214, Loss: 0.08875611424446106\n",
      "Iteration: 69/214, Loss: 0.06005602702498436\n",
      "Iteration: 70/214, Loss: 0.06254848837852478\n",
      "Iteration: 71/214, Loss: 0.38313883543014526\n",
      "Iteration: 72/214, Loss: 0.20830482244491577\n",
      "Iteration: 73/214, Loss: 0.10415332019329071\n",
      "Iteration: 74/214, Loss: 0.06658824533224106\n",
      "Iteration: 75/214, Loss: 0.24378713965415955\n",
      "Iteration: 76/214, Loss: 0.026018688455224037\n",
      "Iteration: 77/214, Loss: 0.051440123468637466\n",
      "Iteration: 78/214, Loss: 0.11380551755428314\n",
      "Iteration: 79/214, Loss: 0.10299253463745117\n",
      "Iteration: 80/214, Loss: 0.0768866166472435\n",
      "Iteration: 81/214, Loss: 0.06846729665994644\n",
      "Iteration: 82/214, Loss: 0.02518938109278679\n",
      "Iteration: 83/214, Loss: 0.015519947744905949\n",
      "Iteration: 84/214, Loss: 0.03131238743662834\n",
      "Iteration: 85/214, Loss: 0.2157115936279297\n",
      "Iteration: 86/214, Loss: 0.10329116880893707\n",
      "Iteration: 87/214, Loss: 0.16822926700115204\n",
      "Iteration: 88/214, Loss: 0.000448469421826303\n",
      "Iteration: 89/214, Loss: 0.037894800305366516\n",
      "Iteration: 90/214, Loss: 0.042844559997320175\n",
      "Iteration: 91/214, Loss: 0.054817453026771545\n",
      "Iteration: 92/214, Loss: 0.15153568983078003\n",
      "Iteration: 93/214, Loss: 0.22367924451828003\n",
      "Iteration: 94/214, Loss: 0.06970452517271042\n",
      "Iteration: 95/214, Loss: 0.21771885454654694\n",
      "Iteration: 96/214, Loss: 0.034549858421087265\n",
      "Iteration: 97/214, Loss: 0.09500207006931305\n",
      "Iteration: 98/214, Loss: 0.26268455386161804\n",
      "Iteration: 99/214, Loss: 0.049099281430244446\n",
      "Iteration: 100/214, Loss: 0.1627829670906067\n",
      "Iteration: 101/214, Loss: 0.16081807017326355\n",
      "Iteration: 102/214, Loss: 0.001064075855538249\n",
      "Iteration: 103/214, Loss: 0.21116067469120026\n",
      "Iteration: 104/214, Loss: 0.0921023041009903\n",
      "Iteration: 105/214, Loss: 0.14264219999313354\n",
      "Iteration: 106/214, Loss: 0.001304516801610589\n",
      "Iteration: 107/214, Loss: 0.3529811203479767\n",
      "Iteration: 108/214, Loss: 0.33955931663513184\n",
      "Iteration: 109/214, Loss: 0.03576381877064705\n",
      "Iteration: 110/214, Loss: 0.08577542006969452\n",
      "Iteration: 111/214, Loss: 0.3257840871810913\n",
      "Iteration: 112/214, Loss: 0.2099158763885498\n",
      "Iteration: 113/214, Loss: 0.20594681799411774\n",
      "Iteration: 114/214, Loss: 0.14509007334709167\n",
      "Iteration: 115/214, Loss: 0.04738840460777283\n",
      "Iteration: 116/214, Loss: 0.3619135618209839\n",
      "Iteration: 117/214, Loss: 0.09000799059867859\n",
      "Iteration: 118/214, Loss: 0.028057850897312164\n",
      "Iteration: 119/214, Loss: 0.020954687148332596\n",
      "Iteration: 120/214, Loss: 0.14967484772205353\n",
      "Iteration: 121/214, Loss: 0.03536967560648918\n",
      "Iteration: 122/214, Loss: 0.06151209771633148\n",
      "Iteration: 123/214, Loss: 0.0868622288107872\n",
      "Iteration: 124/214, Loss: 0.03542129322886467\n",
      "Iteration: 125/214, Loss: 0.06475021690130234\n",
      "Iteration: 126/214, Loss: 0.029845017939805984\n",
      "Iteration: 127/214, Loss: 0.035222139209508896\n",
      "Iteration: 128/214, Loss: 0.12130146473646164\n",
      "Iteration: 129/214, Loss: 0.25314533710479736\n",
      "Iteration: 130/214, Loss: 0.17223186790943146\n",
      "Iteration: 131/214, Loss: 0.02234971895813942\n",
      "Iteration: 132/214, Loss: 0.22660353779792786\n",
      "Iteration: 133/214, Loss: 0.0007702998700551689\n",
      "Iteration: 134/214, Loss: 0.03615370765328407\n",
      "Iteration: 135/214, Loss: 0.05200514942407608\n",
      "Iteration: 136/214, Loss: 0.00045536330435425043\n",
      "Iteration: 137/214, Loss: 0.0006729114102199674\n",
      "Iteration: 138/214, Loss: 0.04952574521303177\n",
      "Iteration: 139/214, Loss: 0.06511639058589935\n",
      "Iteration: 140/214, Loss: 0.4330621659755707\n",
      "Iteration: 141/214, Loss: 0.05216722935438156\n",
      "Iteration: 142/214, Loss: 0.37409570813179016\n",
      "Iteration: 143/214, Loss: 0.03309445083141327\n",
      "Iteration: 144/214, Loss: 0.22857661545276642\n",
      "Iteration: 145/214, Loss: 0.054080210626125336\n",
      "Iteration: 146/214, Loss: 0.02489311806857586\n",
      "Iteration: 147/214, Loss: 0.1661784052848816\n",
      "Iteration: 148/214, Loss: 0.11762876808643341\n",
      "Iteration: 149/214, Loss: 0.0447845458984375\n",
      "Iteration: 150/214, Loss: 0.02687663771212101\n",
      "Iteration: 151/214, Loss: 0.03477400168776512\n",
      "Iteration: 152/214, Loss: 0.09121207147836685\n",
      "Iteration: 153/214, Loss: 0.03542554751038551\n",
      "Iteration: 154/214, Loss: 0.039009176194667816\n",
      "Iteration: 155/214, Loss: 0.3825473487377167\n",
      "Iteration: 156/214, Loss: 0.22150228917598724\n",
      "Iteration: 157/214, Loss: 0.0018001385033130646\n",
      "Iteration: 158/214, Loss: 0.10801728814840317\n",
      "Iteration: 159/214, Loss: 0.02565857768058777\n",
      "Iteration: 160/214, Loss: 0.06763802468776703\n",
      "Iteration: 161/214, Loss: 0.18769794702529907\n",
      "Iteration: 162/214, Loss: 0.048658888787031174\n",
      "Iteration: 163/214, Loss: 0.10776355862617493\n",
      "Iteration: 164/214, Loss: 0.04944479838013649\n",
      "Iteration: 165/214, Loss: 0.3268478214740753\n",
      "Iteration: 166/214, Loss: 0.0009091261308640242\n",
      "Iteration: 167/214, Loss: 0.0355699397623539\n",
      "Iteration: 168/214, Loss: 0.021726485341787338\n",
      "Iteration: 169/214, Loss: 0.18877574801445007\n",
      "Iteration: 170/214, Loss: 0.2480594962835312\n",
      "Iteration: 171/214, Loss: 0.275062620639801\n",
      "Iteration: 172/214, Loss: 0.09349697083234787\n",
      "Iteration: 173/214, Loss: 0.03251520171761513\n",
      "Iteration: 174/214, Loss: 0.027249924838542938\n",
      "Iteration: 175/214, Loss: 0.21658432483673096\n",
      "Iteration: 176/214, Loss: 0.1871453821659088\n",
      "Iteration: 177/214, Loss: 0.2347295582294464\n",
      "Iteration: 178/214, Loss: 0.10027298331260681\n",
      "Iteration: 179/214, Loss: 0.025069808587431908\n",
      "Iteration: 180/214, Loss: 0.0003346521407365799\n",
      "Iteration: 181/214, Loss: 0.16338661313056946\n",
      "Iteration: 182/214, Loss: 0.02832997962832451\n",
      "Iteration: 183/214, Loss: 0.34019556641578674\n",
      "Iteration: 184/214, Loss: 0.207395538687706\n",
      "Iteration: 185/214, Loss: 0.08235976099967957\n",
      "Iteration: 186/214, Loss: 0.05927647650241852\n",
      "Iteration: 187/214, Loss: 0.08368012309074402\n",
      "Iteration: 188/214, Loss: 0.06128657981753349\n",
      "Iteration: 189/214, Loss: 0.4761921763420105\n",
      "Iteration: 190/214, Loss: 0.10153713077306747\n",
      "Iteration: 191/214, Loss: 0.20909899473190308\n",
      "Iteration: 192/214, Loss: 0.08628430217504501\n",
      "Iteration: 193/214, Loss: 0.21618899703025818\n",
      "Iteration: 194/214, Loss: 0.004973029717803001\n",
      "Iteration: 195/214, Loss: 0.06473949551582336\n",
      "Iteration: 196/214, Loss: 0.28264257311820984\n",
      "Iteration: 197/214, Loss: 0.18565905094146729\n",
      "Iteration: 198/214, Loss: 0.1172075867652893\n",
      "Iteration: 199/214, Loss: 0.04802294075489044\n",
      "Iteration: 200/214, Loss: 0.19950152933597565\n",
      "Iteration: 201/214, Loss: 0.18935099244117737\n",
      "Iteration: 202/214, Loss: 0.3099276125431061\n",
      "Iteration: 203/214, Loss: 0.02767711505293846\n",
      "Iteration: 204/214, Loss: 0.1064724251627922\n",
      "Iteration: 205/214, Loss: 0.152562215924263\n",
      "Iteration: 206/214, Loss: 0.018829025328159332\n",
      "Iteration: 207/214, Loss: 0.24124063551425934\n",
      "Iteration: 208/214, Loss: 0.02438083291053772\n",
      "Iteration: 209/214, Loss: 0.07811430096626282\n",
      "Iteration: 210/214, Loss: 0.22157414257526398\n",
      "Iteration: 211/214, Loss: 0.06966644525527954\n",
      "Iteration: 212/214, Loss: 0.07023013383150101\n",
      "Iteration: 213/214, Loss: 0.23390978574752808\n",
      "Iteration: 214/214, Loss: 0.0320846252143383\n",
      "tensor(25.7777, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Iteration: 1/214, Loss: 0.0666353777050972\n",
      "Iteration: 2/214, Loss: 0.03239051625132561\n",
      "Iteration: 3/214, Loss: 0.039446111768484116\n",
      "Iteration: 4/214, Loss: 0.2836536467075348\n",
      "Iteration: 5/214, Loss: 0.03130898252129555\n",
      "Iteration: 6/214, Loss: 0.026093484833836555\n",
      "Iteration: 7/214, Loss: 0.06303222477436066\n",
      "Iteration: 8/214, Loss: 0.03440222889184952\n",
      "Iteration: 9/214, Loss: 0.07229401171207428\n",
      "Iteration: 10/214, Loss: 0.24706816673278809\n",
      "Iteration: 11/214, Loss: 0.06833408772945404\n",
      "Iteration: 12/214, Loss: 0.07040949910879135\n",
      "Iteration: 13/214, Loss: 0.0005808002315461636\n",
      "Iteration: 14/214, Loss: 0.40055426955223083\n",
      "Iteration: 15/214, Loss: 0.024365555495023727\n",
      "Iteration: 16/214, Loss: 0.0458516888320446\n",
      "Iteration: 17/214, Loss: 1.0709072351455688\n",
      "Iteration: 18/214, Loss: 0.000864454370457679\n",
      "Iteration: 19/214, Loss: 0.0019467498641461134\n",
      "Iteration: 20/214, Loss: 0.14920520782470703\n",
      "Iteration: 21/214, Loss: 0.0386173352599144\n",
      "Iteration: 22/214, Loss: 0.11557495594024658\n",
      "Iteration: 23/214, Loss: 0.022774770855903625\n",
      "Iteration: 24/214, Loss: 0.18073780834674835\n",
      "Iteration: 25/214, Loss: 0.0390017107129097\n",
      "Iteration: 26/214, Loss: 0.12072673439979553\n",
      "Iteration: 27/214, Loss: 0.073482945561409\n",
      "Iteration: 28/214, Loss: 0.08227420598268509\n",
      "Iteration: 29/214, Loss: 0.033151060342788696\n",
      "Iteration: 30/214, Loss: 0.2062150090932846\n",
      "Iteration: 31/214, Loss: 0.02260810323059559\n",
      "Iteration: 32/214, Loss: 0.08027287572622299\n",
      "Iteration: 33/214, Loss: 0.0013883693609386683\n",
      "Iteration: 34/214, Loss: 0.059167683124542236\n",
      "Iteration: 35/214, Loss: 0.02005206234753132\n",
      "Iteration: 36/214, Loss: 0.07979560643434525\n",
      "Iteration: 37/214, Loss: 0.10359158366918564\n",
      "Iteration: 38/214, Loss: 0.0848250538110733\n",
      "Iteration: 39/214, Loss: 0.19150367379188538\n",
      "Iteration: 40/214, Loss: 0.23283837735652924\n",
      "Iteration: 41/214, Loss: 0.042719729244709015\n",
      "Iteration: 42/214, Loss: 0.15556244552135468\n",
      "Iteration: 43/214, Loss: 0.08377928286790848\n",
      "Iteration: 44/214, Loss: 0.0003920990275219083\n",
      "Iteration: 45/214, Loss: 0.17837615311145782\n",
      "Iteration: 46/214, Loss: 0.026863019913434982\n",
      "Iteration: 47/214, Loss: 0.25191161036491394\n",
      "Iteration: 48/214, Loss: 0.04175114631652832\n",
      "Iteration: 49/214, Loss: 0.01786058023571968\n",
      "Iteration: 50/214, Loss: 0.03515307605266571\n",
      "Iteration: 51/214, Loss: 0.08214150369167328\n",
      "Iteration: 52/214, Loss: 0.1750572919845581\n",
      "Iteration: 53/214, Loss: 0.04327026382088661\n",
      "Iteration: 54/214, Loss: 0.011867489665746689\n",
      "Iteration: 55/214, Loss: 0.13615313172340393\n",
      "Iteration: 56/214, Loss: 0.023738225921988487\n",
      "Iteration: 57/214, Loss: 0.048911117017269135\n",
      "Iteration: 58/214, Loss: 0.03198342025279999\n",
      "Iteration: 59/214, Loss: 0.1369747519493103\n",
      "Iteration: 60/214, Loss: 0.000539124826900661\n",
      "Iteration: 61/214, Loss: 0.14123430848121643\n",
      "Iteration: 62/214, Loss: 0.0760216936469078\n",
      "Iteration: 63/214, Loss: 0.0004459739138837904\n",
      "Iteration: 64/214, Loss: 0.05850356072187424\n",
      "Iteration: 65/214, Loss: 0.21587330102920532\n",
      "Iteration: 66/214, Loss: 0.35895004868507385\n",
      "Iteration: 67/214, Loss: 0.25704658031463623\n",
      "Iteration: 68/214, Loss: 0.07730177789926529\n",
      "Iteration: 69/214, Loss: 0.049119237810373306\n",
      "Iteration: 70/214, Loss: 0.061809245496988297\n",
      "Iteration: 71/214, Loss: 0.32281917333602905\n",
      "Iteration: 72/214, Loss: 0.1841261088848114\n",
      "Iteration: 73/214, Loss: 0.10696649551391602\n",
      "Iteration: 74/214, Loss: 0.06400807946920395\n",
      "Iteration: 75/214, Loss: 0.23684540390968323\n",
      "Iteration: 76/214, Loss: 0.015678737312555313\n",
      "Iteration: 77/214, Loss: 0.06132111698389053\n",
      "Iteration: 78/214, Loss: 0.0846467912197113\n",
      "Iteration: 79/214, Loss: 0.10250946134328842\n",
      "Iteration: 80/214, Loss: 0.06983975321054459\n",
      "Iteration: 81/214, Loss: 0.054980214685201645\n",
      "Iteration: 82/214, Loss: 0.019529886543750763\n",
      "Iteration: 83/214, Loss: 0.019671574234962463\n",
      "Iteration: 84/214, Loss: 0.05115854740142822\n",
      "Iteration: 85/214, Loss: 0.17765812575817108\n",
      "Iteration: 86/214, Loss: 0.08815394341945648\n",
      "Iteration: 87/214, Loss: 0.1733279824256897\n",
      "Iteration: 88/214, Loss: 0.0008703228668309748\n",
      "Iteration: 89/214, Loss: 0.026705140247941017\n",
      "Iteration: 90/214, Loss: 0.030274709686636925\n",
      "Iteration: 91/214, Loss: 0.04522249847650528\n",
      "Iteration: 92/214, Loss: 0.10779006034135818\n",
      "Iteration: 93/214, Loss: 0.2054361253976822\n",
      "Iteration: 94/214, Loss: 0.05533474311232567\n",
      "Iteration: 95/214, Loss: 0.21922649443149567\n",
      "Iteration: 96/214, Loss: 0.02123665064573288\n",
      "Iteration: 97/214, Loss: 0.10027895867824554\n",
      "Iteration: 98/214, Loss: 0.24489106237888336\n",
      "Iteration: 99/214, Loss: 0.03898210823535919\n",
      "Iteration: 100/214, Loss: 0.18000105023384094\n",
      "Iteration: 101/214, Loss: 0.14204829931259155\n",
      "Iteration: 102/214, Loss: 0.000908793299458921\n",
      "Iteration: 103/214, Loss: 0.20362530648708344\n",
      "Iteration: 104/214, Loss: 0.0830753818154335\n",
      "Iteration: 105/214, Loss: 0.12696512043476105\n",
      "Iteration: 106/214, Loss: 0.001632567378692329\n",
      "Iteration: 107/214, Loss: 0.2933719754219055\n",
      "Iteration: 108/214, Loss: 0.29637521505355835\n",
      "Iteration: 109/214, Loss: 0.037980567663908005\n",
      "Iteration: 110/214, Loss: 0.08619947731494904\n",
      "Iteration: 111/214, Loss: 0.23219257593154907\n",
      "Iteration: 112/214, Loss: 0.12117253988981247\n",
      "Iteration: 113/214, Loss: 0.12346059083938599\n",
      "Iteration: 114/214, Loss: 0.12976987659931183\n",
      "Iteration: 115/214, Loss: 0.024928729981184006\n",
      "Iteration: 116/214, Loss: 0.3174571990966797\n",
      "Iteration: 117/214, Loss: 0.10809306055307388\n",
      "Iteration: 118/214, Loss: 0.05129088833928108\n",
      "Iteration: 119/214, Loss: 0.048038870096206665\n",
      "Iteration: 120/214, Loss: 0.2626936137676239\n",
      "Iteration: 121/214, Loss: 0.026749318465590477\n",
      "Iteration: 122/214, Loss: 0.05827292054891586\n",
      "Iteration: 123/214, Loss: 0.0740877166390419\n",
      "Iteration: 124/214, Loss: 0.030383985489606857\n",
      "Iteration: 125/214, Loss: 0.035860203206539154\n",
      "Iteration: 126/214, Loss: 0.03161261975765228\n",
      "Iteration: 127/214, Loss: 0.0315244197845459\n",
      "Iteration: 128/214, Loss: 0.15541501343250275\n",
      "Iteration: 129/214, Loss: 0.17836004495620728\n",
      "Iteration: 130/214, Loss: 0.18909820914268494\n",
      "Iteration: 131/214, Loss: 0.023263413459062576\n",
      "Iteration: 132/214, Loss: 0.20558013021945953\n",
      "Iteration: 133/214, Loss: 0.0016706610331311822\n",
      "Iteration: 134/214, Loss: 0.03365068510174751\n",
      "Iteration: 135/214, Loss: 0.030974334105849266\n",
      "Iteration: 136/214, Loss: 0.0006421318976208568\n",
      "Iteration: 137/214, Loss: 0.0011238274164497852\n",
      "Iteration: 138/214, Loss: 0.04848514124751091\n",
      "Iteration: 139/214, Loss: 0.04393285512924194\n",
      "Iteration: 140/214, Loss: 0.2579975128173828\n",
      "Iteration: 141/214, Loss: 0.03802601993083954\n",
      "Iteration: 142/214, Loss: 0.3736981153488159\n",
      "Iteration: 143/214, Loss: 0.04880773648619652\n",
      "Iteration: 144/214, Loss: 0.15598271787166595\n",
      "Iteration: 145/214, Loss: 0.06189105287194252\n",
      "Iteration: 146/214, Loss: 0.028271246701478958\n",
      "Iteration: 147/214, Loss: 0.1370653212070465\n",
      "Iteration: 148/214, Loss: 0.10818729549646378\n",
      "Iteration: 149/214, Loss: 0.027139637619256973\n",
      "Iteration: 150/214, Loss: 0.05507297441363335\n",
      "Iteration: 151/214, Loss: 0.03404324874281883\n",
      "Iteration: 152/214, Loss: 0.07556817680597305\n",
      "Iteration: 153/214, Loss: 0.03174830600619316\n",
      "Iteration: 154/214, Loss: 0.03498074784874916\n",
      "Iteration: 155/214, Loss: 0.28258925676345825\n",
      "Iteration: 156/214, Loss: 0.28000250458717346\n",
      "Iteration: 157/214, Loss: 0.001541854813694954\n",
      "Iteration: 158/214, Loss: 0.08816047012805939\n",
      "Iteration: 159/214, Loss: 0.029859865084290504\n",
      "Iteration: 160/214, Loss: 0.0895710438489914\n",
      "Iteration: 161/214, Loss: 0.22302506864070892\n",
      "Iteration: 162/214, Loss: 0.034964803606271744\n",
      "Iteration: 163/214, Loss: 0.08888959884643555\n",
      "Iteration: 164/214, Loss: 0.036735132336616516\n",
      "Iteration: 165/214, Loss: 0.2429819405078888\n",
      "Iteration: 166/214, Loss: 0.0015247836709022522\n",
      "Iteration: 167/214, Loss: 0.030496645718812943\n",
      "Iteration: 168/214, Loss: 0.027513986453413963\n",
      "Iteration: 169/214, Loss: 0.1300497204065323\n",
      "Iteration: 170/214, Loss: 0.24339455366134644\n",
      "Iteration: 171/214, Loss: 0.2884533703327179\n",
      "Iteration: 172/214, Loss: 0.09182936698198318\n",
      "Iteration: 173/214, Loss: 0.015483922325074673\n",
      "Iteration: 174/214, Loss: 0.02290245145559311\n",
      "Iteration: 175/214, Loss: 0.19049306213855743\n",
      "Iteration: 176/214, Loss: 0.08987316489219666\n",
      "Iteration: 177/214, Loss: 0.17898568511009216\n",
      "Iteration: 178/214, Loss: 0.08111285418272018\n",
      "Iteration: 179/214, Loss: 0.024150218814611435\n",
      "Iteration: 180/214, Loss: 0.00034450989915058017\n",
      "Iteration: 181/214, Loss: 0.15471549332141876\n",
      "Iteration: 182/214, Loss: 0.03575596213340759\n",
      "Iteration: 183/214, Loss: 0.2949187457561493\n",
      "Iteration: 184/214, Loss: 0.1835033893585205\n",
      "Iteration: 185/214, Loss: 0.06738627701997757\n",
      "Iteration: 186/214, Loss: 0.04347695782780647\n",
      "Iteration: 187/214, Loss: 0.07609883695840836\n",
      "Iteration: 188/214, Loss: 0.061187490820884705\n",
      "Iteration: 189/214, Loss: 0.330594927072525\n",
      "Iteration: 190/214, Loss: 0.07612054795026779\n",
      "Iteration: 191/214, Loss: 0.13299213349819183\n",
      "Iteration: 192/214, Loss: 0.09259884804487228\n",
      "Iteration: 193/214, Loss: 0.21608591079711914\n",
      "Iteration: 194/214, Loss: 0.007826310582458973\n",
      "Iteration: 195/214, Loss: 0.08054077625274658\n",
      "Iteration: 196/214, Loss: 0.33718645572662354\n",
      "Iteration: 197/214, Loss: 0.23578840494155884\n",
      "Iteration: 198/214, Loss: 0.11799535900354385\n",
      "Iteration: 199/214, Loss: 0.03654374182224274\n",
      "Iteration: 200/214, Loss: 0.21791286766529083\n",
      "Iteration: 201/214, Loss: 0.15663598477840424\n",
      "Iteration: 202/214, Loss: 0.2729795575141907\n",
      "Iteration: 203/214, Loss: 0.018581507727503777\n",
      "Iteration: 204/214, Loss: 0.1251232624053955\n",
      "Iteration: 205/214, Loss: 0.16695207357406616\n",
      "Iteration: 206/214, Loss: 0.013518523424863815\n",
      "Iteration: 207/214, Loss: 0.28718963265419006\n",
      "Iteration: 208/214, Loss: 0.026870183646678925\n",
      "Iteration: 209/214, Loss: 0.07183639705181122\n",
      "Iteration: 210/214, Loss: 0.24660547077655792\n",
      "Iteration: 211/214, Loss: 0.07759891450405121\n",
      "Iteration: 212/214, Loss: 0.04706404730677605\n",
      "Iteration: 213/214, Loss: 0.1616193652153015\n",
      "Iteration: 214/214, Loss: 0.033595554530620575\n",
      "tensor(23.1295, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Iteration: 1/214, Loss: 0.0443330816924572\n",
      "Iteration: 2/214, Loss: 0.03328317031264305\n",
      "Iteration: 3/214, Loss: 0.04112699255347252\n",
      "Iteration: 4/214, Loss: 0.22573091089725494\n",
      "Iteration: 5/214, Loss: 0.03498828038573265\n",
      "Iteration: 6/214, Loss: 0.03023259900510311\n",
      "Iteration: 7/214, Loss: 0.04879288375377655\n",
      "Iteration: 8/214, Loss: 0.02139466442167759\n",
      "Iteration: 9/214, Loss: 0.11273651570081711\n",
      "Iteration: 10/214, Loss: 0.18113920092582703\n",
      "Iteration: 11/214, Loss: 0.07710393518209457\n",
      "Iteration: 12/214, Loss: 0.0699634701013565\n",
      "Iteration: 13/214, Loss: 0.0024057505652308464\n",
      "Iteration: 14/214, Loss: 0.3440088927745819\n",
      "Iteration: 15/214, Loss: 0.025621341541409492\n",
      "Iteration: 16/214, Loss: 0.04810414835810661\n",
      "Iteration: 17/214, Loss: 0.7545827627182007\n",
      "Iteration: 18/214, Loss: 0.0003014139656443149\n",
      "Iteration: 19/214, Loss: 0.0004728399799205363\n",
      "Iteration: 20/214, Loss: 0.1747286170721054\n",
      "Iteration: 21/214, Loss: 0.03747973591089249\n",
      "Iteration: 22/214, Loss: 0.12131506204605103\n",
      "Iteration: 23/214, Loss: 0.03323134034872055\n",
      "Iteration: 24/214, Loss: 0.2061125636100769\n",
      "Iteration: 25/214, Loss: 0.04535491392016411\n",
      "Iteration: 26/214, Loss: 0.10652967542409897\n",
      "Iteration: 27/214, Loss: 0.05879019573330879\n",
      "Iteration: 28/214, Loss: 0.11838398873806\n",
      "Iteration: 29/214, Loss: 0.051757629960775375\n",
      "Iteration: 30/214, Loss: 0.18763992190361023\n",
      "Iteration: 31/214, Loss: 0.035660602152347565\n",
      "Iteration: 32/214, Loss: 0.07928206026554108\n",
      "Iteration: 33/214, Loss: 0.001500375452451408\n",
      "Iteration: 34/214, Loss: 0.04950041323900223\n",
      "Iteration: 35/214, Loss: 0.029719900339841843\n",
      "Iteration: 36/214, Loss: 0.05770545080304146\n",
      "Iteration: 37/214, Loss: 0.09434748440980911\n",
      "Iteration: 38/214, Loss: 0.1295413225889206\n",
      "Iteration: 39/214, Loss: 0.20616182684898376\n",
      "Iteration: 40/214, Loss: 0.22796688973903656\n",
      "Iteration: 41/214, Loss: 0.07077428698539734\n",
      "Iteration: 42/214, Loss: 0.19482006132602692\n",
      "Iteration: 43/214, Loss: 0.08778885751962662\n",
      "Iteration: 44/214, Loss: 0.0005849362351000309\n",
      "Iteration: 45/214, Loss: 0.15657447278499603\n",
      "Iteration: 46/214, Loss: 0.027302546426653862\n",
      "Iteration: 47/214, Loss: 0.20884574949741364\n",
      "Iteration: 48/214, Loss: 0.04476677626371384\n",
      "Iteration: 49/214, Loss: 0.02767282724380493\n",
      "Iteration: 50/214, Loss: 0.049427054822444916\n",
      "Iteration: 51/214, Loss: 0.11183840036392212\n",
      "Iteration: 52/214, Loss: 0.16228297352790833\n",
      "Iteration: 53/214, Loss: 0.0701318085193634\n",
      "Iteration: 54/214, Loss: 0.031009819358587265\n",
      "Iteration: 55/214, Loss: 0.11322589218616486\n",
      "Iteration: 56/214, Loss: 0.041273605078458786\n",
      "Iteration: 57/214, Loss: 0.03604588285088539\n",
      "Iteration: 58/214, Loss: 0.04046972095966339\n",
      "Iteration: 59/214, Loss: 0.12576723098754883\n",
      "Iteration: 60/214, Loss: 0.0009018545970320702\n",
      "Iteration: 61/214, Loss: 0.10598069429397583\n",
      "Iteration: 62/214, Loss: 0.06382299959659576\n",
      "Iteration: 63/214, Loss: 0.0011087669990956783\n",
      "Iteration: 64/214, Loss: 0.06631677597761154\n",
      "Iteration: 65/214, Loss: 0.20676839351654053\n",
      "Iteration: 66/214, Loss: 0.2974745035171509\n",
      "Iteration: 67/214, Loss: 0.2506529986858368\n",
      "Iteration: 68/214, Loss: 0.0751267597079277\n",
      "Iteration: 69/214, Loss: 0.04875453934073448\n",
      "Iteration: 70/214, Loss: 0.034772761166095734\n",
      "Iteration: 71/214, Loss: 0.32967472076416016\n",
      "Iteration: 72/214, Loss: 0.23448018729686737\n",
      "Iteration: 73/214, Loss: 0.08669701963663101\n",
      "Iteration: 74/214, Loss: 0.05602705851197243\n",
      "Iteration: 75/214, Loss: 0.17534014582633972\n",
      "Iteration: 76/214, Loss: 0.030199037864804268\n",
      "Iteration: 77/214, Loss: 0.10514038801193237\n",
      "Iteration: 78/214, Loss: 0.12871050834655762\n",
      "Iteration: 79/214, Loss: 0.1416415125131607\n",
      "Iteration: 80/214, Loss: 0.09597547352313995\n",
      "Iteration: 81/214, Loss: 0.10118595510721207\n",
      "Iteration: 82/214, Loss: 0.036693278700113297\n",
      "Iteration: 83/214, Loss: 0.020188044756650925\n",
      "Iteration: 84/214, Loss: 0.03487132862210274\n",
      "Iteration: 85/214, Loss: 0.18954823911190033\n",
      "Iteration: 86/214, Loss: 0.10333850234746933\n",
      "Iteration: 87/214, Loss: 0.18579968810081482\n",
      "Iteration: 88/214, Loss: 0.00040927663212642074\n",
      "Iteration: 89/214, Loss: 0.03495417535305023\n",
      "Iteration: 90/214, Loss: 0.03904782608151436\n",
      "Iteration: 91/214, Loss: 0.06354518979787827\n",
      "Iteration: 92/214, Loss: 0.18373610079288483\n",
      "Iteration: 93/214, Loss: 0.2650425136089325\n",
      "Iteration: 94/214, Loss: 0.08845077455043793\n",
      "Iteration: 95/214, Loss: 0.255572646856308\n",
      "Iteration: 96/214, Loss: 0.03145371377468109\n",
      "Iteration: 97/214, Loss: 0.10802963376045227\n",
      "Iteration: 98/214, Loss: 0.2012195736169815\n",
      "Iteration: 99/214, Loss: 0.06641341745853424\n",
      "Iteration: 100/214, Loss: 0.10866285115480423\n",
      "Iteration: 101/214, Loss: 0.12191440165042877\n",
      "Iteration: 102/214, Loss: 0.007546666543930769\n",
      "Iteration: 103/214, Loss: 0.2685210406780243\n",
      "Iteration: 104/214, Loss: 0.07472316920757294\n",
      "Iteration: 105/214, Loss: 0.16190822422504425\n",
      "Iteration: 106/214, Loss: 0.0016512965084984899\n",
      "Iteration: 107/214, Loss: 0.3961569666862488\n",
      "Iteration: 108/214, Loss: 0.3705902695655823\n",
      "Iteration: 109/214, Loss: 0.027479153126478195\n",
      "Iteration: 110/214, Loss: 0.05961904302239418\n",
      "Iteration: 111/214, Loss: 0.2806003987789154\n",
      "Iteration: 112/214, Loss: 0.12958717346191406\n",
      "Iteration: 113/214, Loss: 0.14621175825595856\n",
      "Iteration: 114/214, Loss: 0.14183156192302704\n",
      "Iteration: 115/214, Loss: 0.0354602187871933\n",
      "Iteration: 116/214, Loss: 0.3020267188549042\n",
      "Iteration: 117/214, Loss: 0.12997667491436005\n",
      "Iteration: 118/214, Loss: 0.034380875527858734\n",
      "Iteration: 119/214, Loss: 0.04708520695567131\n",
      "Iteration: 120/214, Loss: 0.14863868057727814\n",
      "Iteration: 121/214, Loss: 0.03546678647398949\n",
      "Iteration: 122/214, Loss: 0.07427506148815155\n",
      "Iteration: 123/214, Loss: 0.07070901244878769\n",
      "Iteration: 124/214, Loss: 0.01798517443239689\n",
      "Iteration: 125/214, Loss: 0.025689631700515747\n",
      "Iteration: 126/214, Loss: 0.034218113869428635\n",
      "Iteration: 127/214, Loss: 0.04560062661767006\n",
      "Iteration: 128/214, Loss: 0.14897094666957855\n",
      "Iteration: 129/214, Loss: 0.2024698108434677\n",
      "Iteration: 130/214, Loss: 0.17949716746807098\n",
      "Iteration: 131/214, Loss: 0.02568003721535206\n",
      "Iteration: 132/214, Loss: 0.23101745545864105\n",
      "Iteration: 133/214, Loss: 0.0009488309733569622\n",
      "Iteration: 134/214, Loss: 0.031638212502002716\n",
      "Iteration: 135/214, Loss: 0.029895970597863197\n",
      "Iteration: 136/214, Loss: 0.011253425851464272\n",
      "Iteration: 137/214, Loss: 0.0009456278057768941\n",
      "Iteration: 138/214, Loss: 0.04353697970509529\n",
      "Iteration: 139/214, Loss: 0.050734903663396835\n",
      "Iteration: 140/214, Loss: 0.2961953282356262\n",
      "Iteration: 141/214, Loss: 0.05251679569482803\n",
      "Iteration: 142/214, Loss: 0.37183883786201477\n",
      "Iteration: 143/214, Loss: 0.04250502586364746\n",
      "Iteration: 144/214, Loss: 0.17816895246505737\n",
      "Iteration: 145/214, Loss: 0.07556858658790588\n",
      "Iteration: 146/214, Loss: 0.04389214515686035\n",
      "Iteration: 147/214, Loss: 0.14913737773895264\n",
      "Iteration: 148/214, Loss: 0.05371212586760521\n",
      "Iteration: 149/214, Loss: 0.029804052785038948\n",
      "Iteration: 150/214, Loss: 0.034830961376428604\n",
      "Iteration: 151/214, Loss: 0.03764289245009422\n",
      "Iteration: 152/214, Loss: 0.05760384723544121\n",
      "Iteration: 153/214, Loss: 0.04588845372200012\n",
      "Iteration: 154/214, Loss: 0.02698906511068344\n",
      "Iteration: 155/214, Loss: 0.3285020887851715\n",
      "Iteration: 156/214, Loss: 0.25784119963645935\n",
      "Iteration: 157/214, Loss: 0.001506317756138742\n",
      "Iteration: 158/214, Loss: 0.07464640587568283\n",
      "Iteration: 159/214, Loss: 0.033062953501939774\n",
      "Iteration: 160/214, Loss: 0.10365298390388489\n",
      "Iteration: 161/214, Loss: 0.20443026721477509\n",
      "Iteration: 162/214, Loss: 0.02777084894478321\n",
      "Iteration: 163/214, Loss: 0.10026106238365173\n",
      "Iteration: 164/214, Loss: 0.0343039408326149\n",
      "Iteration: 165/214, Loss: 0.2722446024417877\n",
      "Iteration: 166/214, Loss: 0.0010493099689483643\n",
      "Iteration: 167/214, Loss: 0.029912669211626053\n",
      "Iteration: 168/214, Loss: 0.03149537742137909\n",
      "Iteration: 169/214, Loss: 0.20369936525821686\n",
      "Iteration: 170/214, Loss: 0.269600510597229\n",
      "Iteration: 171/214, Loss: 0.24838972091674805\n",
      "Iteration: 172/214, Loss: 0.08063369244337082\n",
      "Iteration: 173/214, Loss: 0.023499667644500732\n",
      "Iteration: 174/214, Loss: 0.03531722351908684\n",
      "Iteration: 175/214, Loss: 0.15568888187408447\n",
      "Iteration: 176/214, Loss: 0.05063839256763458\n",
      "Iteration: 177/214, Loss: 0.14728274941444397\n",
      "Iteration: 178/214, Loss: 0.079500213265419\n",
      "Iteration: 179/214, Loss: 0.019466429948806763\n",
      "Iteration: 180/214, Loss: 0.00024301724624820054\n",
      "Iteration: 181/214, Loss: 0.13581492006778717\n",
      "Iteration: 182/214, Loss: 0.04970043897628784\n",
      "Iteration: 183/214, Loss: 0.24970348179340363\n",
      "Iteration: 184/214, Loss: 0.1728840172290802\n",
      "Iteration: 185/214, Loss: 0.061545293778181076\n",
      "Iteration: 186/214, Loss: 0.043763358145952225\n",
      "Iteration: 187/214, Loss: 0.05801154673099518\n",
      "Iteration: 188/214, Loss: 0.04513958469033241\n",
      "Iteration: 189/214, Loss: 0.25778552889823914\n",
      "Iteration: 190/214, Loss: 0.09160278737545013\n",
      "Iteration: 191/214, Loss: 0.12597252428531647\n",
      "Iteration: 192/214, Loss: 0.07433879375457764\n",
      "Iteration: 193/214, Loss: 0.2654620110988617\n",
      "Iteration: 194/214, Loss: 0.0043772500939667225\n",
      "Iteration: 195/214, Loss: 0.09656088054180145\n",
      "Iteration: 196/214, Loss: 0.36495286226272583\n",
      "Iteration: 197/214, Loss: 0.26275870203971863\n",
      "Iteration: 198/214, Loss: 0.14805224537849426\n",
      "Iteration: 199/214, Loss: 0.035162150859832764\n",
      "Iteration: 200/214, Loss: 0.232130229473114\n",
      "Iteration: 201/214, Loss: 0.16812150180339813\n",
      "Iteration: 202/214, Loss: 0.2433260828256607\n",
      "Iteration: 203/214, Loss: 0.015272232703864574\n",
      "Iteration: 204/214, Loss: 0.08725515007972717\n",
      "Iteration: 205/214, Loss: 0.13389842212200165\n",
      "Iteration: 206/214, Loss: 0.02424784004688263\n",
      "Iteration: 207/214, Loss: 0.2216315120458603\n",
      "Iteration: 208/214, Loss: 0.02221544459462166\n",
      "Iteration: 209/214, Loss: 0.050480715930461884\n",
      "Iteration: 210/214, Loss: 0.2289251983165741\n",
      "Iteration: 211/214, Loss: 0.07227106392383575\n",
      "Iteration: 212/214, Loss: 0.06460294127464294\n",
      "Iteration: 213/214, Loss: 0.20647315680980682\n",
      "Iteration: 214/214, Loss: 0.05420112982392311\n",
      "tensor(23.2294, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Iteration: 1/214, Loss: 0.05551687628030777\n",
      "Iteration: 2/214, Loss: 0.040772318840026855\n",
      "Iteration: 3/214, Loss: 0.04287458211183548\n",
      "Iteration: 4/214, Loss: 0.2916572690010071\n",
      "Iteration: 5/214, Loss: 0.03449269384145737\n",
      "Iteration: 6/214, Loss: 0.02489224448800087\n",
      "Iteration: 7/214, Loss: 0.06210161745548248\n",
      "Iteration: 8/214, Loss: 0.02839820645749569\n",
      "Iteration: 9/214, Loss: 0.0693696066737175\n",
      "Iteration: 10/214, Loss: 0.28095167875289917\n",
      "Iteration: 11/214, Loss: 0.11960634589195251\n",
      "Iteration: 12/214, Loss: 0.1292274296283722\n",
      "Iteration: 13/214, Loss: 0.0031182176899164915\n",
      "Iteration: 14/214, Loss: 0.37525129318237305\n",
      "Iteration: 15/214, Loss: 0.035175420343875885\n",
      "Iteration: 16/214, Loss: 0.04169056564569473\n",
      "Iteration: 17/214, Loss: 0.699118971824646\n",
      "Iteration: 18/214, Loss: 0.0003050232771784067\n",
      "Iteration: 19/214, Loss: 0.0004073731251992285\n",
      "Iteration: 20/214, Loss: 0.1557920277118683\n",
      "Iteration: 21/214, Loss: 0.050420548766851425\n",
      "Iteration: 22/214, Loss: 0.09597159177064896\n",
      "Iteration: 23/214, Loss: 0.03704399615526199\n",
      "Iteration: 24/214, Loss: 0.17510712146759033\n",
      "Iteration: 25/214, Loss: 0.06365758180618286\n",
      "Iteration: 26/214, Loss: 0.12063783407211304\n",
      "Iteration: 27/214, Loss: 0.09748655557632446\n",
      "Iteration: 28/214, Loss: 0.13727347552776337\n",
      "Iteration: 29/214, Loss: 0.034909144043922424\n",
      "Iteration: 30/214, Loss: 0.22413791716098785\n",
      "Iteration: 31/214, Loss: 0.024068014696240425\n",
      "Iteration: 32/214, Loss: 0.08271998912096024\n",
      "Iteration: 33/214, Loss: 0.0011466490104794502\n",
      "Iteration: 34/214, Loss: 0.031217826530337334\n",
      "Iteration: 35/214, Loss: 0.021627740934491158\n",
      "Iteration: 36/214, Loss: 0.051737621426582336\n",
      "Iteration: 37/214, Loss: 0.13939616084098816\n",
      "Iteration: 38/214, Loss: 0.08267108350992203\n",
      "Iteration: 39/214, Loss: 0.2546711266040802\n",
      "Iteration: 40/214, Loss: 0.2271617352962494\n",
      "Iteration: 41/214, Loss: 0.05717237666249275\n",
      "Iteration: 42/214, Loss: 0.17169463634490967\n",
      "Iteration: 43/214, Loss: 0.13176214694976807\n",
      "Iteration: 44/214, Loss: 0.0007135634077712893\n",
      "Iteration: 45/214, Loss: 0.3066575825214386\n",
      "Iteration: 46/214, Loss: 0.026407599449157715\n",
      "Iteration: 47/214, Loss: 0.2035602629184723\n",
      "Iteration: 48/214, Loss: 0.01740417629480362\n",
      "Iteration: 49/214, Loss: 0.028794700279831886\n",
      "Iteration: 50/214, Loss: 0.022043969482183456\n",
      "Iteration: 51/214, Loss: 0.06769798696041107\n",
      "Iteration: 52/214, Loss: 0.21626240015029907\n",
      "Iteration: 53/214, Loss: 0.06256818026304245\n",
      "Iteration: 54/214, Loss: 0.026040254160761833\n",
      "Iteration: 55/214, Loss: 0.18801720440387726\n",
      "Iteration: 56/214, Loss: 0.029485473409295082\n",
      "Iteration: 57/214, Loss: 0.031960178166627884\n",
      "Iteration: 58/214, Loss: 0.027477538213133812\n",
      "Iteration: 59/214, Loss: 0.21897156536579132\n",
      "Iteration: 60/214, Loss: 0.000567568582482636\n",
      "Iteration: 61/214, Loss: 0.10255750268697739\n",
      "Iteration: 62/214, Loss: 0.03173762187361717\n",
      "Iteration: 63/214, Loss: 0.0008829044527374208\n",
      "Iteration: 64/214, Loss: 0.04507851228117943\n",
      "Iteration: 65/214, Loss: 0.22305890917778015\n",
      "Iteration: 66/214, Loss: 0.25988367199897766\n",
      "Iteration: 67/214, Loss: 0.2727353572845459\n",
      "Iteration: 68/214, Loss: 0.06098411604762077\n",
      "Iteration: 69/214, Loss: 0.07773390412330627\n",
      "Iteration: 70/214, Loss: 0.07795464992523193\n",
      "Iteration: 71/214, Loss: 0.23654629290103912\n",
      "Iteration: 72/214, Loss: 0.1758677214384079\n",
      "Iteration: 73/214, Loss: 0.07695115357637405\n",
      "Iteration: 74/214, Loss: 0.0659174919128418\n",
      "Iteration: 75/214, Loss: 0.17577148973941803\n",
      "Iteration: 76/214, Loss: 0.015134142711758614\n",
      "Iteration: 77/214, Loss: 0.06782142072916031\n",
      "Iteration: 78/214, Loss: 0.12263113260269165\n",
      "Iteration: 79/214, Loss: 0.07942388206720352\n",
      "Iteration: 80/214, Loss: 0.07161670178174973\n",
      "Iteration: 81/214, Loss: 0.08130286633968353\n",
      "Iteration: 82/214, Loss: 0.04321525990962982\n",
      "Iteration: 83/214, Loss: 0.0194864384829998\n",
      "Iteration: 84/214, Loss: 0.034169819205999374\n",
      "Iteration: 85/214, Loss: 0.15564779937267303\n",
      "Iteration: 86/214, Loss: 0.08643227070569992\n",
      "Iteration: 87/214, Loss: 0.1215062141418457\n",
      "Iteration: 88/214, Loss: 0.000823287176899612\n",
      "Iteration: 89/214, Loss: 0.02916780859231949\n",
      "Iteration: 90/214, Loss: 0.04263938590884209\n",
      "Iteration: 91/214, Loss: 0.042775921523571014\n",
      "Iteration: 92/214, Loss: 0.11283112317323685\n",
      "Iteration: 93/214, Loss: 0.22948087751865387\n",
      "Iteration: 94/214, Loss: 0.05599385127425194\n",
      "Iteration: 95/214, Loss: 0.2798115909099579\n",
      "Iteration: 96/214, Loss: 0.03413695469498634\n",
      "Iteration: 97/214, Loss: 0.09786397963762283\n",
      "Iteration: 98/214, Loss: 0.17841291427612305\n",
      "Iteration: 99/214, Loss: 0.03623737022280693\n",
      "Iteration: 100/214, Loss: 0.09977807104587555\n",
      "Iteration: 101/214, Loss: 0.097732312977314\n",
      "Iteration: 102/214, Loss: 0.00137240847107023\n",
      "Iteration: 103/214, Loss: 0.1986769586801529\n",
      "Iteration: 104/214, Loss: 0.06447411328554153\n",
      "Iteration: 105/214, Loss: 0.13322752714157104\n",
      "Iteration: 106/214, Loss: 0.0007435032748617232\n",
      "Iteration: 107/214, Loss: 0.36515867710113525\n",
      "Iteration: 108/214, Loss: 0.3252466320991516\n",
      "Iteration: 109/214, Loss: 0.032690878957509995\n",
      "Iteration: 110/214, Loss: 0.03211255744099617\n",
      "Iteration: 111/214, Loss: 0.1978556513786316\n",
      "Iteration: 112/214, Loss: 0.0880763828754425\n",
      "Iteration: 113/214, Loss: 0.11535736918449402\n",
      "Iteration: 114/214, Loss: 0.0706595852971077\n",
      "Iteration: 115/214, Loss: 0.02655889466404915\n",
      "Iteration: 116/214, Loss: 0.22532884776592255\n",
      "Iteration: 117/214, Loss: 0.10322502255439758\n",
      "Iteration: 118/214, Loss: 0.042936667799949646\n",
      "Iteration: 119/214, Loss: 0.02148064225912094\n",
      "Iteration: 120/214, Loss: 0.16997256875038147\n",
      "Iteration: 121/214, Loss: 0.022933367639780045\n",
      "Iteration: 122/214, Loss: 0.03838590905070305\n",
      "Iteration: 123/214, Loss: 0.055992577224969864\n",
      "Iteration: 124/214, Loss: 0.023263337090611458\n",
      "Iteration: 125/214, Loss: 0.0448799766600132\n",
      "Iteration: 126/214, Loss: 0.009698261506855488\n",
      "Iteration: 127/214, Loss: 0.04272416979074478\n",
      "Iteration: 128/214, Loss: 0.11780216544866562\n",
      "Iteration: 129/214, Loss: 0.16397534310817719\n",
      "Iteration: 130/214, Loss: 0.13411283493041992\n",
      "Iteration: 131/214, Loss: 0.033577121794223785\n",
      "Iteration: 132/214, Loss: 0.19218766689300537\n",
      "Iteration: 133/214, Loss: 0.0012224564561620355\n",
      "Iteration: 134/214, Loss: 0.022171420976519585\n",
      "Iteration: 135/214, Loss: 0.02237994410097599\n",
      "Iteration: 136/214, Loss: 0.00033769101719371974\n",
      "Iteration: 137/214, Loss: 0.0006623121444135904\n",
      "Iteration: 138/214, Loss: 0.032713599503040314\n",
      "Iteration: 139/214, Loss: 0.023972470313310623\n",
      "Iteration: 140/214, Loss: 0.2207067608833313\n",
      "Iteration: 141/214, Loss: 0.05316412076354027\n",
      "Iteration: 142/214, Loss: 0.2684749364852905\n",
      "Iteration: 143/214, Loss: 0.02317662537097931\n",
      "Iteration: 144/214, Loss: 0.13978508114814758\n",
      "Iteration: 145/214, Loss: 0.06420911103487015\n",
      "Iteration: 146/214, Loss: 0.02117007225751877\n",
      "Iteration: 147/214, Loss: 0.19281132519245148\n",
      "Iteration: 148/214, Loss: 0.08916375041007996\n",
      "Iteration: 149/214, Loss: 0.033312708139419556\n",
      "Iteration: 150/214, Loss: 0.05019212141633034\n",
      "Iteration: 151/214, Loss: 0.0271895844489336\n",
      "Iteration: 152/214, Loss: 0.08476997911930084\n",
      "Iteration: 153/214, Loss: 0.05687088146805763\n",
      "Iteration: 154/214, Loss: 0.024974394589662552\n",
      "Iteration: 155/214, Loss: 0.20825313031673431\n",
      "Iteration: 156/214, Loss: 0.1636626273393631\n",
      "Iteration: 157/214, Loss: 0.000698074814863503\n",
      "Iteration: 158/214, Loss: 0.08629626035690308\n",
      "Iteration: 159/214, Loss: 0.018248023465275764\n",
      "Iteration: 160/214, Loss: 0.10375013947486877\n",
      "Iteration: 161/214, Loss: 0.22042836248874664\n",
      "Iteration: 162/214, Loss: 0.029234079644083977\n",
      "Iteration: 163/214, Loss: 0.08976925164461136\n",
      "Iteration: 164/214, Loss: 0.027927465736865997\n",
      "Iteration: 165/214, Loss: 0.2405373454093933\n",
      "Iteration: 166/214, Loss: 0.0010095168836414814\n",
      "Iteration: 167/214, Loss: 0.016857003793120384\n",
      "Iteration: 168/214, Loss: 0.01703457720577717\n",
      "Iteration: 169/214, Loss: 0.21453332901000977\n",
      "Iteration: 170/214, Loss: 0.18365263938903809\n",
      "Iteration: 171/214, Loss: 0.18496568500995636\n",
      "Iteration: 172/214, Loss: 0.07512688636779785\n",
      "Iteration: 173/214, Loss: 0.031247109174728394\n",
      "Iteration: 174/214, Loss: 0.015476698987185955\n",
      "Iteration: 175/214, Loss: 0.14306016266345978\n",
      "Iteration: 176/214, Loss: 0.03826489672064781\n",
      "Iteration: 177/214, Loss: 0.14742043614387512\n",
      "Iteration: 178/214, Loss: 0.049016743898391724\n",
      "Iteration: 179/214, Loss: 0.01332832034677267\n",
      "Iteration: 180/214, Loss: 0.0003026809135917574\n",
      "Iteration: 181/214, Loss: 0.11113040149211884\n",
      "Iteration: 182/214, Loss: 0.04633486643433571\n",
      "Iteration: 183/214, Loss: 0.21539685130119324\n",
      "Iteration: 184/214, Loss: 0.21251806616783142\n",
      "Iteration: 185/214, Loss: 0.059946317225694656\n",
      "Iteration: 186/214, Loss: 0.050677340477705\n",
      "Iteration: 187/214, Loss: 0.04542297497391701\n",
      "Iteration: 188/214, Loss: 0.05276073142886162\n",
      "Iteration: 189/214, Loss: 0.2535190284252167\n",
      "Iteration: 190/214, Loss: 0.06703270971775055\n",
      "Iteration: 191/214, Loss: 0.1289690136909485\n",
      "Iteration: 192/214, Loss: 0.025118276476860046\n",
      "Iteration: 193/214, Loss: 0.21474498510360718\n",
      "Iteration: 194/214, Loss: 0.0025861787144094706\n",
      "Iteration: 195/214, Loss: 0.06216740608215332\n",
      "Iteration: 196/214, Loss: 0.21287298202514648\n",
      "Iteration: 197/214, Loss: 0.19845961034297943\n",
      "Iteration: 198/214, Loss: 0.1412843018770218\n",
      "Iteration: 199/214, Loss: 0.032080866396427155\n",
      "Iteration: 200/214, Loss: 0.2383398413658142\n",
      "Iteration: 201/214, Loss: 0.1352684497833252\n",
      "Iteration: 202/214, Loss: 0.19404558837413788\n",
      "Iteration: 203/214, Loss: 0.03607862815260887\n",
      "Iteration: 204/214, Loss: 0.10687950253486633\n",
      "Iteration: 205/214, Loss: 0.1906827986240387\n",
      "Iteration: 206/214, Loss: 0.03737165406346321\n",
      "Iteration: 207/214, Loss: 0.30932390689849854\n",
      "Iteration: 208/214, Loss: 0.03131188079714775\n",
      "Iteration: 209/214, Loss: 0.10037899017333984\n",
      "Iteration: 210/214, Loss: 0.26522505283355713\n",
      "Iteration: 211/214, Loss: 0.0908258706331253\n",
      "Iteration: 212/214, Loss: 0.07408179342746735\n",
      "Iteration: 213/214, Loss: 0.18684476613998413\n",
      "Iteration: 214/214, Loss: 0.05260101333260536\n",
      "tensor(21.4493, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Iteration: 1/214, Loss: 0.056842152029275894\n",
      "Iteration: 2/214, Loss: 0.030960194766521454\n",
      "Iteration: 3/214, Loss: 0.062230274081230164\n",
      "Iteration: 4/214, Loss: 0.2920851707458496\n",
      "Iteration: 5/214, Loss: 0.0363788977265358\n",
      "Iteration: 6/214, Loss: 0.047199543565511703\n",
      "Iteration: 7/214, Loss: 0.0795605406165123\n",
      "Iteration: 8/214, Loss: 0.056963179260492325\n",
      "Iteration: 9/214, Loss: 0.13818968832492828\n",
      "Iteration: 10/214, Loss: 0.3388882279396057\n",
      "Iteration: 11/214, Loss: 0.1197979748249054\n",
      "Iteration: 12/214, Loss: 0.12236863374710083\n",
      "Iteration: 13/214, Loss: 0.0033943450544029474\n",
      "Iteration: 14/214, Loss: 0.40442317724227905\n",
      "Iteration: 15/214, Loss: 0.02644234150648117\n",
      "Iteration: 16/214, Loss: 0.03243885934352875\n",
      "Iteration: 17/214, Loss: 0.6363488435745239\n",
      "Iteration: 18/214, Loss: 9.146509546553716e-05\n",
      "Iteration: 19/214, Loss: 0.0003455691912677139\n",
      "Iteration: 20/214, Loss: 0.19531357288360596\n",
      "Iteration: 21/214, Loss: 0.045838646590709686\n",
      "Iteration: 22/214, Loss: 0.11416246742010117\n",
      "Iteration: 23/214, Loss: 0.058051686733961105\n",
      "Iteration: 24/214, Loss: 0.2285826951265335\n",
      "Iteration: 25/214, Loss: 0.05490545555949211\n",
      "Iteration: 26/214, Loss: 0.09338217228651047\n",
      "Iteration: 27/214, Loss: 0.06686922907829285\n",
      "Iteration: 28/214, Loss: 0.08906649798154831\n",
      "Iteration: 29/214, Loss: 0.029907993972301483\n",
      "Iteration: 30/214, Loss: 0.20723937451839447\n",
      "Iteration: 31/214, Loss: 0.04602234438061714\n",
      "Iteration: 32/214, Loss: 0.10683208703994751\n",
      "Iteration: 33/214, Loss: 0.0013768095523118973\n",
      "Iteration: 34/214, Loss: 0.03841090574860573\n",
      "Iteration: 35/214, Loss: 0.05788608267903328\n",
      "Iteration: 36/214, Loss: 0.08394031226634979\n",
      "Iteration: 37/214, Loss: 0.14334484934806824\n",
      "Iteration: 38/214, Loss: 0.14024338126182556\n",
      "Iteration: 39/214, Loss: 0.2383338361978531\n",
      "Iteration: 40/214, Loss: 0.25663506984710693\n",
      "Iteration: 41/214, Loss: 0.06519552320241928\n",
      "Iteration: 42/214, Loss: 0.15375392138957977\n",
      "Iteration: 43/214, Loss: 0.09623727202415466\n",
      "Iteration: 44/214, Loss: 0.0006243220414035022\n",
      "Iteration: 45/214, Loss: 0.2336728572845459\n",
      "Iteration: 46/214, Loss: 0.04995497688651085\n",
      "Iteration: 47/214, Loss: 0.2708943486213684\n",
      "Iteration: 48/214, Loss: 0.036139704287052155\n",
      "Iteration: 49/214, Loss: 0.027060935273766518\n",
      "Iteration: 50/214, Loss: 0.04758412018418312\n",
      "Iteration: 51/214, Loss: 0.08890222758054733\n",
      "Iteration: 52/214, Loss: 0.2609970271587372\n",
      "Iteration: 53/214, Loss: 0.04870098456740379\n",
      "Iteration: 54/214, Loss: 0.014725913293659687\n",
      "Iteration: 55/214, Loss: 0.16428890824317932\n",
      "Iteration: 56/214, Loss: 0.02364620938897133\n",
      "Iteration: 57/214, Loss: 0.048757705837488174\n",
      "Iteration: 58/214, Loss: 0.03550146520137787\n",
      "Iteration: 59/214, Loss: 0.12133198231458664\n",
      "Iteration: 60/214, Loss: 0.0006848656921647489\n",
      "Iteration: 61/214, Loss: 0.11137249320745468\n",
      "Iteration: 62/214, Loss: 0.04667344689369202\n",
      "Iteration: 63/214, Loss: 0.0006857268163003027\n",
      "Iteration: 64/214, Loss: 0.05492720007896423\n",
      "Iteration: 65/214, Loss: 0.20695695281028748\n",
      "Iteration: 66/214, Loss: 0.2866683006286621\n",
      "Iteration: 67/214, Loss: 0.258543461561203\n",
      "Iteration: 68/214, Loss: 0.06706643104553223\n",
      "Iteration: 69/214, Loss: 0.044930342584848404\n",
      "Iteration: 70/214, Loss: 0.04054933041334152\n",
      "Iteration: 71/214, Loss: 0.26876309514045715\n",
      "Iteration: 72/214, Loss: 0.22076497972011566\n",
      "Iteration: 73/214, Loss: 0.1146763488650322\n",
      "Iteration: 74/214, Loss: 0.07722482085227966\n",
      "Iteration: 75/214, Loss: 0.1997377574443817\n",
      "Iteration: 76/214, Loss: 0.011926639825105667\n",
      "Iteration: 77/214, Loss: 0.0417000837624073\n",
      "Iteration: 78/214, Loss: 0.1071556881070137\n",
      "Iteration: 79/214, Loss: 0.09280737489461899\n",
      "Iteration: 80/214, Loss: 0.08861549943685532\n",
      "Iteration: 81/214, Loss: 0.09513770043849945\n",
      "Iteration: 82/214, Loss: 0.022291108965873718\n",
      "Iteration: 83/214, Loss: 0.017503220587968826\n",
      "Iteration: 84/214, Loss: 0.029850684106349945\n",
      "Iteration: 85/214, Loss: 0.16366395354270935\n",
      "Iteration: 86/214, Loss: 0.08330538123846054\n",
      "Iteration: 87/214, Loss: 0.13501429557800293\n",
      "Iteration: 88/214, Loss: 0.0005902638076804578\n",
      "Iteration: 89/214, Loss: 0.01716853491961956\n",
      "Iteration: 90/214, Loss: 0.027307331562042236\n",
      "Iteration: 91/214, Loss: 0.046585697680711746\n",
      "Iteration: 92/214, Loss: 0.10103374719619751\n",
      "Iteration: 93/214, Loss: 0.20010270178318024\n",
      "Iteration: 94/214, Loss: 0.052147943526506424\n",
      "Iteration: 95/214, Loss: 0.2082752287387848\n",
      "Iteration: 96/214, Loss: 0.03381222486495972\n",
      "Iteration: 97/214, Loss: 0.11506976187229156\n",
      "Iteration: 98/214, Loss: 0.22527819871902466\n",
      "Iteration: 99/214, Loss: 0.06336432695388794\n",
      "Iteration: 100/214, Loss: 0.10913842171430588\n",
      "Iteration: 101/214, Loss: 0.09351696819067001\n",
      "Iteration: 102/214, Loss: 0.0011239468585699797\n",
      "Iteration: 103/214, Loss: 0.17122748494148254\n",
      "Iteration: 104/214, Loss: 0.04604673758149147\n",
      "Iteration: 105/214, Loss: 0.09163247793912888\n",
      "Iteration: 106/214, Loss: 0.0009469088399782777\n",
      "Iteration: 107/214, Loss: 0.331259161233902\n",
      "Iteration: 108/214, Loss: 0.356729656457901\n",
      "Iteration: 109/214, Loss: 0.027333304286003113\n",
      "Iteration: 110/214, Loss: 0.06773381680250168\n",
      "Iteration: 111/214, Loss: 0.16698580980300903\n",
      "Iteration: 112/214, Loss: 0.15524962544441223\n",
      "Iteration: 113/214, Loss: 0.09882970154285431\n",
      "Iteration: 114/214, Loss: 0.06689030677080154\n",
      "Iteration: 115/214, Loss: 0.03560999035835266\n",
      "Iteration: 116/214, Loss: 0.20913748443126678\n",
      "Iteration: 117/214, Loss: 0.1223314180970192\n",
      "Iteration: 118/214, Loss: 0.03834749013185501\n",
      "Iteration: 119/214, Loss: 0.04406731575727463\n",
      "Iteration: 120/214, Loss: 0.17062929272651672\n",
      "Iteration: 121/214, Loss: 0.029695026576519012\n",
      "Iteration: 122/214, Loss: 0.03916110098361969\n",
      "Iteration: 123/214, Loss: 0.08088315278291702\n",
      "Iteration: 124/214, Loss: 0.022417955100536346\n",
      "Iteration: 125/214, Loss: 0.04384678602218628\n",
      "Iteration: 126/214, Loss: 0.022086024284362793\n",
      "Iteration: 127/214, Loss: 0.019934872165322304\n",
      "Iteration: 128/214, Loss: 0.12350334227085114\n",
      "Iteration: 129/214, Loss: 0.2002134621143341\n",
      "Iteration: 130/214, Loss: 0.18214848637580872\n",
      "Iteration: 131/214, Loss: 0.03435695171356201\n",
      "Iteration: 132/214, Loss: 0.1823256015777588\n",
      "Iteration: 133/214, Loss: 0.0006845122552476823\n",
      "Iteration: 134/214, Loss: 0.042474471032619476\n",
      "Iteration: 135/214, Loss: 0.03929745405912399\n",
      "Iteration: 136/214, Loss: 0.00045370732550509274\n",
      "Iteration: 137/214, Loss: 0.0005974849336780608\n",
      "Iteration: 138/214, Loss: 0.0379372164607048\n",
      "Iteration: 139/214, Loss: 0.025921985507011414\n",
      "Iteration: 140/214, Loss: 0.28424546122550964\n",
      "Iteration: 141/214, Loss: 0.03428182378411293\n",
      "Iteration: 142/214, Loss: 0.29218509793281555\n",
      "Iteration: 143/214, Loss: 0.05180131271481514\n",
      "Iteration: 144/214, Loss: 0.18045805394649506\n",
      "Iteration: 145/214, Loss: 0.07933533936738968\n",
      "Iteration: 146/214, Loss: 0.031230539083480835\n",
      "Iteration: 147/214, Loss: 0.21301305294036865\n",
      "Iteration: 148/214, Loss: 0.11944429576396942\n",
      "Iteration: 149/214, Loss: 0.03408220410346985\n",
      "Iteration: 150/214, Loss: 0.02273748256266117\n",
      "Iteration: 151/214, Loss: 0.023084290325641632\n",
      "Iteration: 152/214, Loss: 0.10300035774707794\n",
      "Iteration: 153/214, Loss: 0.037473879754543304\n",
      "Iteration: 154/214, Loss: 0.030985385179519653\n",
      "Iteration: 155/214, Loss: 0.2991035282611847\n",
      "Iteration: 156/214, Loss: 0.18200090527534485\n",
      "Iteration: 157/214, Loss: 0.00121262576431036\n",
      "Iteration: 158/214, Loss: 0.07021884620189667\n",
      "Iteration: 159/214, Loss: 0.04130242392420769\n",
      "Iteration: 160/214, Loss: 0.09788385033607483\n",
      "Iteration: 161/214, Loss: 0.21957682073116302\n",
      "Iteration: 162/214, Loss: 0.047165412455797195\n",
      "Iteration: 163/214, Loss: 0.08199533820152283\n",
      "Iteration: 164/214, Loss: 0.024240311235189438\n",
      "Iteration: 165/214, Loss: 0.28676193952560425\n",
      "Iteration: 166/214, Loss: 0.0006755874492228031\n",
      "Iteration: 167/214, Loss: 0.01909461058676243\n",
      "Iteration: 168/214, Loss: 0.01834363490343094\n",
      "Iteration: 169/214, Loss: 0.14815448224544525\n",
      "Iteration: 170/214, Loss: 0.21027790009975433\n",
      "Iteration: 171/214, Loss: 0.260091096162796\n",
      "Iteration: 172/214, Loss: 0.11557327955961227\n",
      "Iteration: 173/214, Loss: 0.028138654306530952\n",
      "Iteration: 174/214, Loss: 0.02646343782544136\n",
      "Iteration: 175/214, Loss: 0.18301954865455627\n",
      "Iteration: 176/214, Loss: 0.08163147419691086\n",
      "Iteration: 177/214, Loss: 0.19136658310890198\n",
      "Iteration: 178/214, Loss: 0.06105019524693489\n",
      "Iteration: 179/214, Loss: 0.024321025237441063\n",
      "Iteration: 180/214, Loss: 0.00030521131702698767\n",
      "Iteration: 181/214, Loss: 0.11681897938251495\n",
      "Iteration: 182/214, Loss: 0.027055980637669563\n",
      "Iteration: 183/214, Loss: 0.3094814717769623\n",
      "Iteration: 184/214, Loss: 0.16046763956546783\n",
      "Iteration: 185/214, Loss: 0.06739216297864914\n",
      "Iteration: 186/214, Loss: 0.04814111068844795\n",
      "Iteration: 187/214, Loss: 0.05470268428325653\n",
      "Iteration: 188/214, Loss: 0.04094443470239639\n",
      "Iteration: 189/214, Loss: 0.32360345125198364\n",
      "Iteration: 190/214, Loss: 0.08719640970230103\n",
      "Iteration: 191/214, Loss: 0.14273682236671448\n",
      "Iteration: 192/214, Loss: 0.06729135662317276\n",
      "Iteration: 193/214, Loss: 0.1882978081703186\n",
      "Iteration: 194/214, Loss: 0.002394546288996935\n",
      "Iteration: 195/214, Loss: 0.07760266959667206\n",
      "Iteration: 196/214, Loss: 0.22738194465637207\n",
      "Iteration: 197/214, Loss: 0.17694273591041565\n",
      "Iteration: 198/214, Loss: 0.12449973076581955\n",
      "Iteration: 199/214, Loss: 0.02971666119992733\n",
      "Iteration: 200/214, Loss: 0.24479757249355316\n",
      "Iteration: 201/214, Loss: 0.17186394333839417\n",
      "Iteration: 202/214, Loss: 0.27399876713752747\n",
      "Iteration: 203/214, Loss: 0.028936509042978287\n",
      "Iteration: 204/214, Loss: 0.13002416491508484\n",
      "Iteration: 205/214, Loss: 0.1772867739200592\n",
      "Iteration: 206/214, Loss: 0.021667536348104477\n",
      "Iteration: 207/214, Loss: 0.30254730582237244\n",
      "Iteration: 208/214, Loss: 0.04053992033004761\n",
      "Iteration: 209/214, Loss: 0.03617076948285103\n",
      "Iteration: 210/214, Loss: 0.24502164125442505\n",
      "Iteration: 211/214, Loss: 0.08306840062141418\n",
      "Iteration: 212/214, Loss: 0.0601433627307415\n",
      "Iteration: 213/214, Loss: 0.17262697219848633\n",
      "Iteration: 214/214, Loss: 0.04003389924764633\n",
      "tensor(22.6406, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Iteration: 1/214, Loss: 0.04054734483361244\n",
      "Iteration: 2/214, Loss: 0.02948722243309021\n",
      "Iteration: 3/214, Loss: 0.04623381048440933\n",
      "Iteration: 4/214, Loss: 0.3186357617378235\n",
      "Iteration: 5/214, Loss: 0.03603777661919594\n",
      "Iteration: 6/214, Loss: 0.037496380507946014\n",
      "Iteration: 7/214, Loss: 0.055244334042072296\n",
      "Iteration: 8/214, Loss: 0.03842302784323692\n",
      "Iteration: 9/214, Loss: 0.12941928207874298\n",
      "Iteration: 10/214, Loss: 0.29659682512283325\n",
      "Iteration: 11/214, Loss: 0.1432025134563446\n",
      "Iteration: 12/214, Loss: 0.09106532484292984\n",
      "Iteration: 13/214, Loss: 0.0014387972187250853\n",
      "Iteration: 14/214, Loss: 0.4370421767234802\n",
      "Iteration: 15/214, Loss: 0.025966571643948555\n",
      "Iteration: 16/214, Loss: 0.0276408102363348\n",
      "Iteration: 17/214, Loss: 0.6381423473358154\n",
      "Iteration: 18/214, Loss: 0.0002966794418171048\n",
      "Iteration: 19/214, Loss: 0.0004523007373791188\n",
      "Iteration: 20/214, Loss: 0.14455822110176086\n",
      "Iteration: 21/214, Loss: 0.03737200051546097\n",
      "Iteration: 22/214, Loss: 0.10714315623044968\n",
      "Iteration: 23/214, Loss: 0.042765915393829346\n",
      "Iteration: 24/214, Loss: 0.1872052401304245\n",
      "Iteration: 25/214, Loss: 0.05489334836602211\n",
      "Iteration: 26/214, Loss: 0.13291507959365845\n",
      "Iteration: 27/214, Loss: 0.06941985338926315\n",
      "Iteration: 28/214, Loss: 0.17830488085746765\n",
      "Iteration: 29/214, Loss: 0.08072975277900696\n",
      "Iteration: 30/214, Loss: 0.1174888014793396\n",
      "Iteration: 31/214, Loss: 0.02433064579963684\n",
      "Iteration: 32/214, Loss: 0.0930042639374733\n",
      "Iteration: 33/214, Loss: 0.0010180824901908636\n",
      "Iteration: 34/214, Loss: 0.048214152455329895\n",
      "Iteration: 35/214, Loss: 0.03765705227851868\n",
      "Iteration: 36/214, Loss: 0.06423458456993103\n",
      "Iteration: 37/214, Loss: 0.16609430313110352\n",
      "Iteration: 38/214, Loss: 0.1467641144990921\n",
      "Iteration: 39/214, Loss: 0.20805665850639343\n",
      "Iteration: 40/214, Loss: 0.22933423519134521\n",
      "Iteration: 41/214, Loss: 0.0661364272236824\n",
      "Iteration: 42/214, Loss: 0.15256546437740326\n",
      "Iteration: 43/214, Loss: 0.10232982784509659\n",
      "Iteration: 44/214, Loss: 0.0005350664141587913\n",
      "Iteration: 45/214, Loss: 0.2282356470823288\n",
      "Iteration: 46/214, Loss: 0.05138147249817848\n",
      "Iteration: 47/214, Loss: 0.2964957058429718\n",
      "Iteration: 48/214, Loss: 0.04092119634151459\n",
      "Iteration: 49/214, Loss: 0.035132937133312225\n",
      "Iteration: 50/214, Loss: 0.036963190883398056\n",
      "Iteration: 51/214, Loss: 0.10125362128019333\n",
      "Iteration: 52/214, Loss: 0.2399100810289383\n",
      "Iteration: 53/214, Loss: 0.06028461828827858\n",
      "Iteration: 54/214, Loss: 0.023302363231778145\n",
      "Iteration: 55/214, Loss: 0.20563681423664093\n",
      "Iteration: 56/214, Loss: 0.03578857332468033\n",
      "Iteration: 57/214, Loss: 0.04063815623521805\n",
      "Iteration: 58/214, Loss: 0.031163156032562256\n",
      "Iteration: 59/214, Loss: 0.18633979558944702\n",
      "Iteration: 60/214, Loss: 0.0005005401908420026\n",
      "Iteration: 61/214, Loss: 0.09415421634912491\n",
      "Iteration: 62/214, Loss: 0.045948076993227005\n",
      "Iteration: 63/214, Loss: 0.0008405769476667047\n",
      "Iteration: 64/214, Loss: 0.06400738656520844\n",
      "Iteration: 65/214, Loss: 0.19386570155620575\n",
      "Iteration: 66/214, Loss: 0.25306981801986694\n",
      "Iteration: 67/214, Loss: 0.2598843574523926\n",
      "Iteration: 68/214, Loss: 0.057631514966487885\n",
      "Iteration: 69/214, Loss: 0.07042282074689865\n",
      "Iteration: 70/214, Loss: 0.05190957337617874\n",
      "Iteration: 71/214, Loss: 0.29268577694892883\n",
      "Iteration: 72/214, Loss: 0.15351037681102753\n",
      "Iteration: 73/214, Loss: 0.13368402421474457\n",
      "Iteration: 74/214, Loss: 0.06926405429840088\n",
      "Iteration: 75/214, Loss: 0.17794005572795868\n",
      "Iteration: 76/214, Loss: 0.023047903552651405\n",
      "Iteration: 77/214, Loss: 0.05539240688085556\n",
      "Iteration: 78/214, Loss: 0.08088716119527817\n",
      "Iteration: 79/214, Loss: 0.09685560315847397\n",
      "Iteration: 80/214, Loss: 0.07689856737852097\n",
      "Iteration: 81/214, Loss: 0.08329512178897858\n",
      "Iteration: 82/214, Loss: 0.029502779245376587\n",
      "Iteration: 83/214, Loss: 0.021031247451901436\n",
      "Iteration: 84/214, Loss: 0.013905586674809456\n",
      "Iteration: 85/214, Loss: 0.15292459726333618\n",
      "Iteration: 86/214, Loss: 0.09843027591705322\n",
      "Iteration: 87/214, Loss: 0.158079594373703\n",
      "Iteration: 88/214, Loss: 0.0005381715018302202\n",
      "Iteration: 89/214, Loss: 0.026799727231264114\n",
      "Iteration: 90/214, Loss: 0.02465972676873207\n",
      "Iteration: 91/214, Loss: 0.04150915890932083\n",
      "Iteration: 92/214, Loss: 0.11105814576148987\n",
      "Iteration: 93/214, Loss: 0.26006031036376953\n",
      "Iteration: 94/214, Loss: 0.06032818183302879\n",
      "Iteration: 95/214, Loss: 0.19877980649471283\n",
      "Iteration: 96/214, Loss: 0.029809020459651947\n",
      "Iteration: 97/214, Loss: 0.08930137753486633\n",
      "Iteration: 98/214, Loss: 0.2357138991355896\n",
      "Iteration: 99/214, Loss: 0.056049708276987076\n",
      "Iteration: 100/214, Loss: 0.16230516135692596\n",
      "Iteration: 101/214, Loss: 0.15468169748783112\n",
      "Iteration: 102/214, Loss: 0.0008614924736320972\n",
      "Iteration: 103/214, Loss: 0.25509682297706604\n",
      "Iteration: 104/214, Loss: 0.07634569704532623\n",
      "Iteration: 105/214, Loss: 0.13333149254322052\n",
      "Iteration: 106/214, Loss: 0.0006649489514529705\n",
      "Iteration: 107/214, Loss: 0.30198052525520325\n",
      "Iteration: 108/214, Loss: 0.31468677520751953\n",
      "Iteration: 109/214, Loss: 0.031970854848623276\n",
      "Iteration: 110/214, Loss: 0.07227954268455505\n",
      "Iteration: 111/214, Loss: 0.3121107518672943\n",
      "Iteration: 112/214, Loss: 0.16609492897987366\n",
      "Iteration: 113/214, Loss: 0.1834673434495926\n",
      "Iteration: 114/214, Loss: 0.14986172318458557\n",
      "Iteration: 115/214, Loss: 0.04269575700163841\n",
      "Iteration: 116/214, Loss: 0.2794053852558136\n",
      "Iteration: 117/214, Loss: 0.13222414255142212\n",
      "Iteration: 118/214, Loss: 0.05235564336180687\n",
      "Iteration: 119/214, Loss: 0.04023544862866402\n",
      "Iteration: 120/214, Loss: 0.18734821677207947\n",
      "Iteration: 121/214, Loss: 0.03258730471134186\n",
      "Iteration: 122/214, Loss: 0.03362645208835602\n",
      "Iteration: 123/214, Loss: 0.1091751903295517\n",
      "Iteration: 124/214, Loss: 0.061076145619153976\n",
      "Iteration: 125/214, Loss: 0.034974921494722366\n",
      "Iteration: 126/214, Loss: 0.04174863547086716\n",
      "Iteration: 127/214, Loss: 0.03786015510559082\n",
      "Iteration: 128/214, Loss: 0.22673329710960388\n",
      "Iteration: 129/214, Loss: 0.42240968346595764\n",
      "Iteration: 130/214, Loss: 0.308052122592926\n",
      "Iteration: 131/214, Loss: 0.018950488418340683\n",
      "Iteration: 132/214, Loss: 0.2914351224899292\n",
      "Iteration: 133/214, Loss: 0.0004951105802319944\n",
      "Iteration: 134/214, Loss: 0.02727552503347397\n",
      "Iteration: 135/214, Loss: 0.029339883476495743\n",
      "Iteration: 136/214, Loss: 0.0016114080790430307\n",
      "Iteration: 137/214, Loss: 0.0024918564595282078\n",
      "Iteration: 138/214, Loss: 0.05294344201683998\n",
      "Iteration: 139/214, Loss: 0.0617704913020134\n",
      "Iteration: 140/214, Loss: 0.2883698046207428\n",
      "Iteration: 141/214, Loss: 0.04194796457886696\n",
      "Iteration: 142/214, Loss: 0.3502356708049774\n",
      "Iteration: 143/214, Loss: 0.08438777923583984\n",
      "Iteration: 144/214, Loss: 0.23070357739925385\n",
      "Iteration: 145/214, Loss: 0.10092215985059738\n",
      "Iteration: 146/214, Loss: 0.02674407884478569\n",
      "Iteration: 147/214, Loss: 0.15259802341461182\n",
      "Iteration: 148/214, Loss: 0.08428562432527542\n",
      "Iteration: 149/214, Loss: 0.029930807650089264\n",
      "Iteration: 150/214, Loss: 0.028212808072566986\n",
      "Iteration: 151/214, Loss: 0.023261401802301407\n",
      "Iteration: 152/214, Loss: 0.07239458709955215\n",
      "Iteration: 153/214, Loss: 0.03259989619255066\n",
      "Iteration: 154/214, Loss: 0.03352995216846466\n",
      "Iteration: 155/214, Loss: 0.2859077453613281\n",
      "Iteration: 156/214, Loss: 0.1331949383020401\n",
      "Iteration: 157/214, Loss: 0.0023718078155070543\n",
      "Iteration: 158/214, Loss: 0.06655526161193848\n",
      "Iteration: 159/214, Loss: 0.042421404272317886\n",
      "Iteration: 160/214, Loss: 0.07789462059736252\n",
      "Iteration: 161/214, Loss: 0.24870619177818298\n",
      "Iteration: 162/214, Loss: 0.048357509076595306\n",
      "Iteration: 163/214, Loss: 0.0885588526725769\n",
      "Iteration: 164/214, Loss: 0.03425419330596924\n",
      "Iteration: 165/214, Loss: 0.3021758198738098\n",
      "Iteration: 166/214, Loss: 0.00207290961407125\n",
      "Iteration: 167/214, Loss: 0.02837013453245163\n",
      "Iteration: 168/214, Loss: 0.02385040372610092\n",
      "Iteration: 169/214, Loss: 0.15305720269680023\n",
      "Iteration: 170/214, Loss: 0.22435155510902405\n",
      "Iteration: 171/214, Loss: 0.26037541031837463\n",
      "Iteration: 172/214, Loss: 0.07059518992900848\n",
      "Iteration: 173/214, Loss: 0.026730380952358246\n",
      "Iteration: 174/214, Loss: 0.011265402659773827\n",
      "Iteration: 175/214, Loss: 0.18825747072696686\n",
      "Iteration: 176/214, Loss: 0.0735597014427185\n",
      "Iteration: 177/214, Loss: 0.1827053427696228\n",
      "Iteration: 178/214, Loss: 0.07532470673322678\n",
      "Iteration: 179/214, Loss: 0.02226511389017105\n",
      "Iteration: 180/214, Loss: 0.0008827132405713201\n",
      "Iteration: 181/214, Loss: 0.14598089456558228\n",
      "Iteration: 182/214, Loss: 0.02161642163991928\n",
      "Iteration: 183/214, Loss: 0.3759683072566986\n",
      "Iteration: 184/214, Loss: 0.1820921003818512\n",
      "Iteration: 185/214, Loss: 0.04952241852879524\n",
      "Iteration: 186/214, Loss: 0.02873743511736393\n",
      "Iteration: 187/214, Loss: 0.05277581885457039\n",
      "Iteration: 188/214, Loss: 0.034319616854190826\n",
      "Iteration: 189/214, Loss: 0.3868357837200165\n",
      "Iteration: 190/214, Loss: 0.0807165876030922\n",
      "Iteration: 191/214, Loss: 0.13598065078258514\n",
      "Iteration: 192/214, Loss: 0.0674852728843689\n",
      "Iteration: 193/214, Loss: 0.2310413271188736\n",
      "Iteration: 194/214, Loss: 0.0070551130920648575\n",
      "Iteration: 195/214, Loss: 0.11352130770683289\n",
      "Iteration: 196/214, Loss: 0.30352577567100525\n",
      "Iteration: 197/214, Loss: 0.26386141777038574\n",
      "Iteration: 198/214, Loss: 0.1509593427181244\n",
      "Iteration: 199/214, Loss: 0.039651960134506226\n",
      "Iteration: 200/214, Loss: 0.19856980443000793\n",
      "Iteration: 201/214, Loss: 0.11003350466489792\n",
      "Iteration: 202/214, Loss: 0.3212440609931946\n",
      "Iteration: 203/214, Loss: 0.025763725861907005\n",
      "Iteration: 204/214, Loss: 0.14019231498241425\n",
      "Iteration: 205/214, Loss: 0.24755790829658508\n",
      "Iteration: 206/214, Loss: 0.034428540617227554\n",
      "Iteration: 207/214, Loss: 0.45502352714538574\n",
      "Iteration: 208/214, Loss: 0.030860641971230507\n",
      "Iteration: 209/214, Loss: 0.05207762494683266\n",
      "Iteration: 210/214, Loss: 0.1882394254207611\n",
      "Iteration: 211/214, Loss: 0.06079397723078728\n",
      "Iteration: 212/214, Loss: 0.062414783984422684\n",
      "Iteration: 213/214, Loss: 0.2314397543668747\n",
      "Iteration: 214/214, Loss: 0.037643708288669586\n",
      "tensor(24.4107, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Iteration: 1/214, Loss: 0.05726371705532074\n",
      "Iteration: 2/214, Loss: 0.029276074841618538\n",
      "Iteration: 3/214, Loss: 0.03594295680522919\n",
      "Iteration: 4/214, Loss: 0.31987765431404114\n",
      "Iteration: 5/214, Loss: 0.04320180043578148\n",
      "Iteration: 6/214, Loss: 0.0401596762239933\n",
      "Iteration: 7/214, Loss: 0.04536258801817894\n",
      "Iteration: 8/214, Loss: 0.02944994531571865\n",
      "Iteration: 9/214, Loss: 0.10086526721715927\n",
      "Iteration: 10/214, Loss: 0.2889643609523773\n",
      "Iteration: 11/214, Loss: 0.1684032380580902\n",
      "Iteration: 12/214, Loss: 0.08590614795684814\n",
      "Iteration: 13/214, Loss: 0.0009353697532787919\n",
      "Iteration: 14/214, Loss: 0.417414128780365\n",
      "Iteration: 15/214, Loss: 0.037901852279901505\n",
      "Iteration: 16/214, Loss: 0.039670031517744064\n",
      "Iteration: 17/214, Loss: 0.8370158672332764\n",
      "Iteration: 18/214, Loss: 0.00033302634255960584\n",
      "Iteration: 19/214, Loss: 0.0004284488968551159\n",
      "Iteration: 20/214, Loss: 0.17103956639766693\n",
      "Iteration: 21/214, Loss: 0.020566148683428764\n",
      "Iteration: 22/214, Loss: 0.11268969625234604\n",
      "Iteration: 23/214, Loss: 0.02580227516591549\n",
      "Iteration: 24/214, Loss: 0.160564124584198\n",
      "Iteration: 25/214, Loss: 0.04633805528283119\n",
      "Iteration: 26/214, Loss: 0.09007514268159866\n",
      "Iteration: 27/214, Loss: 0.07992804795503616\n",
      "Iteration: 28/214, Loss: 0.08859789371490479\n",
      "Iteration: 29/214, Loss: 0.0223089586943388\n",
      "Iteration: 30/214, Loss: 0.18383069336414337\n",
      "Iteration: 31/214, Loss: 0.020492561161518097\n",
      "Iteration: 32/214, Loss: 0.1056845635175705\n",
      "Iteration: 33/214, Loss: 0.001577125396579504\n",
      "Iteration: 34/214, Loss: 0.0513736829161644\n",
      "Iteration: 35/214, Loss: 0.036572035402059555\n",
      "Iteration: 36/214, Loss: 0.05697016045451164\n",
      "Iteration: 37/214, Loss: 0.10654236376285553\n",
      "Iteration: 38/214, Loss: 0.1348002403974533\n",
      "Iteration: 39/214, Loss: 0.18460896611213684\n",
      "Iteration: 40/214, Loss: 0.1839161366224289\n",
      "Iteration: 41/214, Loss: 0.045041847974061966\n",
      "Iteration: 42/214, Loss: 0.12123248726129532\n",
      "Iteration: 43/214, Loss: 0.07227963209152222\n",
      "Iteration: 44/214, Loss: 0.0004578432417474687\n",
      "Iteration: 45/214, Loss: 0.19281837344169617\n",
      "Iteration: 46/214, Loss: 0.029621969908475876\n",
      "Iteration: 47/214, Loss: 0.2028365582227707\n",
      "Iteration: 48/214, Loss: 0.02186882682144642\n",
      "Iteration: 49/214, Loss: 0.02414744347333908\n",
      "Iteration: 50/214, Loss: 0.04404445365071297\n",
      "Iteration: 51/214, Loss: 0.08911683410406113\n",
      "Iteration: 52/214, Loss: 0.21522104740142822\n",
      "Iteration: 53/214, Loss: 0.055038027465343475\n",
      "Iteration: 54/214, Loss: 0.02348027564585209\n",
      "Iteration: 55/214, Loss: 0.16100379824638367\n",
      "Iteration: 56/214, Loss: 0.029965875670313835\n",
      "Iteration: 57/214, Loss: 0.041684649884700775\n",
      "Iteration: 58/214, Loss: 0.031017009168863297\n",
      "Iteration: 59/214, Loss: 0.11061360687017441\n",
      "Iteration: 60/214, Loss: 0.0005948423640802503\n",
      "Iteration: 61/214, Loss: 0.12634867429733276\n",
      "Iteration: 62/214, Loss: 0.04230870306491852\n",
      "Iteration: 63/214, Loss: 0.0006364906439557672\n",
      "Iteration: 64/214, Loss: 0.055513083934783936\n",
      "Iteration: 65/214, Loss: 0.2527937889099121\n",
      "Iteration: 66/214, Loss: 0.3003126382827759\n",
      "Iteration: 67/214, Loss: 0.2437223643064499\n",
      "Iteration: 68/214, Loss: 0.06609396636486053\n",
      "Iteration: 69/214, Loss: 0.038024261593818665\n",
      "Iteration: 70/214, Loss: 0.04138347879052162\n",
      "Iteration: 71/214, Loss: 0.23681849241256714\n",
      "Iteration: 72/214, Loss: 0.15826290845870972\n",
      "Iteration: 73/214, Loss: 0.10682253539562225\n",
      "Iteration: 74/214, Loss: 0.05836709588766098\n",
      "Iteration: 75/214, Loss: 0.16662128269672394\n",
      "Iteration: 76/214, Loss: 0.03773164004087448\n",
      "Iteration: 77/214, Loss: 0.04248259216547012\n",
      "Iteration: 78/214, Loss: 0.09115196019411087\n",
      "Iteration: 79/214, Loss: 0.11666398495435715\n",
      "Iteration: 80/214, Loss: 0.0785706490278244\n",
      "Iteration: 81/214, Loss: 0.12199150770902634\n",
      "Iteration: 82/214, Loss: 0.032128799706697464\n",
      "Iteration: 83/214, Loss: 0.023370416834950447\n",
      "Iteration: 84/214, Loss: 0.02346017397940159\n",
      "Iteration: 85/214, Loss: 0.16950276494026184\n",
      "Iteration: 86/214, Loss: 0.08568088710308075\n",
      "Iteration: 87/214, Loss: 0.17144079506397247\n",
      "Iteration: 88/214, Loss: 0.001358017441816628\n",
      "Iteration: 89/214, Loss: 0.023603329434990883\n",
      "Iteration: 90/214, Loss: 0.02576438896358013\n",
      "Iteration: 91/214, Loss: 0.04866531863808632\n",
      "Iteration: 92/214, Loss: 0.15751272439956665\n",
      "Iteration: 93/214, Loss: 0.2873438596725464\n",
      "Iteration: 94/214, Loss: 0.08328273892402649\n",
      "Iteration: 95/214, Loss: 0.2416587620973587\n",
      "Iteration: 96/214, Loss: 0.022475017234683037\n",
      "Iteration: 97/214, Loss: 0.08872280269861221\n",
      "Iteration: 98/214, Loss: 0.21157146990299225\n",
      "Iteration: 99/214, Loss: 0.04692497104406357\n",
      "Iteration: 100/214, Loss: 0.1326536387205124\n",
      "Iteration: 101/214, Loss: 0.10139045119285583\n",
      "Iteration: 102/214, Loss: 0.000708657200448215\n",
      "Iteration: 103/214, Loss: 0.30656546354293823\n",
      "Iteration: 104/214, Loss: 0.11123556643724442\n",
      "Iteration: 105/214, Loss: 0.1303883045911789\n",
      "Iteration: 106/214, Loss: 0.0026280214078724384\n",
      "Iteration: 107/214, Loss: 0.28038179874420166\n",
      "Iteration: 108/214, Loss: 0.32946258783340454\n",
      "Iteration: 109/214, Loss: 0.03130533918738365\n",
      "Iteration: 110/214, Loss: 0.06439696252346039\n",
      "Iteration: 111/214, Loss: 0.24509942531585693\n",
      "Iteration: 112/214, Loss: 0.150806725025177\n",
      "Iteration: 113/214, Loss: 0.2026609480381012\n",
      "Iteration: 114/214, Loss: 0.08262559026479721\n",
      "Iteration: 115/214, Loss: 0.04892962425947189\n",
      "Iteration: 116/214, Loss: 0.3300311863422394\n",
      "Iteration: 117/214, Loss: 0.1864825189113617\n",
      "Iteration: 118/214, Loss: 0.03292614594101906\n",
      "Iteration: 119/214, Loss: 0.02514396235346794\n",
      "Iteration: 120/214, Loss: 0.17627018690109253\n",
      "Iteration: 121/214, Loss: 0.021628977730870247\n",
      "Iteration: 122/214, Loss: 0.06537532806396484\n",
      "Iteration: 123/214, Loss: 0.08080890029668808\n",
      "Iteration: 124/214, Loss: 0.05301942676305771\n",
      "Iteration: 125/214, Loss: 0.02980141155421734\n",
      "Iteration: 126/214, Loss: 0.053335417062044144\n",
      "Iteration: 127/214, Loss: 0.039215054363012314\n",
      "Iteration: 128/214, Loss: 0.2195521891117096\n",
      "Iteration: 129/214, Loss: 0.33768320083618164\n",
      "Iteration: 130/214, Loss: 0.3052801191806793\n",
      "Iteration: 131/214, Loss: 0.025294369086623192\n",
      "Iteration: 132/214, Loss: 0.31919926404953003\n",
      "Iteration: 133/214, Loss: 0.0015783491544425488\n",
      "Iteration: 134/214, Loss: 0.03541122004389763\n",
      "Iteration: 135/214, Loss: 0.021861568093299866\n",
      "Iteration: 136/214, Loss: 0.0007822636398486793\n",
      "Iteration: 137/214, Loss: 0.0009685562690719962\n",
      "Iteration: 138/214, Loss: 0.038684237748384476\n",
      "Iteration: 139/214, Loss: 0.02596498280763626\n",
      "Iteration: 140/214, Loss: 0.19427652657032013\n",
      "Iteration: 141/214, Loss: 0.03826971724629402\n",
      "Iteration: 142/214, Loss: 0.29709506034851074\n",
      "Iteration: 143/214, Loss: 0.0796758234500885\n",
      "Iteration: 144/214, Loss: 0.13735520839691162\n",
      "Iteration: 145/214, Loss: 0.08665446937084198\n",
      "Iteration: 146/214, Loss: 0.02424022927880287\n",
      "Iteration: 147/214, Loss: 0.15337075293064117\n",
      "Iteration: 148/214, Loss: 0.08698681741952896\n",
      "Iteration: 149/214, Loss: 0.03994102403521538\n",
      "Iteration: 150/214, Loss: 0.03334721177816391\n",
      "Iteration: 151/214, Loss: 0.019303707405924797\n",
      "Iteration: 152/214, Loss: 0.07785006612539291\n",
      "Iteration: 153/214, Loss: 0.02223372273147106\n",
      "Iteration: 154/214, Loss: 0.017308643087744713\n",
      "Iteration: 155/214, Loss: 0.23793384432792664\n",
      "Iteration: 156/214, Loss: 0.10481465607881546\n",
      "Iteration: 157/214, Loss: 0.0006133175920695066\n",
      "Iteration: 158/214, Loss: 0.05409137159585953\n",
      "Iteration: 159/214, Loss: 0.026491180062294006\n",
      "Iteration: 160/214, Loss: 0.09527085721492767\n",
      "Iteration: 161/214, Loss: 0.2258969247341156\n",
      "Iteration: 162/214, Loss: 0.05147076025605202\n",
      "Iteration: 163/214, Loss: 0.1143798828125\n",
      "Iteration: 164/214, Loss: 0.026493214070796967\n",
      "Iteration: 165/214, Loss: 0.2668103277683258\n",
      "Iteration: 166/214, Loss: 0.0011534071527421474\n",
      "Iteration: 167/214, Loss: 0.026710469275712967\n",
      "Iteration: 168/214, Loss: 0.028502315282821655\n",
      "Iteration: 169/214, Loss: 0.17509178817272186\n",
      "Iteration: 170/214, Loss: 0.21509727835655212\n",
      "Iteration: 171/214, Loss: 0.19601571559906006\n",
      "Iteration: 172/214, Loss: 0.053330197930336\n",
      "Iteration: 173/214, Loss: 0.02683814987540245\n",
      "Iteration: 174/214, Loss: 0.014618555083870888\n",
      "Iteration: 175/214, Loss: 0.19785386323928833\n",
      "Iteration: 176/214, Loss: 0.11272694915533066\n",
      "Iteration: 177/214, Loss: 0.20441001653671265\n",
      "Iteration: 178/214, Loss: 0.06731148064136505\n",
      "Iteration: 179/214, Loss: 0.02299005538225174\n",
      "Iteration: 180/214, Loss: 0.0005434232298284769\n",
      "Iteration: 181/214, Loss: 0.1173698827624321\n",
      "Iteration: 182/214, Loss: 0.027050117030739784\n",
      "Iteration: 183/214, Loss: 0.21710419654846191\n",
      "Iteration: 184/214, Loss: 0.15603908896446228\n",
      "Iteration: 185/214, Loss: 0.05280057340860367\n",
      "Iteration: 186/214, Loss: 0.04439828544855118\n",
      "Iteration: 187/214, Loss: 0.05861911177635193\n",
      "Iteration: 188/214, Loss: 0.04908669739961624\n",
      "Iteration: 189/214, Loss: 0.3348642587661743\n",
      "Iteration: 190/214, Loss: 0.07718055695295334\n",
      "Iteration: 191/214, Loss: 0.14552092552185059\n",
      "Iteration: 192/214, Loss: 0.06493569165468216\n",
      "Iteration: 193/214, Loss: 0.19813205301761627\n",
      "Iteration: 194/214, Loss: 0.0027622217312455177\n",
      "Iteration: 195/214, Loss: 0.08473610877990723\n",
      "Iteration: 196/214, Loss: 0.33959585428237915\n",
      "Iteration: 197/214, Loss: 0.29644978046417236\n",
      "Iteration: 198/214, Loss: 0.19630113244056702\n",
      "Iteration: 199/214, Loss: 0.06518977135419846\n",
      "Iteration: 200/214, Loss: 0.21436932682991028\n",
      "Iteration: 201/214, Loss: 0.14078699052333832\n",
      "Iteration: 202/214, Loss: 0.2516176402568817\n",
      "Iteration: 203/214, Loss: 0.018940016627311707\n",
      "Iteration: 204/214, Loss: 0.13167376816272736\n",
      "Iteration: 205/214, Loss: 0.20724092423915863\n",
      "Iteration: 206/214, Loss: 0.024504242464900017\n",
      "Iteration: 207/214, Loss: 0.28189539909362793\n",
      "Iteration: 208/214, Loss: 0.03372747451066971\n",
      "Iteration: 209/214, Loss: 0.06260020285844803\n",
      "Iteration: 210/214, Loss: 0.2982129156589508\n",
      "Iteration: 211/214, Loss: 0.11809641122817993\n",
      "Iteration: 212/214, Loss: 0.08938432484865189\n",
      "Iteration: 213/214, Loss: 0.2853366434574127\n",
      "Iteration: 214/214, Loss: 0.04571071267127991\n",
      "tensor(23.3370, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 20\n",
    "model.to(device)\n",
    "    \n",
    "# parameters\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005,\n",
    "                                momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "len_dataloader = len(dataloader)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    i = 0    \n",
    "    epoch_loss = 0\n",
    "    for imgs, annotations in dataloader:\n",
    "        i += 1\n",
    "        imgs = list(img.to(device) for img in imgs)\n",
    "        annotations = [{k: v.to(device) for k, v in t.items()} for t in annotations]\n",
    "        loss_dict = model([imgs[0]], [annotations[0]])\n",
    "        losses = sum(loss for loss in loss_dict.values())        \n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step() \n",
    "        print(f'Iteration: {i}/{len_dataloader}, Loss: {losses}')\n",
    "        epoch_loss += losses\n",
    "    print(epoch_loss)\n",
    "torch.save(model.state_dict(),'model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def plot_image(img_tensor, annotation):\n",
    "    \n",
    "    fig,ax = plt.subplots(1)\n",
    "    img = img_tensor.cpu().data\n",
    "\n",
    "    # Display the image\n",
    "    ax.imshow(img.permute(1, 2, 0))\n",
    "    for box in annotation[\"boxes\"]:\n",
    "        box=box.cpu().detach().numpy()\n",
    "        xmin, ymin, xmax, ymax = box\n",
    "\n",
    "        # Create a Rectangle patch\n",
    "        rect = patches.Rectangle((xmin,ymin),(xmax-xmin),(ymax-ymin),linewidth=1,edgecolor='r',facecolor='none')\n",
    "        # Add the patch to the Axes\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "    plt.show()\n",
    "for imgs, annotations in dataloader:\n",
    "        imgs = list(img.to(device) for img in imgs)\n",
    "        annotations = [{k: v.to(device) for k, v in t.items()} for t in annotations]\n",
    "        break\n",
    "model2 = get_model_instance_segmentation(3)\n",
    "model2.load_state_dict(torch.load(\"C:/Users/90761/Desktop/kaggle/face-mask-detection/model.pt\"))\n",
    "model2.eval()\n",
    "model2.to(device)\n",
    "preds = model2(imgs)\n",
    "plot_image(imgs[0],preds[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15 (main, Nov 24 2022, 14:39:17) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9b49298f62599efe596f6a9996bf3021bf5b4cd8f79e7be97d94f0ec46e1954d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
